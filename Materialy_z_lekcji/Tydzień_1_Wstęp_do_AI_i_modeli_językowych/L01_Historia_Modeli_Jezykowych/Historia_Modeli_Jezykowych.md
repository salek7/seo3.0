## Przewodnik po Lekcji: Historia Modeli JÄ™zykowych ğŸ§  (Wersja Poprawiona)

**Link do lekcji na platformie:** [Obejrzyj lekcjÄ™ na SensAI Academy](https://learn.sensai.academy/next/public/lesson/252)

CzeÅ›Ä‡! Ta notatka pomoÅ¼e Ci zrozumieÄ‡ fascynujÄ…cÄ… historiÄ™ sztucznej inteligencji (AI), a zwÅ‚aszcza modeli jÄ™zykowych, ktÃ³re rewolucjonizujÄ… nasz Å›wiat. Dowiesz siÄ™, skÄ…d wziÄ™Å‚a siÄ™ ta technologia, jak dziaÅ‚a (na Å›wietnym przykÅ‚adzie od `Financial Times`) i dlaczego jest tak waÅ¼na. Zaczynajmy!

### Dlaczego AI i Modele JÄ™zykowe? Cel Google ğŸ¯

-   **Problem:** Google od zawsze potrzebowaÅ‚o rozumieÄ‡ **kontekst** â€“ zarÃ³wno tego, co wpisujesz w wyszukiwarkÄ™ (np. "czarna konsola do gier" -> myÅ›lisz PlayStation), jak i treÅ›ci na stronach internetowych.
-   **Cel Google:** DostarczaÄ‡ jak najlepsze wyniki wyszukiwania. To napÄ™dzaÅ‚o rozwÃ³j technologii rozumienia jÄ™zyka.
-   **Kluczowa Rola Google:** WedÅ‚ug prowadzÄ…cego, Google jest gÅ‚Ã³wnym motorem rozwoju AI dziÄ™ki swoim technologiom i patentom.

### Kamienie Milowe w Rozumieniu JÄ™zyka

1.  **2013: `Word2Vec` - SÅ‚owa jako Wektory**

    **Co to?** Algorytm zamieniajÄ…cy sÅ‚owa na wektory (liczby) w przestrzeni matematycznej.

    **Co to daÅ‚o?** Google zrozumiaÅ‚o, Å¼e pewne sÅ‚owa sÄ… sobie "bliskie" znaczeniowo (np. "pianino" jest blisko "skrzypiec", "Mercedes" blisko "Volkswagen"). To byÅ‚ poczÄ…tek rozumienia powiÄ…zaÅ„ tematycznych na stronach (np. strona o instrumentach powinna omawiaÄ‡ pianino, skrzypce, gitarÄ™).

    **Ograniczenie:** Jeszcze nie pozwalaÅ‚o to w peÅ‚ni zrozumieÄ‡ kontekstu caÅ‚ych zdaÅ„.

2.  **2014: `Sequence-to-Sequence` - TÅ‚umaczenie Maszynowe**

    **Co to?** Metoda pozwalajÄ…ca na przetwarzanie sekwencji danych (np. zdaÅ„).

    **Zastosowanie:** UmoÅ¼liwiÅ‚o stworzenie `Google Translate`. OkazaÅ‚o siÄ™, Å¼e sÅ‚owa o tym samym znaczeniu w rÃ³Å¼nych jÄ™zykach (np. "pies" i "dog") majÄ… podobne reprezentacje wektorowe.

### Rewolucja: Architektura TransformerÃ³w âœ¨

To technologia, ktÃ³ra **rzÄ…dzi obecnym AI**. ProwadzÄ…cy omawia jÄ… na podstawie doskonaÅ‚ego opracowania `Financial Times` (znajdziesz je [tutaj](https://ig.ft.com/ai-explained/)).

**Jak DziaÅ‚ajÄ… Transformery (Krok po Kroku wg Financial Times):**

1.  **Tokenizacja:** Zdanie jest dzielone na mniejsze jednostki â€“ **tokeny**. W przykÅ‚adzie FT dla uproszczenia tokenami sÄ… caÅ‚e sÅ‚owa (np. "We | go | to | work | by | train"). *W praktyce tokeny to czÄ™sto fragmenty sÅ‚Ã³w (wiÄ™cej o tym pÃ³Åºniej).*

2.  **Szukanie Bliskich SÅ‚Ã³w i PrawdopodobieÅ„stwa:** Algorytm analizuje, jakie sÅ‚owa czÄ™sto wystÄ™pujÄ… obok siebie (np. co moÅ¼e wystÄ…piÄ‡ obok sÅ‚owa "work"? "processes", "hour", "from home", ale raczej nie "zebra" czy "polka").

3.  **Embedding (Zamiana na Wektory):** Tokeny (lub ich kombinacje) sÄ… zamieniane na wektory (liczby). Pozwala to matematycznie mierzyÄ‡ "dystans" miÄ™dzy sÅ‚owami/konceptami.
    *PrzykÅ‚ad:* "Sea" (morze) jest blisko "ocean", ale to nie to samo. "Football" jest blisko "soccer".

4.  **Klastrowanie:** Podobne koncepcyjnie sÅ‚owa grupujÄ… siÄ™ razem (np. "train", "bus", "car" jako pojazdy; "walk", "swim", "run" jako sporty).

5.  **Self-Attention (SamouwaÅ¼noÅ›Ä‡) - Klucz do Kontekstu!** ğŸ§ 

    **Co to?** Mechanizm, dziÄ™ki ktÃ³remu model **rozumie relacje miÄ™dzy sÅ‚owami w zdaniu** i okreÅ›la, ktÃ³re sÅ‚owa sÄ… najwaÅ¼niejsze dla ogÃ³lnego znaczenia.

    *PrzykÅ‚ad 1:* W zdaniu "I have **no** interest in politics", `Self-Attention` wie, Å¼e sÅ‚owo "**no**" jest kluczowe i zmienia sens caÅ‚oÅ›ci.

    *PrzykÅ‚ad 2:* W zdaniu "Bank **interest** rate rises", `Self-Attention` rozumie, Å¼e "**interest**" oznacza tu "oprocentowanie", a nie "zainteresowanie" (jak w przykÅ‚adzie 1), dziÄ™ki kontekstowi sÅ‚Ã³w "bank" i "rate".

    *PrzykÅ‚ad 3:* W "The dog chewed the bone because **it** was hungry", model wie, Å¼e "**it**" odnosi siÄ™ do "**dog**". Ale w "The dog chewed the bone because **it** was delicious", model wie, Å¼e "**it**" odnosi siÄ™ do "**bone**".

6.  **Generowanie TreÅ›ci (Przewidywanie):** Modele jÄ™zykowe generujÄ… tekst, przewidujÄ…c najbardziej prawdopodobne nastÄ™pne sÅ‚owo (token) w danym kontekÅ›cie.
    *PrzykÅ‚ad:* Po "The Financial Times is...", model oblicza **wynik prawdopodobieÅ„stwa (`probability score`)** dla rÃ³Å¼nych moÅ¼liwych nastÄ™pnych sÅ‚Ã³w ("a", "about", "the"...) i wybiera to najbardziej prawdopodobne (np. "a newspaper found in 1888").

    **Cel:** Naszym celem (np. w SEO, marketingu) jest tworzenie treÅ›ci, gdzie prawdopodobieÅ„stwo wystÄ…pienia kolejnych "dobrych" tokenÃ³w jest jak najwyÅ¼sze.

**Podsumowanie Procesu Transformera:**
-   Tokenizacja
-   Embedding
-   Self-Attention
-   Wynik (Encoded Output)

### Tokenizacja w Praktyce: WiÄ™cej niÅ¼ SÅ‚owa

-   **Realne Tokeny:** Modele AI (jak te od OpenAI) zazwyczaj dzielÄ… tekst na **fragmenty sÅ‚Ã³w** lub **czÄ™ste sekwencje znakÃ³w**, a nie tylko caÅ‚e sÅ‚owa.
    *PrzykÅ‚ad:* "Hej, co u Ciebie sÅ‚ychaÄ‡?" (24 znaki) zostaÅ‚o podzielone na 11 tokenÃ³w (rÃ³Å¼ne kolory w przykÅ‚adzie).

-   **Dlaczego To WaÅ¼ne?**

    **Koszt:** PÅ‚acisz za przetwarzanie **tokenÃ³w**, nie sÅ‚Ã³w czy znakÃ³w.

    **WydajnoÅ›Ä‡:** JÄ™zyk angielski jest zwykle "taÅ„szy" w tokenizacji niÅ¼ polski (mniej tokenÃ³w na tÄ™ samÄ… treÅ›Ä‡). *Pro Tip: Promptowanie po angielsku moÅ¼e byÄ‡ taÅ„sze.*

    **SEO:** Google uÅ¼ywa TransformerÃ³w (jak `BERT`) do oceny treÅ›ci, a one dziaÅ‚ajÄ… wÅ‚aÅ›nie na tokenach.

### OÅ› Czasu Kluczowych WydarzeÅ„

-   **2018:** Google wprowadza `BERT` (Bi-directional Encoder Representations from Transformers) â€“ kluczowy algorytm do oceny treÅ›ci w wyszukiwarce. Powstaje teÅ¼ pierwszy `GPT` od OpenAI.
-   **2021:** Google zapowiada pierwsze kroki w kierunku multimodalnoÅ›ci (wyszukiwanie gÅ‚osem, obrazem), ale technologia nie byÅ‚a jeszcze gotowa.
-   **~2022:** Pojawia siÄ™ `ChatGPT` â€“ przeÅ‚om i poczÄ…tek ogromnego przyspieszenia.
-   **2024:** Mamy juÅ¼ `GPT-4o` (zaawansowany model multimodalny).
-   **Wycena OpenAI:** GwaÅ‚townie roÅ›nie, pokazujÄ…c tempo rozwoju (wspomniane 157 mld USD, a pÃ³Åºniej aktualizacja do 300 mld USD).

### Aktualizacja na Rok 2025: Niesamowite Przyspieszenie ğŸš€

*(Ta czÄ™Å›Ä‡ zostaÅ‚a nagrana pÃ³Åºniej, aby odzwierciedliÄ‡ bÅ‚yskawiczny postÄ™p)*

-   **PostÄ™p WykÅ‚adniczo-WykÅ‚adniczy:** RozwÃ³j AI przyspiesza w niewyobraÅ¼alnym tempie.
-   **Nowe Modele:** PojawiajÄ… siÄ™ modele z zaawansowanym **reasoningiem** (wnioskowaniem), przewyÅ¼szajÄ…ce Å›rednie ludzkie IQ (np. `Gemini 2.5`, `O3`, `O4`). *PorÃ³wnanie: Åšrednie IQ 100, modele 130+, Einstein 167.*
-   **Grafika i Wideo - ZdjÄ™cia:** Fotorealizm, generowanie napisÃ³w (narzÄ™dzie `Ideogram`).
-   **Grafika i Wideo - Wideo:** Niesamowity detal, kontrola kamery, generowanie dÅºwiÄ™ku, storytelling, kategoria OscarÃ³w dla AI.
-   **MultimodalnoÅ›Ä‡ PeÅ‚nÄ… ParÄ…:** AI rozumie i wnioskuje nie tylko z tekstu, ale i z obrazÃ³w (np. analiza kreacji reklamowych konkurencji).
-   **PrzyszÅ‚oÅ›Ä‡:** Generative AI â¡ï¸ Agenci AI (autonomiczne programy) â¡ï¸ Physical AI (robotyka).
-   **Physical AI - Trening:** Wirtualne Å›wiaty jak `NVIDIA Cosmos` do treningu autonomicznych samochodÃ³w i humanoidÃ³w.
-   **Physical AI - Zastosowania:** Humanoidy w produkcji (np. iPhony), polska firma tworzÄ…ca zaawansowanego humanoida, neuroprotezy (przywracanie mowy), dekodowanie jÄ™zyka delfinÃ³w, medycyna (diagnostyka, odkrywanie lekÃ³w).

### Podsumowanie i Co Dalej?

ZrozumieliÅ›my, jak ewolucja potrzeby rozumienia kontekstu doprowadziÅ‚a do powstania TransformerÃ³w â€“ kluczowej technologii AI. WidzieliÅ›my, jak dziaÅ‚ajÄ… (tokenizacja, embedding, self-attention, prawdopodobieÅ„stwo) i jak bÅ‚yskawicznie rozwija siÄ™ caÅ‚a dziedzina.

W kolejnej lekcji zagÅ‚Ä™bimy siÄ™ bardziej w **modele jÄ™zykowe**, ich wyzwania i jak sobie z nimi radziÄ‡. Do zobaczenia! ğŸ‘‹ 
Cześć! Witam Cię w kolejnej lekcji. Dzisiaj omówimy sobie, teraz omówimy sobie aspekty modeli open source. Uważam, że jest to świat, którego nie wolno przeoczyć i de facto w tym świecie dzieje się dużo więcej niż wśród modeli komercyjnych. W tej lekcji porozmawiamy o tym sobie, o tym czym przede wszystkim są modele open source i dlaczego warto. Pokażę Ci źródła wiedzy i miejsca, w których możesz znaleźć nowości w tym świecie oraz jak się w tym świecie poruszać, nawigować, gdzie szukać wiedzy, gdzie szukać nowości i jak to wykorzystać. Dodatkowo pogadamy o infrastrukturach, w tym o infrastrukturach alternatywnych, w których będziecie mogli zarówno zakupić moce obliczeniowe, wykorzystując modele open source, ale również pokażę Ci, w jaki sposób odpalić model open source na swoim komputerze lokalnie. Jest to szalenie istotne, jeśli mamy w naszej organizacji na przykład dane wrażliwe, czy też po prostu chcemy zoptymalizować koszty. Ja odpalę sztuczną inteligencję powiedzmy w cudzysłowie na moim MacBooku. I na końcu pokażę Ci jak to zrobić zdalnie, na przykład na jakiejś tam infrastrukturze cloudowej, czy na serwerze, gdzie wykupimy sobie maszynę wspólnie i pokażę Ci jak ten model odpalić, jak się z nim komunikować. W kolejnych częściach również pokażę Ci jak orkiestrować modele open source'owe. Zacznijmy sobie od pierwszego głównego pytania. Czy warto? Uważam, że zdecydowanie na rynku mamy kilku liderów, na przykład Meta, czyli Facebook, NVIDIA, Google, Microsoft oraz Mistrala na przykład. Wszystkie z tych organizacji udostępniają te modele za darmo, więc jeżeli Facebook, który płaci za to setki milionów dolarów, czy Google, czy Microsoft daje nam za darmo, należy z tego korzystać, albo przynajmniej mieć świadomość, że taki świat istnieje. dodatkowo OpenAI w ostatnim czasie zmieniło charakter swojej organizacji z organizacji non-profit na organizację for-profit więc warto mieć alternatywę, a przynajmniej ją znać technologia również rozwija się na tyle szybko, że z tygodnia na tydzień jak z Damianem organizowaliśmy, organizujemy AI Launch co tydzień wychodzą nowe modele, które już doświadczają swoją doskonałością te największe komercyjne modele językowe by pokazać Ci sytuację tu i teraz na dzisiaj przejdźmy na stronę internetową, gdzie znajduje się tak zwany leaderboard. W materiałach dodatkowych znajdziesz adres tej strony internetowej. Jeżeli wpiszesz sobie w Google po prostu spróbujmy znaleźć tę stronę wspólnie AI leaderboards i pierwszy wynik, myślę, że to jest dobry wynik znajduje się tutaj tablica z najnowszymi wszystkimi modelami AI językowymi na świecie po której można się po prostu nawigować i patrzeć z jednej strony, co jest nowe, z drugiej strony porównywać z innymi. W poprzedniej lekcji dowiedziałeś się, jak czytać benchmarki modeli językowych i w jaki sposób one są porównywane. Podsumujmy, są to testy i zadania na przykład logiczne, które modele językowe spełniają i w zależności, wiecie, próba Turinga, jak się model pomyli, tak są to oceniane. Ale chodzimy do tego leaderboardu. OK, czyli to, co wiemy i co jest oczywistością, OpenAI jako najlepszy model językowy i też najlepsza infrastruktura na świecie oczywiście jest na pierwszym miejscu, natomiast są to modele komercyjne, w których zapłacimy zarówno za dostęp czy to do jakichś usług premium, czy też po prostu zapłacimy za tokeny. Natomiast to, co widzimy poniżej i to jest bardzo istotne, są to modele od Mety, czyli od Facebooka. Facebook wszystkie swoje modele udostępnia za darmo i jest to bardzo kuszące. Wokół tego tworzy się ogromna rzesza deweloperów, którzy tworzą niesamowite rozwiązania z wykorzystaniem właśnie modeli Mety. Kilka dni temu odbył się nawet w Singapurze hackathon, w którym Meta rozdawała granty dla deweloperów, którzy wykorzystują te modele. Można było tam spotkać rozwiązania do zarządzania finansami gdzieś w Azji, więc to jest dosyć ciekawe. Kolejnym z graczy, który jest oceniany najwyżej, zgodnie z benchmarkami, czyli z parametrami tych modeli, będzie Google. Część tych modeli jest darmowych, na przykład Gemma, pokażę Wam gdzie ją znaleźć i w jaki sposób akurat nie będziemy jej odpalać, natomiast one są darmowe. Gemini są to modele płatne, dostępne w Google Cloudzie. One będą też stosowane do zasilenia sztucznej inteligencji w wynikach wyszukiwania, więc to jest dosyć ciekawe, że Google, który co do zasady powinien być gdzieś w okolicy lidera rynku, jest dopiero na trzecim miejscu i też udostępnia modele za darmo. Kolejne są modele Antropica, które są płatne, ale poniżej są modele Mistral, które również są darmowe i możesz je wykorzystywać w swoich strategiach, infrastrukturach firmowych czy procesach. są całkiem niezłe. Także w ten sposób czytamy leaderboardy. Tu oczywiście mamy jakieś tam indeksy znormalizowanej jakości tych modeli. Co do zasady nie wchodząc w głębsze szczegóły, te indeksy potrafią nam ocenić, które są najlepsze. W poprzednich lekcjach również nauczyłem się czytać, co znaczą te wartości. Przypomnijmy sobie, są to rozmiary modeli. Czyli ten 450 miliardów parametrów będzie modelem największym, który jest za darmo i będzie doskonały na przykład generowania treści. Nawet sobie radzi w języku polskim? Te mniejsze modele raczej będą do szybkich akcji, typu na przykład chatbot, albo coś innego. Meta również udostępnia modele wizyjne, do których zaraz przejdziemy, czyli te modele zaczynają widzieć. To też jest bardzo ciekawe, to się wydarzyło stosunkowo niedawno. Okej, powiedziałem o mecie. Spójrzmy na rodziny. Na rodziny tych modeli. Na dzień dzisiejszy mamy dwa, znając prędkość rozwoju technologii pewnie zaraz będą kolejne. Rozróżniamy dwie rodziny. 3-1 są to modele, które służą do generowania treści i operacji logicznych. Jak się przyjrzymy, ten największy model, o którym przed chwilą wspomniałem, 405 miliardów parametrów, jest porównywalny, a w niektórych benchmarkach nawet lepszy, aniżeli modele od OpenAI to GPT-4, czyli troszeczkę starszy model, ale realizuje doskonale Nematronem, to jest model od NVID, czy też Sonetem, czy też z GPT-4O Mini. I jest za darmo. Dodatkowo mniejsze modele, gdzie mówiłem o na przykład o GEM-ie, czyli o Google, również rywalizują ze sobą i te modele okazuje się, że są najlepsze od mety. No i model, który 75-70 miliardów parametrów wydaje się być najbardziej kuszący z zastosowań SEO w porównaniu do konkurencji, czyli Mixtralla i GPT-4, GVT 3,5 turbo, bo wtedy tak, gdy były modele oceniane, wypada najlepiej. To jest pierwsza rodzina. Podsumujmy. Jeżeli chcemy mieć super jakość, na przykład naszej treści końcowej, wybieramy ten model największy, 405 miliardów parametrów, radzi sobie całkiem nieźle z językiem polskim. 70 miliardów parametrów jest to model, który jest do takich zastosowań normalnych, powiedzmy, jakiegoś tam przetwarzania szybkiego albo jakichś takich zastosowań niekrytycznych. Ten najmniejszy 8 miliardów parametrów jest to model, który możecie zastosować na przykład do szybkich akcji, chatbotów, klasyfikacji, kategoryzacji, czegoś szybkiego, lekkiego, gdzie nie potrzeba takiego rozwiniętego, powiedzmy, mózgu. Drugą rodziną modeli językowych od mety są modele oznaczone numerem 3,2. Co tu jest ciekawe, to jest to, że te modele zaczynają być wizyjne. Co znaczy? Znaczy tyle, że modele zaczynają widzieć. I okazuje się, że również darmowe modele wizyjne od MET-y są lepsze niż na przykład modele od Antropica Cloud, Cloud Haiku, czy też GPT-4O Mini, które również mają możliwości wizyjne. Do czego możecie zastosować to w swojej strategii czy w swoich aplikacjach? Chociażby do skanowania faktur, chociażby do sprawdzania jakichś statystyk, czy odczytywania zdjęć, czy dokumentów PDF-ów, czy dokumentacji wewnętrznej, która na przykład była zbudowana w PDF-ie albo wydrukowana. Do tego te modele mogą zostać zastosowane. Kolejną rodziną modeli, które sobie sprawdzimy, są to modele ze stajni Mistral. Mistral jest to francuska firma, która również udostępnia swoje modele za darmo i również zaczęły całkiem nieźle komunikować się w języku polskim. Rodzina jest mniej więcej taka sama. Modele małe, które widzimy po lewej, czyli ten Mistral 7B, Model duży, odpowiednik tej mety 70 miliardowej 8x777b. No i największy model, który będziemy stosować do generowania treści, jak widzimy, tutaj mamy napisane fluent in English, French, Italian, German, Spanish and strong in code. Dokładnie tak. Całkiem nieźle radzi sobie po polsku, możemy to stosować. I również Mistral ostatnimi dniami, ostatnimi czasy zaprezentował darmowe modele wizyjne, czyli po raz kolejny to, co zrobiła Meta, modele językowe zaczynają widzieć i możemy się z nimi komunikować obrazem i na przykład klasyfikować faktury, czy też inne rzeczy. Wartym podkreśleniem jest również to, że na polskim rynku całkiem dużo się dzieje w obszarze modeli językowych. Mamy polskiego bielika. Jest to inicjatywa, która nazywa się Spichlerz. Zebrało się kilku inżynierów, innowatorów. Stworzyli polski model językowy. Uwaga, oparty na polskich danych treningowych. Zostało to zrobione we współpracy, jeśli dobrze pamiętam, z akademią górniczo-chutniczą. Jak widzimy przed oczami, pamiętamy takie superkomputery na uniwersytetach, to właśnie na takim superkomputerze ten model został nauczony w Polsce. Na polskim korpusie językowym, również w tej lekcji pokażę Ci, jak ten model odpalić na laptopie. Testowałem go, dobrze sobie radzi z klasyfikacją języka polskiego. także bardzo interesujące z perspektywy zastosowań SEO czy też zastosowań marketingowych całkiem nieźle radzi sobie z tworzeniem treści w języku polskim, więc chyba szkoda byłoby przejść koło bielika obojętnie aktywnie udzielają się chłopaki na Linkedinie, co chwilę anonsują nowe wersje warto na niego patrzeć, oczywiście on nie dojeżdża jeszcze swoim rozmiarem i jakością do modeli tych największych, powiedzmy, ze stajem OpenAI, ale to nic. Dlaczego? Dlatego, że w korpusie treningowym, GPT-4 na przykład, ilość danych o języku polskim, czyli ilość danych korpusa językowego polskiego, stanowi pewnie jakieś może 1%, a może mniej. Nie mamy takiej wiedzy, ale na pewno są to marginalne ilości. Jeśli chodzi o Bielika, to cały korpus językowy jest po prostu polski, przez co Jako procesor logiczny w języku polskim sprawdzi się dużo lepiej, więc fajnie mieć go na oku i być może stosować w waszych strategiach, procesach, budowie. W kolejnych lekcjach pokażę ci, jak orkiestrować agenty AI, pokażę ci także, jak to zrobić z Bielikiem. No i nowość, mamy taki model Plum. Plum jest modelem, który jeszcze nie został stworzony, natomiast polskie uczelnie, zdaje się Politechnika Wrocławska oraz Politechnika Poznańska pozyskały bardzo duży grant na budowanie kolejnego polskiego modelu językowego na poziomie akademickim, który będzie wykorzystywany w sektorze publicznym, ale też w sektorze prywatnym. Więc to się dzieje, jeszcze to nie działa, będzie działać. Miejmy na oku pluma, to jest kolejny model. Jeszcze nie wiem, czy będzie open source, ale chciałem to pokazać jako kontrę do Bielika, który jest modelem polskim. OK, poszukajmy sobie źródeł wiedzy i w jaki sposób nawigować się w tym świecie. Istnieje społeczność modeli open source, nazywa się HuggingFace. HuggingFace.co, w Google jak sobie wpiszemy, HuggingFace znajdziemy. Okej, kilka zakładek, które nas powinny interesować. Przede wszystkim zakładki z modelami. Znajduje się tu powiedzmy kolejny leaderboard, czy też to jest posartowane po trendingu. Ten leaderboard, który Ci pokazałem na początku, jest posortowany po jakości. Tutaj mamy sortowanie po, przepraszam, pozamykam trochę niepotrzebnych zakładek. O, bielik. Właśnie zaraz sobie do niego jeszcze wrócimy. To sobie przewinę w ten sposób. Po trendzie. Co w danym momencie trenduje i co społeczność modeli open source, czy też deweloperów na świecie w danej chwili wykorzystuje. I widzimy, co teraz jest powiedzmy w gazie. Dzisiaj nagrywam ten film, jest chyba 10 czy 11 października. mamy OpenAI, Whisper Large jest to model, który przetwarza dźwięk w tekst. On jest również za darmo więc całkiem kusząca perspektywa, w której dzięki temu za darmo możemy na przykład sobie podsumowywać YouTube'y czy pożyskiwać treści z telewizji czy z YouTube'a, no nie? Kolejnym trendującym jest od NVIDIA Flux. To Damian pokazywał w kontekście generowania zdjęć. Ten model jest również za darmo wiesz, nie mówiłem na samym początku w kontekście Mety czy Mistrala o tego typu modelach, ale Flux też jest za darmo. Apple również daje modele za darmo. W jednym z AI Launchy mówiłem o tym, są to modele do mierzenia głębi obrazu, czyli jeżeli wyobrazimy sobie samochód autonomiczny, który jedzie i ma zdjęcie z kamery z przodu, to ten model jest w stanie określić głębiej, co tam się znajduje z tyłu. Być może pomoże to samochodem autonomicznym się poruszać. No i widzimy to, co się dzieje właśnie, czyli lame od byśmy się tutaj troszeczkę poscrollowali w dół. Znajdziemy tego dużo, dużo więcej. Możemy też poszukać naszego bielika. Bielik oczywiście się również tutaj znajduje. Zobaczmy. Półtora tysiąca ludzi już sobie ściągnęło bielika, więc pewnie fajnie się tym pobawić i również na tym się skupimy. To, co jest ciekawe, to są robisz datasety. To jest zaawansowane. O tym możemy rozmawiać przy fine-tuningu, natomiast możemy tutaj pozyskać na przykład polskie datasety do fine-tuningu, czy do pracy z modelami językowymi po polsku, ponieważ modele open-source, co na końcu podsumuję, również mogą być fine-tuningowane, czyli dostrajane do języka polskiego, czy też dostrajane do jakiegoś tam zadania. Kolejna ciekawa rzecz, jest to troszeczkę już w obszarze infrastrukturalnym, ale będąc na Hugging Face'ie, chciałbym o tym Ci powiedzieć. Zobacz, jeżeli mamy na przykład model Mety, Lame, to to jakiś tam najmniejszy, uczymy się czytać, powtarzamy, 1 miliard parametrów, malutki model, proste zadania, teksty, proste akcje. Możemy te modele również tutaj u nich odpalić. To jest całkiem ciekawe. Czyli wybieramy sobie z listy na przykład, okej, chcemy korzystać z mety, chodzimy sobie Diplo i proszę bardzo, mamy możliwość infrastruktury u nich bezpośrednio, mamy możliwość sobie kupić jakąś tam dedykowaną infrastrukturę, Azure'a, Cloud'a, nawet w Amazonie możemy to postawić, więc co do zasady, ten Hugging Face jest całkiem obszerne miejsce, w którym nie z jednej strony znajdziemy modele, poczytamy o nich, ale jednocześnie możemy je odpalić. Więc Hugging Face jest super i polecam Ci go pod rozwagę. Wróćmy teraz jeszcze chwileczkę do Bielika. Widzimy, tutaj jest strona główna bielik.io, 11 miliardów parametrów, dostępny na Hugging Face. Super. Chłopaki mają, jeśli dobrze pamiętam, społeczność na Discordzie, można z nimi rozmawiać o tym modelu, śledzić newsy i tak dalej. Dobra, co my tu mamy dalej? Kolejne źródło wiedzy, OpenRouter. OpenRouter, adres openrouter.ai Z jednej strony jest to infrastruktura, o tym opowiem troszeczkę później, ale z drugiej strony jest to miejsce, w którym możemy po raz kolejny poszukać modeli, czy sprawdzić, co w trawie pisz, czy jakie są fine tuningi modeli, czy kto czego używa. No i mamy, powiedzmy, weźmy sobie browse, czyli podzimy w jakiś tam katalog i zobaczmy, tutaj mamy całkiem niezłe filtrowanie. Po tych modelach możemy się poruszać na przykład na serię modeli, czyli ten Gemini, lama i tak dalej, poszukamy, czego sobie potrzebujemy. Co jest ciekawe, tam są gotowe fine tuningi, czyli na przykład możesz sobie poszukać, okej, że jeżeli wiesz, że twoim zadaniem jest na przykład kategoryzacja jakiejś tam akcji, to te fine tuningi zazwyczaj tam się znajdują, no nie, czyli będziesz miał na przykład model mety przygotowany do konkretnego zadania, którego ty potrzebujesz. Znajdziesz to na Hugging Face'ie i znajdziesz to również na Open Routerze, do czego cię zachęcam. Wejdźmy na przykład sobie na tą na tą gemmę. Mamy informacje, gdzie one się znajdują. Fantastycznie infrastruktury. Pokażę Ci dalej, jak tego korzystać. To, co Cię może zainteresować, to jest również informacja o tym. To jest taki grubszy temat, ale powiedzmy okej. To jest informacja o tym, ile tokenów dany model przetwarza w tej infrastrukturze. Im więcej, tym jest części użytkowany, więc sobie można posortować i w ten sposób wybrać, co w danej chwili drabieloperzy na świecie używają w największym wolumenie. no to pewnie to jest jakby taki szybki, tak jak na Allegro, sortuj po popularności, kupujesz pewnie pierwsze, drugie, trzecie, bo tam dużo ludzi kupiło, czyli jest dobrze. Podsumujmy, open router, kolejne miejsce do poszukiwania modeli do rozglądania się za gotowymi fine tuningami, ale również gotowa infrastruktura, gdzie sobie dzisiaj ten model przetrenujemy. Dobra, co my tam mamy dalej? Olame. Kolejna świetna rzecz. Z jednej strony jest oprogramowanie, które pokażecie, jak dzisiaj odpalić, na komputerze, jak zrobić model językowy. Z drugiej strony po raz kolejny jest to źródło wiedzy o modelach językowych, open source'owych, które od razu możesz zainstalować u siebie na komputerze. Czyli mamy oczywiście tutaj troszeczkę mniej informacji o tych, mniej filtrów, natomiast mamy tą lamę, o której mówiłem z modelami wizyjnymi, detale, informacje, benchmarki, jak odpalić, parametry, to jest spoko i jeżeli sobie wpiszemy bielik, no to my go znajdziemy. Także fajnie, że w tych światowych rozwiązaniach, bo chyba O-Lama jest największym na świecie tego typu rozwiązaniem do utrzymywania modeli językowych, znajdują się już również polskie produkcje. No niestety to widzimy na Hugging Face'ie, było półtora tysiąca pobrań, na Lamy jest 249. No ale okej, Polacy są dostępni na tego typu infrastrukturach. Wróćmy do prezentacji. Przed przejściem do infrastruktur chyla podsumowania. Wiemy czym są modele językowe open source. Wiemy, jak to są liderze rynku, jak się poruszać w tym świecie, gdzie poszukiwać, co znaczą parametry. Chodźmy do infrastruktur. Bo to jest troszeczkę tak. Jeżeli idziemy do OpenAI, czyli do GPT-4, czy do wolnego modelu, który tam stosujecie, oni wam dają infrastrukturę. Modele open source charakteryzują się tym, że wy musicie gdzieś umieścić. To jest dobre i niedobre. W opcji open AI jest to no-brainer. Idziesz, dzwonisz, masz, płacisz, koniec. End of story, załatwione. Tu też tak możesz, zaraz ci pokażę. Ale to jest ciągle wysyłanie danych gdzieś. Możesz te modele utrzymać również na własnych maszynach, przez co twoje dane, dane wrażliwe, dane biznesowe, GDPR i wszystkie inne historie, są po prostu bezpieczne. Jest kilka fajnych infrastruktur, które używam. Pokrótce przeklikamy się przez nie bez głębszego wejścia. Może wejdziemy w jedno. Wszystkie są podobne. Różnią się co do zasady pieniędzmi, które będziemy płacić po raz kolejny za tokeny, które wiemy, co to są tokeny i dlaczego trzeba promptować w języku angielskim, żeby płacić mniej. Można też płacić za jakieś tam godzinę. Najfajniejsza infrastruktura, bo przynajmniej bardzo przyjazne, jest to AnyScale. Gdzieś powiedziałem, nie mam mieć otwartą. Platforma AnyScale, jak widzimy tutaj ogromne firmy, Intel, Airbnb, OpenAI, nawet tam kupują od nich zasoby. My również możemy. Tam znajdują się modele, głównie z Facebooka, ale nie tylko. Sporo różnych rozwiązań. Zachęcam sobie wejść na AnyScale i po prostu nawet się zapoznać, bo uważam, że tego typu firmy, Jest to przyszłość startupów na świecie, bo tych infrastruktur będzie rosnąć wraz z ilością zapotrzebowania na moc obliczeniową na świecie. Mamy tam wiele możliwości, fine-tuningu modeli, stosowania tych modeli, deploya modeli. Mamy taki sam playground praktycznie jak w OpenAI, gdzie możemy te modele testować, więc jakby any scale polecam się tym zainteresować. Na moich slajdach, do których nie będę wracał, następną pozycją jest Together AI, jest to tożsama infrastruktura różni się co zasady ofertą natomiast jak widzimy na tym kolażu tutaj znajdują się wszystkie rzeczy, których potrzebujemy mamy Lamy, najnowsze modele mamy Mistralę mamy, co my tu jeszcze mamy? Snowflake, Stable Diffusion czyli mamy zdjęcia, kod Lama co do zasady tam praktycznie jest wszystko co nowego się pojawia, tam się pojawia kupujemy sobie od nich moc obliczeniową mają również playground, jest to całkiem tanie, więc Together AI jest kolejną polecajką, którą można wziąć pod uwagę szukając infrastruktur do modeli open source na świecie. Następną jest perplexity, czyli to jest to fajne narzędzie, które omawiamy do przeszukiwania treści. Oni też dają swoją infrastrukturę, gdzie można utrzymać te modele i zdecydowanie obniżyć koszty. Jak sobie poszukacie perplexity AI, macie możliwość API, tu są wszystkie informacje, których potrzebujecie oraz perplexity ma całkiem niezłą dokumentację. Widzimy tutaj karty, wiadomo, okej, ale mamy też playground, gdzie możemy się tymi modelami pobawić i zobaczmy. Tutaj po raz kolejny z Perplexity mamy modele Lamy, Sonoro Large i to chyba jest istotna informacja. Cała Perplexity, które jest cudowne z perspektywy SEO do pozyskiwania danych, przetwarzania danych, doszukiwania danych, jest oparte o swoje fine tuningi Lamy i działa fantastycznie, więc jakby też możecie z tego używać. Zapytamy, hi, how are you? no i w Perflex City akurat to jest jakaś dziwna historia, a jakiś kontekst ma przytrzymane, no nie bardzo działa i grok jest ostatnią taką infrastrukturą, którą chciałbym podkreślić, dlaczego? dlatego, że na dzień dzisiejszy mówię na dzień dzisiejszy, bo dużo się dzieje i dużo się anonsuje, jest najszybszy na świecie więc jeżeli wasze aplikacje wymagają bardzo szybkiego przetwarzania, wyobraźmy sobie dzwoni telefon i jest chatbot czy jakiś tam asystent, który dzwoni. To musi dziać szybko i właśnie grok dostarcza tego typu rozwiązania. Zalogujmy się tam. Mamy możliwość glock platform, jeśli jesteśmy duzi i chcemy tam sobie troszeczkę wykupić. Mamy możliwość właśnie platform i zaraz się gdzieś tutaj zalogujemy. Gdzie oni mieli ten link? Właśnie. Product overview platform i oni tu wicielibili ten konsol. Start loading now. Jest. OK. I mamy tożsame playground z OpenAI, gdzie możemy z nimi rozmawiać. Jak spojrzymy na... To jest takie podsumowanie już w tym momencie, jakby wychodząc z infrastruktur. Wiemy, że meta jest najlepsza. Mistra, Google daje. I spójrzmy, jakie modele dają infrastruktury na świecie. Gemma. Google. Z Groka. To są fine tuningi, mety, czyli to samo, co Perplexity robią. Dokładnie to samo. Z Hugging Face'a Whisper, czyli to, co już pokazałem do przetwarzania dźwięku. Jak dzwonimy czy coś takiego, to oni to robią najszybciej. Meta. Mistral. Open AI, czyli po raz kolejny to samo. I koniec. I tak będzie na większości infrastruktur. Modele, które spotkacie, to właśnie będą to największe. I okej. Działa. Piszmy sobie jakiś tam user prompt. gave me informations about bloggers. To nie jest lekcja z finding, z prądu inżynieringu. Jak widzieliśmy, momentalnie pojawiła się odpowiedź. To jest właśnie charakterystyka tych modeli. Spójrzmy jeszcze raz, że jest odpowiedź. Jeżeli twoje aplikacje potrzebują działać naprawdę szybko, to grok jest dla ciebie najlepszy w rozwiązaniu. Dobra, wróćmy do prezentacji. Ja już to sobie ładnie omówiłem. Infrastruktura jeszcze jedna, czyli ten open router. Wróćmy do niego. Open router AI. Teraz open router w kontekście infrastrukturalnym. Na początku rozmawialiśmy jako źródło wiedzy, poszukiwania, modeli. OK, fajnie. Natomiast oni zrobili coś, co w mojej opinii jest przynajmniej warte uwagi, jeśli nie rewolucyjne. Znajdźmy jakiś większy model, powiedzmy, wpiszmy, co byśmy tutaj sobie wpisali blablabla lama metalama instruk, okej, nikt będzie zobaczmy, znaleźliśmy sobie ten model powiedzmy będziemy rozmawiać z lamą to nie jest to o tym co znaleźliśmy natomiast to co oni wam dostarczają z punktu widzenia infrastruktury to jest to, że oni wiedzą że ten konkretny model czyli nasza lama znajduje się na dzień dzisiejszy w raz, dwa, trzy, czterech infrastrukturach i na przykład ma wolne moce obliczenia. I teraz tak, zobaczmy. Znaleźliśmy model i mamy infrastruktury. Powiedzmy, że on jest w Deep Infra, w Together AI, które wam pokazałem, Fireworks, Hyperbolic. Tych infrastruktur naprawdę jest dużo na świecie i będzie jeszcze więcej. I teraz tak, oni potrafią skumać w ten sposób, że jeżeli na przykład, załóżmy, ok, Hyperbolic ma najtaniej w danej chwili, bo ma wolne moce obliczeniowe, to oni wyślą was do hyperbolik. No nie? W momencie, w którym uznają, ok, hyperbolik jest obciążony, bo coś tam albo zaczyna być drożej, to oni to samo wyślą wam do deep infra. Czyli to, co oni stworzyli dla infrastruktur i modeli open source'owych, to jest taki jakby jeden ustandaryzowany pośrodku konektor, miejsce, load balancer, no nie wiem. Jeden standard, który rozsyła po całym świecie tam, gdzie są wolne moce obliczeniowe i tam, gdzie jest najtaniej. Więc to jest całkiem fajne. zalogujmy się tam, ja to pokażę trochę szerzej przy orkiestracji agentów AI i zobaczcie, gdzieś tutaj mamy credits, ja tutaj jestem najbogatszy na świecie, wiadomo, 9,5 dolara, okej, ale zobaczmy usage, jak się tym bawimy jakiś czas temu, zobaczmy, mamy tą metę, jakoś tam buduje system do tworzenia artykułów lifestyle'owych wokół tego i zobaczmy, część powiedzmy promptów poszła do infrastruktury hiperbolic, bo hiperbolik w danej chwili miał moc obliczeniową, a w tym samym procesie część pochodzą do Dibi, Infra Dibi, Infra Nowita, Avian i on tak miesza. Bardzo kuszące rozwiązanie, bo zdejmuje nam ciężar z głowy wyboru, wybiera tam, gdzie jest moc. Przyszłość takich rozwiązań wyobrażam sobie trochę jak wiecie, zielona energia na przykład z fotowoltaiki jest jak świeci słońce, czyli potencjalnie można by wysłać nasze żądania AI do miejsc, gdzie świeci słońce w danej chwili, potencjalnie jest tam zielona energia z fotowoltaiki, na pewno takie coś będzie. To jest 100%. I ostatnia infrastruktura, tutaj też pokażę troszkę więcej, to jest RunPod. W RunPodzie mamy dwie opcje, dedykowaną, gdzie sobie odpalimy zaraz nasz model językowy, ale również możemy też wykupić jakieś tam, powiedzmy, end pointy, czyli miejsce, gdzie nasz model będzie utrzymany i to jest już totalnie prywatne w tym momencie, bo te hiperboliki, DB Infra, Together AI, AnyScale i tak dalej, to gdzieś to idzie, a tu mamy zamknięte tylko dla nas. Czyli właśnie tutaj mamy aspekty bezpieczeństwa. Zaraz sobie do tego dojdziemy. Podsumujmy. Infrastruktury alternatywne służą przede wszystkim do tego, żeby obcinać koszty. Pojawiają się rozwiązania takie jak open router, które już dystrybuują nam moc na świat i pozyskują chociażby, no załóżmy, że powiedzmy tą zieloną energię z fotowoltaiki i tam akurat nam nasze modele utrzymują. I to działa. I wierzę w to, że to będzie ogromny trend. Natomiast zastanówmy się do czego warto, nie? Bo takie proste myślenie, ok, idę do OpenAI, oni są najlepsi na świecie, ok, spoko, ok. Ale jednak być może warto. Pierwsza rzecz, która dla mnie jest dosyć istotna, są to operacje klasy backend, czyli powiedzmy, ok, naszym zadaniem jest kategoryzacja słów kluczowych. Naszym zadaniem jest dopisanie produktów do kategorii w sklepie internetowym. Czy naszym zadaniem jest, nie wiem, klasyfikacja produktów, czy ten produkt, na przykład ta sukienka ma czerwony kolor, czy tam zielony, czy tam jakiś tam tego, tego. To się nie dzieje z przodu, czyli tego człowiek nie widzi. To się dzieje w naszej infrastrukturze, w naszych systemach. To zdecydowanie modele open source można tam stosować w zupełności. Są wystarczające, świetnie sobie radzą, przez co obcinamy koszty, mamy aspekty bezpieczeństwa i wiele innych historii. Do tego na pewno warto. Kolejna rzecz. Przygotowanie danych do GPT-4. W kolejnych lekcjach pokażemy Wam, jak budować treść z wykorzystaniem modeli językowych i każdy profesjonalny powiedzmy, nie wiem, workflow czy sposób generowania treści, jest to wieloetapowe przetwarzanie danych przez sztuczną inteligencję, gdzie na samym końcu znajduje się najlepszy obecnie istniejący model na świecie po to, żeby ta treść była najlepsza. End of story. Dosyć proste. Natomiast po środku znajduje się proces, który może mieć na przykład 10-15 kroków i tam już nie trzeba tak doskonale, no nie? Więc tam te modele mają zastosowanie w zupełności wystarczą. Ja właśnie tam je widzę i tam stosuję. Optymalizacja kosztów. Jeżeli na przykład mamy do kategoryzacji, wyobraźmy sobie Senuto. Senuto ma do kategoryzacji powiedzmy, nie wiem, 100 milionów słów kluczowych w Polsce. Więc jakby kupując tą moc od OpenAI to jest nierealne. Używając modeli open source to praktycznie może nic nie kosztować. Więc jeżeli wasze procesy są drogie albo chcecie obniżyć koszty, bo już zaczyna być droga, to na pewno jest to dobre rozwiązanie, żeby w tym kierunku pójść. Ogromna skala przetwarzania, to jest kolejna rzecz. Czyli to już co powiedziałem, 100 milionów słów kluczowych w Senuto, czy nie wiem, 50 tysięcy produktów w waszych sklepie internetowym, razy ileś wariacji, czy to operacji, czy tak dalej. Na pewno te modele się do tego sprawdzą. I ostatnia rzecz, bezpieczeństwo danych. Po prostu. Na dzień dzisiejszy wysyłając cokolwiek do infrastruktur zamkniętych, czyli do OpenAI, do Antropica, czy powiedzmy do Google, to gdzie, gdzie, gdzieś. I obiecują, że nic z tym nie zrobią, ale wiemy jak z tym jest. Nie mamy jeszcze regulacji w Unii Europejskiej, dopiero się pojawią. i jeżeli wasze dane są kluczowe i nie możecie sobie pozwolić, że na przykład jakiś model będzie na niej trenowany, to się może wydarzyć. No właśnie, to musicie sobie odpowiedzieć sami. Na pewno te modele dają wam walor bezpieczeństwa, co wam pokażę, jak zaraz zrealizować. Kolejna rzecz. Point Tuning. Możecie wziąć dowolny praktycznie model open source'owy i go przystosować do konkretnego zadania. Czyli na przykład wiemy, że mamy lamę od mety, ona tak sobie średnio po polsku, to my ją douczymy, nie? Po polsku. Wrzucimy na przykład artykuły z Forbes'a, Damian pokaże w kolejnych sekcjach, jak fine-tuningować modele. Te modele wszystkie można fine-tuningować do konkretnych zadat, do języka polskiego. Fantastyczna opcja. I coś, co się pojawia, destylacja modeli. Nie widziałem tego jeszcze w kontekście modeli open-source'owych. To się pojawi. To się pojawi. Nie wiem, w którym momencie kursu jesteście, znaczy kiedy go oglądacie. Na pewno to się być może już pojawiło, albo dopiero pojawi. Czyli z dużych modeli open-source'owych będą robione malutkie modele do konkretnych akcji. Na przykład tego największego, modelu, lamy, zostanie stworzony jakiś malutki modelik, który będzie realizował jedną akcję, będzie super tani, będzie można go odpalić nawet na telefonie czy na zegarku. To się na pewno wydarzy. Okej, dzięki. W tej lekcji to wszystko. W kolejnej lekcji pokażę Ci, jak wykonać deployment lokalnie, czyli zainstalować model językowy na Twoim komputerze oraz zrobimy sobie ćwiczenie, w którym pokażę Ci, jak zainstalować model językowy na maszynie zdalnej, powiedzmy na chmurze czy na jakimś serwerze dzierżawionym. Do usłyszenia. 
Dzień dobry. Do tego ćwiczenia posłużymy się narzędziem O-Lama. O-Lama w mojej opinii jest najlepszym narzędziem obecnie na świecie i najłatwiejszym do instalacji modeli. W poprzedniej lekcji pokazałem Ci, jak nawigować po tym środowisku, po O-Lamie, czyli przeglądać modele, wybierać te modele, wiemy jakie są najlepsze, czyli powiedzmy te odmety Lama, mamy dostęp również do modeli od Google'a, czy też dostęp do modeli od Microsoftu w Lamy. Okej, ale teraz skupimy się na tym, jak sobie odpalić model na swoim komputerze. Pierwsza rzecz, po prostu ściągamy. Jak widzimy tutaj jest informacja o tym, że to oprogramowanie jest dostępne na Maca, na Linuxa, na Windowsa, czyli co do zasady instalujemy wszędzie. Nie będę przekazywał procesu instalacji, on jest dosyć prosty. Ściągamy, instalujemy dalej, dalej, koniec. No i po zainstalowaniu Lamy, ja akurat pracuję w środowisku Macowym, ona się znajduje tutaj u góry, jest już odpalona, możemy sobie z nią pracować. I teraz tak, lama nie ma żadnego interfejsu, to jest bardzo lekkie rozwiązanie, w którym będziemy pracować w terminalu, nie bójcie się, to jest dosyć proste, command spacja, piszemy terminal i otwiera nam się matryk. Ja troszeczkę przybliżę tutaj, czy powiększę, żebyśmy lepiej widzieli. OK. Co piszemy? Po prostu OLAMA. OK. I tu mamy zestaw funkcji, które OLAMA będzie dla nas realizować. Czyli mamy jakieś tam informacje o tym, jak stworzyć modele, odpalić modele, zastopować modele, wylistować modele, usuwać itd. Jest to taka dosyć prosta konfiguracja. Ja na potrzeby tego ćwiczenia sobie już, tutaj mamy list, czyli w tym momencie podświetlę modele, które już mam zainstalowane na swoim komputerze. Ja na potrzeby tego ćwiczenia ściągnąłem je wcześniej na swój komputer, po to, żeby przyspieszyć tą lekcję. No i widzimy, że mamy zainstalowaną już na moim komputerze jakąś tam lamę 1B, najnowszą lamę. Mamy z tego spichlerza bielika, czyli ten polski model językowy, o którym już również wspomniałem. Zaraz się nim pomyjmy. No i jakąś kolejną lamę. To, co dla Ciebie tutaj jest ciekawą informacją, to jest jakby size, no nie? Czyli zobaczmy, czyli ten bielik, polski model językowy, waży 6,7 giga. Jest to dla Ciebie informacja, że tyle powinieneś mieć giga RAM-u w swoim komputerze, żeby ten model odpalić. Ja tutaj akurat na swoim komputerze mam 16 GB, więc te modele bez problemu sobie odpalimy. Chodźmy do jakiegoś innego. Pokażę Ci różnicę, jakie są rozmiary. Okej, jesteśmy w tej najnowszej LAM-ie. Zobaczmy. I największy model, V-Mall. Największe modele, tutaj akurat są te mniejsze modele. Chodźmy do tej starszej Lamy, czyli sobie wchodzimy w model. lama, to są większe modele, bo te najnowsze są wizyjne, one są troszkę mniejsze, te starsze modele z rodziny 3.1, o której mówiłem, są modelami większymi. No i zobacz, największy model, ten 405 miliardów parametrów, który konkuruje z GPT, plik waży, sam model waży 229 giga. Jest to informacja, ile potrzebujesz mieć pamięci RAM właśnie na swoim komputerze. Ten model, który jest w zupełności wystarczający do tego, żeby generować treści, waży 40 giga, czyli pasuje mieć Pewnie 64 GB RAM-u, najlepiej na karcie graficznej. Dla tego modelu, dla największych modeli, to odpowiednio 256 GB. To pokażę na RAM-podzie, jak sobie tym zarządzić. OK, czyli jesteśmy w miejscu, w którym mamy zainstalowaną LAM-ę. Jest ona już na naszym komputerze, jesteśmy w terminalu. Ja akurat mam ściągnięte modele, ale sobie zrobimy ćwiczenie, poszukamy na przykład, OK, skupimy się na tym modelu. Czyli chcemy sobie zainstalować na moim komputerze tę LAM-ę z rodziny 3.2 z 3 miliardami parametrów. Jak to robimy? Wyszukujemy w środowisku olamy nasz ukochany, ulubiony model, który chcemy sobie zrobić. Wybieramy jego rozmiar. Nas interesuje akurat 3 miliardy. I zobaczcie. Tutaj znajdują się wszystkie parametry modelu, rozmiary i tak dalej. Cały readme i informacje, co to jest, benchmarki. Także możemy sobie poczytać. Na potrzeby odpalenia interesuje nas co zasady to. Czyli mamy gotową komendę, czyli olama run i nazwa modelu. Zobaczcie. Jak sobie wejdziemy do naszego terminalu, po prostu robimy kopiuj, czyli tu. tutaj wklejamy, ok no i ten model nam już się odpala dlatego, że on był wcześniej przeze mnie ściągnięty na mój komputer no i mamy, działa, zobaczmy już możemy sobie z nim rozmawiać zapytajmy what is hamburger? no i mamy już sobie rozmawiamy, ten model jest na moim komputerze jak widzimy całkiem sprawnie działa, mimo że mamy tylko 16 giga RAM-u, to wynika z tego, że te modele są małe, dlatego uważam, że przyszłością właśnie są małe modele, lepiej wytrenowane, lepiej dostosowane do konkretnych zadań, bo po prostu koszty przetwarzania tych modeli będą dużo, dużo tańsze. No jak widzimy, odpowiada mi, czym jest hamburger. Dobra, jeżeli chcielibyśmy sobie odpalić bielika, czyli nasz ten polski model językowy, nic prostszego. Piszemy po prostu bielik. Okej. On się znajduje w najnowszej wersji. Jak widzimy, to są te wersjonowanie i tak dalej. To już sobie umiemy to czytać. To jest najnowsza wersja. No i I po raz kolejny. Jest na Olamie. Fantastycznie. Readme, wszystkie rzeczy. Kopiujemy sobie informację o Bieliku. Odpalamy w naszej Olamie. On się ładuje. To chwileczkę potrwa. W tym momencie mam trochę załadowanych tych rzeczy na swoim komputerze. Więc dajmy mu chwilę. On teraz sobie robi ładowanie. Dobra. No i mamy odpalonego na moim komputerze Bielika. Czyli model językowy stworzony przez Polaków na polskim korpusie językowym, więc zagadajmy z nim po polsku. Co to jest hamburger? Jak widzimy, on na chwilę sobie pomyśli, bo to jest troszeczkę większy model. No i leci. Bielik działa. Podsumujmy. Ściągamy lamę. Raczej o lamę. Instalujemy na naszym komputerze. nawigujemy sobie do modelu, który nas interesuje. To są polecane przeze mnie i przez nich, jakby skupmy się na nich. Kopiujemy, odpalamy, działa. W kolejnych lekcjach pokażę również, jak orkiestrować te modele w jakieś procesy czy w jakieś profesjonalne rozwiązania. Na chwilę obecną ta wiedza, jak to odpalić na swoim komputerze jest dla nas wystarczająca. Ok, teraz pokażę Ci, jak odpalić model językowy na, powiedzmy, serwerze czy w jakiejś tam infrastrukturze chmurowej. Do tego ćwiczenia posłużymy się runpodem. Jest to, jak widzimy, all-in-one cloud. Bardzo lubię to rozwiązanie, ponieważ oni mają dosyć ciekawe podejście, w którym zarówno mają swoje serwery i swoje procesory logiczne, procesory graficzne, ale również mają podejście marketplace'owe, w którym można kupić procesor, czy komputer, czy kartę graficzną od kogoś z rynku. Czyli załóżmy, że ty masz 10 kart graficznych u siebie w biurze i chcesz je sprzedać, no to tutaj jest właśnie taki marketplace dzięki temu możemy kupować to dużo taniej. Dodatkowo, jakie są różnice między podejściem klasycznym? Powiedzmy, idziemy do OpenAI i w OpenAI płacimy za tokeny, czyli pracujemy, pracujemy, pracujemy, pracujemy, pracujemy i za ilość przetwarzanych danych po prostu zapłacimy. W tym podejściu zapłacimy za godzinę, czyli jeżeli mamy ogromne zadanie, powiedzmy, naszym zadaniem jest, tak jak powiedziałem, powiedzmy przykład, skategoryzować 100 tysięcy czy 100 milionów fraz kluczowych w Senuta, no to pewnie płacąc za tokeny zapłacimy bardzo długo. A tutaj możemy odpalić maszynę na 7 dni, zapłacić za 7 dni użytkowania, wykonać nasz proces, wyłączyć, nie płacimy. Jest to bardzo kuszące rozwiązanie i w sumie też zamknięte, ponieważ te maszyny są tylko nasze, nikt tam nie ma dostępu i w sumie z tym pracujemy. Dobra, pokażę Wam, jak to odpalić. Powiedzmy, w tym momencie się zalogowaliśmy do Rampoda, Wy się zalogowaliście, podaliście dane Waszej karty i przeszliście weryfikację w standard. Ja mam jeszcze na koncie 7 dolarów, więc nam starczy. I mamy dwa podejścia. Pierwszym podejściem jest serverless, czyli jakby serverless w tłumaczeniu to jest po prostu bezserwerowe. I tam też możemy sobie odpalać nasze modele. Dosyć proste, nie będziemy się na tym skupiać, natomiast po prostu dajemy link do Hugging Face'a, który omówiłem w poprzedniej lekcji, czyli powiedzmy Hugging Face. Interesuje nas model, powiedzmy no to skupmy się na tej mecie już. Popiecmy ten. Kopijemy sobie adres, wchodzimy tutaj, jest adres skopiowany, Haging to jest token, tutaj musimy się zalogować, to jest akurat mój token, no i tam jakieś konfiguracje, i tak dalej, i tak dalej, kupujemy moc, dostajemy po prostu endpoint, czyli API, gdzie mamy już nasz model językowy tylko dla nas i sobie z nim pracujemy. Natomiast nie będę tego pokazywać, to jest bardzo proste rozwiązanie, skupimy się na rozwiązaniu pods. Rozwiązanie pods jest to rozwiązanie, w którym kupujemy jakby swoją maszynę gdzieś na świecie, gdzie ktoś ma wolne moce obliczeniowe i tam możemy utrzymać nasz model językowy tylko na nasze potrzeby, optymalizując koszty. Czyli co? Wchodzimy sobie w pods, dajemy sobie deploy, dzięki temu jesteśmy już w ich marketplace'ie. To, co widzimy na liście, jest to spis kart graficznych czy procesorów obliczeniowych, które są dla nas dostępne, czyli ten H100, na przykład to jest jakaś bardzo droga kartograficzna, RTX to jest jakiś tam GeForce RTX od NVIDIA, które możemy wynająć w cenie 0,69 dolara za godzinę. Czyli jak nasz proces jest w 24 godziny, to jedziemy pod sufit, realizujemy, wyłączamy i płacimy mało. To, co jest dla mnie fantastyczne, jest to właśnie to Community Cloud. Dużo tańsze. Są to procesory, karty graficzne u kogoś, przez co one są po prostu tańsze. I skupimy się na procesorze, powiedzmy ten H100. H100 jest teraz jedną z najlepszych kart graficznych procesorów obliczeniowych na świecie. I możemy sobie wybrać na przykład do naszych zastosowań dużo mniejszych. Jeżeli mamy mały model, to idziemy na przykład w jakiś tam tani procesor, powiedzmy ten RTX 3090, czyli to właśnie tam GeForce RTX 3090, to jest bardzo tani. Jeżeli mamy duże potrzeby, no idziemy do dużych procesorów. No to okej. Wybieramy sobie jakiś procesor, który nas interesuje i wchodzimy sobie w templatki. Zobaczcie. W templatkach mamy bardzo dużo możliwości instalacyjne, jakieś tam TensorFlow-y, jakieś tam PyTorche i wiele różnych rzeczy, które możemy zastosować, czy to do treningu modeli, czy do innych rzeczy, nawet serwer na Ubuntu postarimy. My się opieramy o Lame, czyli piszemy sobie Olama. I mamy Olama Teamplate, czyli tą Olamę, którą zainstalowaliśmy wcześniej na swoim komputerze, po prostu zainstalujemy na czyimś komputerze. Dobra, mamy wybraną Olamę. I zobaczmy, mamy dwie opcje. Jedną opcją jest On Demand. Opcja On Demand jest to opcja, w której ten procesor jest tylko i wyłącznie dla nas i nikt nam go nie zabierze i za niego płacimy. Ale mamy fantastyczną opcję Spot. Opcja Spot polega na tym, że jeżeli mają wolne moce, nikt tego nie używa, to sprzedadzą Wam za połowę ceny, mieszają te procesory. Oczywiście działa to w ten sposób, że jak się znajdzie klient, który zapłaci więcej, no to Wam wyłączą, czyli trzeba tym zarządzić, ale jeżeli może się okazać, że nie ma klienta w danej chwili, no to w ten sposób też możemy zoptymalizować koszty naszej pracy. My sobie zrobimy on demand potrzeby tego ćwiczenia. No i zobaczcie, mamy nasz setup, zapłacimy 3 dolary za godzinę przetwarzania modeli językowych, mamy tą najdroższą kartę graficzną, 251 giga RAM-u, 24 procesory i dysk. To nie jest istotne. Istotne jest to, że za te trzy dolce, które tutaj mamy, zobaczcie, ta karta graficzna kosztuje 160 tysięcy złotych. No to jakaś za stówę, tutaj za 160 tysięcy złotych, za 166 tysięcy złotych, czyli za 3 dolary za godzinę mamy kartę, która kosztuje 160 tysięcy złotych gdzieś na świecie. Ok, podsumujmy. Wchodzimy sobie w pods, dajmy deploy, bo chcemy stworzyć naszą maszynę. Ja jestem zwolennikiem Community Cloud, lubię takie inicjatywy. Bierzemy tą najdroższą kartę, bo why not? Mamy całe 7 dolarów, więc sobie możemy się pobawić. On demand. Wybieramy Olamę, którą przeciczyliśmy sobie wcześniej. Dajemy on demand. No i co za tyle. Deploy. Teraz gdzieś na świecie ta maszyna się tworzy dla nas. Zaraz zobaczymy, gdzie nam wylosuje miejsce i gdzie są dostępne moce. Dzisiaj nam wylosowało... Okej. Jesteśmy akurat w Kanadzie. Także tam stoją jakieś procesory, które są dla nas wolne. No i teraz trzeba chwileczkę poczekać. Teraz się instaluje system. Będzie to trwało dosłownie kilka minut. Jak widzimy, są ściągane jakieś tam pliki i tak dalej. Po prostu się instaluje system. Tak jak pamiętacie, instalowaliście kiedyś Windowsa, to na pewno to jest coś takiego. Poczekajmy chwilkę. Okej, jesteśmy. Maszyna się zainstalowała, trwa o to kilka minut. Jest to normalne, po prostu musi się zainstalować. Jesteśmy w miejscu, w którym mamy maszynę w Kanadzie, która ma ten najdroższy procesor graficzny NVID-H100 za 160 tysięcy złotych. Jesteśmy gotowi do akcji, działa. Dobra, co musimy teraz wykonać? Ona gdzieś sobie stoi dla nas w Kanadzie, trzeba by tam zainstalować model językowy. Klikamy sobie Connect i dla osób zaawansowanych mamy możliwość terminala, czyli w konsoli, z którym pokazywałem, możemy sobie tam porozmawiać z tą maszyną. My zrobimy sobie to w web terminalu, żeby było szybciej i łatwiej. Czyli klikamy sobie Start Web Terminal, Connect Web Terminal. I teraz łączymy się do tej naszej maszyny, która stoi dla nas przygotowana w Kanadzie. To zawsze chwileczkę musi potrwać, bo ona dopiero powstała. Za chwileczkę się połączymy. O śmiało, Kanada. Ok, jesteśmy. Dobra, jesteśmy połączeni, czyli ten terminal, który Wam pokazywałem na moim komputerze, teraz mamy po prostu w Kanadzie na tym procesorze logicznym. Tutaj już jest wszystko gotowe. Wszystko jest zainstalowane i możemy robić akcje z modelami językowymi. Czyli wracamy sobie do Lamy, czyli O-Lamy. No i jeszcze raz, powiedzmy nas interesuje, żeby sobie ściągnąć jeszcze raz ten model powiedzmy 3 miliardy parametrów, żeby to było szybciej. Kopiujemy tylko i wyłącznie tą komendę do web terminalu. Kopiuj wklej, ogień. I zobaczmy. Tego nie widzieliśmy wcześniej, bo na moim komputerze było już zainstalowane. W tym momencie model się pobrał. Zajęło to kilka sekund, no bo akurat on jest mały, więc jakby to jest okej. W tym momencie zaczynają się instalować dodatkowe jakieś tam manifesty, konfiguracje i są ściągane właśnie na tej maszynie naszej kanadyjskiej. Weryfikacja. No i zaraz będzie działać. Dobra, jesteśmy. Model jest zainstalowany na karcie graficznej za 160 koła w Kanadzie i możemy mu napisać. Jeszcze raz. What is sushi? Chyba mam ochotę dzisiaj na sushi, chociaż nie dzisiaj na żurek. Okej, let's go. No i mamy sushi, informacje o sushi i tak dalej. On sobie pracuje tutaj w Kanadzie. Fantastycznie. Wróćmy do run poda. Zobaczcie. Dodatkowo mamy tutaj connect to HTTP service. Czyli tutaj mamy już wystawiony nasz jakby API endpoint, gdzie możemy wysyłać z naszych orkiestratorów, co pokażę w kolejnych lekcjach, Żądania, czyli wpiąć do naszej aplikacji, czy korzystać z API. Jeżeli chcecie wiedzieć więcej, to jest bardzo, bardzo proste. Wchodzimy sobie na OLAM-ę, DOCS. W DOCS-ie się znajduje API Reference. I w API Reference mamy informację, jak tam sobie rozmawiać z tą LAM-ą, OLAM-ą, która jest tylko i wyłącznie nasza. Używamy po prostu tego adresu do komunikacji. Czyli on znajduje się tutaj, connect. No i sobie działa. OK. Czyli jesteśmy w miejscu, w którym już sobie odpaliliśmy model, przetestowaliśmy, że działa, możemy się do niego dalej połączyć, wykonywać nasze operacje. I załóżmy, wykonaliśmy akcję. 100 milionów operacji, kategoryzacji, słów kluczowych zajęło nam to, nie wiem, dwa dni. Płacimy za godzinę i uznajmy, okej, dobranie, płacimy, koniec. Maszyna się stopuje, czyli wyłączamy, przestajemy już w tym momencie płacić. A żeby nie płacić jeszcze za zajętość dysku, tak kilka centów za to chcą, to my sobie po prostu usuniemy. Koniec. Temat jest zamknięty, maszyna nie istnieje. Jeżeli chcemy zrobić kolejną, deployujemy, instulujemy inny model, podpinamy do naszych procesów, realizujemy masowe operacje, wyłączamy i jedziemy dalej. Dzięki za tą lekcję. 
# Transkrypcje Lekcji - Tydzień 1

## Lekcja: Historia modeli językowych

Cześć! Witam Cię w pierwszej lekcji kursu Sensei. W pierwszej lekcji porozmawiamy sobie chwileczkę o historii oraz postaram Ci się na przykładzie opracowania, które stworzył Financial Times, wytłumaczyć, jak działa AI, skąd to się wzięło i w ogóle dlaczego to działa. Ale po kolei. Zacznijmy od historii. Dwa słowa, szybciutko. W mojej opinii, ale też w sumie na podstawie patentów i technologii, Google jest drajwerem i powodem rozwoju sztucznej inteligencji na świecie, to dzięki ich technologii znamy obecne AI, jakie znamy. Teraz Ci pokażę, jak to działa i dlaczego działa, przynajmniej spróbuję. Ale skąd to się wzięło? Wyobraźmy sobie wyszedł w arkę Google i ich największy problem. Ich największym problemem jest oczywiście potrzeba zrozumienia kontekstu zapytania. Przykład. Piszesz czarna konsola do gier, myślisz PlayStation. Albo piszesz bajka o różowej śwince, myślisz świnka Pepa. Google stało przed wyzwaniem, żeby połączyć z jednej strony zapytania, które wprowadzamy do wyszukiwarki, ale i połączyć z kontentem z tych stron internetowych, które prezentuje nam w wynikach wyszukiwania. Dodatkowo Google też musiało zrozumieć treść na naszych stronach internetowych i po raz kolejny dopasować je do wyników wyszukiwania. jest jedna zasada, która przyświeca Googlowi. Ich głównym, nadrzędnym celem jest dostarczenie sobie jak najlepszych wyników wyszukiwania i dlatego powstały technologie, jak na przykład transformery, do których zaraz dojdziemy i to one rządzą obecnym AI, takim, jakie znamy dzisiaj. Słowo historii. Pierwsza rzecz. 2013 rok Google jest ciągle na tej samej ścieżce rozwoju, czyli potrzebuje znaleźć kontekst zapytania oraz treści na stronie internetowych i powstał Word2Vector. Jest to pierwszy algorytm, którym z Damianem mówimy od zawsze o tym algorytmie, jednak nie sposób go ominąć w kontekście sztucznej inteligencji. Jest to zamienienie słów na wektory. Jak widzimy na tej animacji, nagle słowa opisane w przestrzeni wektorowej zaczęły być blisko siebie. W tym momencie Google zrozumiało, że ok, że pianino jest blisko do skrzypiec albo Mercedes do Volkswagena jako samochód. To jeszcze im nie dało możliwości zrozumienia kontekstu, ale na początku w ten sposób zaczęli przetwarzać wasze strony internetowe i tak też zaczęło powstawać pierwsze topical authority, bo skoro jesteśmy na przykład w kontekście instrumentów muzycznych, należy omówić pianino, skrzypce, gitarę i tak dalej. Z tego to się wzięło. czyli to był 2013 rok w 2014 roku Google opublikowało kolejny algorytm, czy kolejną metodę, którą nazwało sequence to sequence i na tej podstawie Google było w stanie stworzyć Google Translate'a, czyli tłumaczenie maszynowe podejrzewam, że do tej pory w ten sposób to działa okazało się, że na przestrzeni wektorowej słowo pies i słowo dog było po prostu w podobnej pozycji przez co Google było w stanie tłumaczyć na przykład z języka angielskiego na język polski i tak działa Google Translate ale to nie było kluczowe w kontekście AI W kontekście AI kluczowa jest architektura transformerów. Postaram Ci się to omówić na przykładzie chyba najlepszego opracowania, które ostatnimi czasy przygotował Financial Times. Znajduje się on, to opracowanie pod tym adresem i wszystkich zachęcam, żeby tam wejść i poczytać, co tam jest napisane, ponieważ ta strona jest fantastyczna, tam się przełączę i po kolei będziemy sobie rozmawiać, co przygotował Financial Times w obszarze transformerów. Okej. Tu jest chwileczka historii, skąd to się wzięło i tak dalej. No i zobaczmy. Mamy jakieś zdanie we go to work by train, czyli jedziemy do pracy pociągiem, no nie? I architektura transformerów wprowadza pojęcie tokenów. Każdy model językowy ma troszeczkę inny tokenizer, czyli jak tutaj zobaczymy, o ja mam taki świetny świetny pilocik od Mateusza, zobaczcie jaki bajer, o chyba to nie jest Ten bajer znajduje się tu. Czytamy tutaj opowied. Mamy represent fraction of the words, czyli tokeny w obecnym AI reprezentują fragment słowa. Pokażę to na kolejnych slajdach tej prezentacji, natomiast na potrzeby uproszczenia omówienia Financial Times stokenizował zdanie we go to work by train po słowach, żeby łatwiej było zrozumieć koncepcję. Czyli pierwsza sprawa. Mamy zdanie. Jedziemy do pracy pociągiem, tokenizujemy go, to organizujemy to zdanie na pojedyncze słowa. Chodźmy dalej. Dalej. Algorytmy sztucznej inteligencji, czy też na razie transformery jeszcze, szukają prawdopodobieństwa, w jakim prawdopodobieństwie słowo work może wystąpić, czyli because I work in my, coś tam, albo people who work from home, co nie? Czyli w tym momencie architektura transformerów szuka czegoś tutaj mamy nazwane nearby words, czyli słowa bliskie. Co się dzieje dalej? No, zobaczcie. I tutaj dzięki takiemu ćwiczeniu architektura transformerów jest w stanie określić, które słowa są blisko do słowa praca, a które są daleko. To tak na przykładzie tego Word2Vector, tam już było wiadomo, co jest blisko, co jest daleko. I przykład. Work Polka, to jest bardzo daleko. Work Zebra, albo Work Atmosphere, albo Work Dove, Dove to jest gołąb, będą daleko. ale work processes, czy work that, czy work hour, work to, work for, albo work meet, są blisko. Czyli w tym momencie transformery już wybierają słowa, które są blisko słowa work, a które są daleko odrzucając je jakby z dalszego przetwarzania. I teraz zobaczmy. W tym momencie zachodzi proces tak zwany embeddingów, czyli zmiany na te wektory, które były pokazane na przykładzie word to vector, Czyli każdy z tych jakby zlepków, work, plus coś tam, co jest jakimś prawdopodobieństwem, staje się opisane jakimś tam embeddingiem, przez co transformery są w stanie mierzyć dystanse już matematycznie. I teraz będzie fajny przykład, do którego przejdę od razu. O co z tym wszystkim chodzi? Damian w swojej części tego szkolenia będzie bardzo szczegółowo opowiadał, z czego to wynika. Natomiast przyjrzyjmy się po prostu. Jeżeli mamy słowo C opisane jakimś tam embeddingiem, czyli jakimś tam wektorem, blisko znajduje się ocean, czyli sea jako morze i ocean jako ocean. I w ten sposób Transformer wie, że to są słowa bardzo blisko, ale znaczą coś innego. Bo tutaj mamy na przykład football i soccer i one też znaczą praktycznie to samo, ale jednak nie do końca. Albo I, albo we. I w ten sposób sobie opisuje dystansami, co jest blisko, co jest daleko. Chodźmy dalej. I dzięki temu, teraz zobaczycie to, co przed chwilą oglądaliśmy na Word2 Vectorze. Google sobie zrobiło klastry, no nie? Czyli mamy na przykład train, bus, car, czyli w tym momencie Google wie, czy ten transformer wie, że to są jakieś pojazdy, a tutaj będziemy mieli jakieś colleague, school, work, no nie? Albo walk, swim i run, czyli to pewnie będzie jakiś tam klaster sportu, no nie? Jak widzicie, tutaj jest dokładnie opisane o co chodzi. Musicie pamiętać o tym, że w ten sposób te słowa stają się blisko siebie, Ale to jeszcze nic nie znaczy w kontekście transformeru. Na razie po prostu wiemy, że coś jest blisko siebie. Jak sobie przeczytacie dalej i przejdziemy dalej, zaczynamy wchodzić w kontekst transformeru. Kolejne przykładowe zdanie. I have no interest in politics. Czyli nie jestem zainteresowany polityką. I tutaj pojawia się słowo klucz. Tutaj. Self-attention. Self-attention znaczy uważność albo samouważność. Chodzi o to, że w tym momencie transformery zaczynają rozumieć, które słowo się z czym wiąże i co to tak naprawdę zaczyna znaczyć. Bo w tym momencie wiedzą, co jest blisko siebie albo co jest w klastrze na przykład pojazdów, sportu, pianin, instrumentów muzycznych. W tym momencie transformery zaczynają rozumieć, co znaczy co. Chodźmy dalej do przykładów. To jest świetnie opisane. Zobaczcie. I transformery zaczynają porównywać słowa ze sobą i nagle wiedzą, że no w tym zdaniu zaczyna ważyć najwięcej. Self-attentional look at the each token in the body, text and decide which one are most important to understanding the meaning. Czyli w tym momencie zaczynają rozumieć, które słowo znaczy najwięcej dla znaczenia całego zdania. No i w tym konkretnym przypadku jest no, jako nie mam zainteresowania polityką, prawda? Inaczej, I have interest in politics, to znaczy zupełnie coś innego, prawda? No i chodźmy dalej. Zobaczcie. I wcześniej, to jest jakby omówienie starych algorytmów, dlaczego to nie działało. Algorytmy sprawdzały sobie słowo po słowie, słowo po słowie, słowo po słowie, czy które jest najważniejsze, przez co, tu jest akurat nazwa, jakiś neural network sequence, coś tam, przez co to po prostu nie działało. I teraz zobaczcie tak. Jeżeli chodzi o ten self-attention, oni od razu sprawdzają każde słowo i typują najważniejsze, czyli słowo no. Dobra. I chodźcie dalej. W tym przykładzie również widzimy, że transformery są w stanie określić, że słowo interest, czyli zainteresowanie, jest rzeczownikiem w kontekście polityki i w tym zdaniu. Jak pójdziemy sobie dalej, Troszeczkę zmieni się teraz zdanie, które będziemy przerabiać. Czyli mamy bank interest rate rises, czyli oprocentowanie bankowe rośnie. I widzicie, w tym przypadku to słowo interest już nie jest związane tak jak tutaj z zainteresowaniem, tylko to samo słowo w innym kontekście, w innym szyku nagle zaczyna znaczyć coś innego, czyli w tym przykładzie po prostu procenty. I to właśnie robią transformery przez to porównywanie i ten self-attention, że są w stanie określić, co znaczy coś w kontekście, jakimś nam zdaniem itd. No i dobra. I widzicie, to cały czas można rozbudowywać, bo tak wyobraźmy sobie nasz content. And when we combine the sentence, the model is still able to recognize the correct meaning of each word, thanks to attention, give it to accomplished text. Czyli widzicie, jak rozbudowaliśmy po raz kolejny ten text, czyli I have no interest in hearing about the rising interest rate on the bank. Oni ciągle rozumieją, mimo że zdanie jest dużo dłuższe, że ten interest to są ciągle po prostu procenty i że jest najważniejsze. A dokładnie tutaj były nie procenty, tylko tutaj wiedział, że interest, czyli nie mam zainteresowania i zainteresowanie tutaj było ta samo słowa jako zainteresowanie, a w kolejnym zdaniu i w tym szyku jako rising rates i bank określa jako procenty. W tym momencie pojawia się zrozumienie kontekstu i dlatego Google trochę ociągnęło ten cel i dalej modele językowe zaczęły po prostu działać. No właśnie, jak czytamy dalej, te funkcje stały się kluczowe z perspektywy, widzicie, jak mam interest, profits, dividends, albo enthusiasm, encouragement. Czyli pewnie jak będziemy zamieniać te słowa, to nagle zdanie zaczyna znaczyć zupełnie coś innego. Czyli widzicie, jeżeli damy I have no interest in hearing about the rising enthusiasm rate of the bank. To to w ogóle nie funkcjonuje. I dlatego ten self-attention i transformer działa i z tego też AI działa. No i dobra, ostatni przykład, który Wam pokażę i zachęcam, żeby każdy sobie przeklikał tę stronę w Financial Times, bo jest po prostu fantastyczna. Mamy kolejne zdanie. The dog chewed the bone because it was hungry. No i tak, brać, czyli to znaczy mniej więcej tyle, że pies przeżył kość, ponieważ był głodny. No i jak zobaczymy, to słowo eat odnosi się do dog. I w ten sposób oni sobie połączyli, że dog i eat to jest ta sama rzecz. No nie? No bo w zdaniu tak to wychodzi po prostu, że pies żół, bo był głodny. Czyli ciągle ten pies. I zobaczcie. Jeżeli przejdziemy sobie dalej. If we alter the sentence rapping hungry for delicious, the model is able to reclite with it most likely the bone. Czyli w tym momencie sobie rozszerzyliśmy zdanie the dog chewed the bone because it was delicious. Czyli model po raz kolejny wie, że delicious dotyczy kości, którą przeżył pies. I tak dalej, i tak dalej. I w ten sposób generowany jest content przez modele sztucznej inteligencji. Po prostu oni wiedzą, które słowo, co znaczy w zdaniu i co z największym prawdopodobieństwem będzie następne w tym kontekście. I cała nasza zabawa ze sztuczną inteligencją to właśnie będzie walka o prawdopodobieństwo, które bezpośrednio wynika właśnie z architektury Transformerów, którą starałem Wam się pokazać. Jednak mimo wszystko każdego z Was zachęcam, po notatce do tej lekcji będzie link do Financial Timesa, żeby każdy sobie powolutku przeszedł tą stronę, bo jest naprawdę fantastyczna. Lepszej nie znalazłem. Podsumujmy. Czyli mamy na samym początku tą tokenizację, czyli w tym przypadku zmianę, zaraz Wam pokażę to zdanie, tam był pies, czy tam był bank. Później mamy word embedding, żeby sprawdzić te dystansy. Self attention, co z czego wynika i co się z czym łączy. I na końcu encoded output, czyli wynik z naszego transformera, który już po prostu obrazuje, co jest co. Ostatni przykład, na przykładzie Financial Timesa, on jest dosyć ciekawy. Zobaczcie. Wyobraźmy sobie, że w tym momencie generujemy content. AI generuje content. Generuje nam jakiś tam kontekst i szyk zdania. No i mamy po kolei. The financial times is. No właśnie, i co będzie następne po is? No i mamy jakieś tam tutaj propozycje about, more, abate, whatever. Chodźmy, zobaczymy, co Transformer nam z tego wszystkiego zrobi. Prowadza właśnie pojęcie probability score, czyli takim prawdopodobieństwem, które słowo po the financial time is wystąpi w tym konkretnym kontekście. No i widzimy, że tutaj w tym przypadku akurat about występuje jako najwyższe prawdopodobieństwo i economics jako następne słowo, czyli the financial time is about economics, co jest właśnie prawdą, bo ten financial time jest o ekonomii. No ale chodźmy dalej. I tam dalej może sobie budować, zobaczmy, tutaj jest właśnie budowanie tego, jak AI buduje nam zdania, czyli financial time is about economics i do tego dostanie doklejony jakiś end podcast, no nie? No i zobaczcie, i tych różnych wersji AI nam generuje powiedzmy kilka, w zależności od konfiguracji, którą dokładnie Damian będzie omawiał w swojej części. Natomiast w tym momencie AI wybiera nam wersję, która wystąpi z największym prawdopodobieństwem. I zobaczcie, mamy tutaj Pewnie po prostu wszystkie są prawidłowe, natomiast jak sobie policzą to prawdopodobieństwo, to im wyjdzie, że The Financial Times is a newspaper found in 1888. I w ten sposób AI właśnie generuje kontekst, content. I co jest naszym celem? Naszym celem jest to, żeby prawdopodobieństwo następnego tokenu w jakimś kontekście było jak najwyższe, co właśnie pokazuje ten przykład. Tu jest trochę więcej opisów na tej stronie. Zachęcam wszystkich do zapoznania się z tym. Mam nadzieję, udało mi się chociaż pokrótce wytłumaczyć, o co chodzi. Natomiast jak sobie przeklikacie tą stronę, to zrozumiecie doskonale, o co chodzi. Chodźmy dalej. I tam była informacja o tym, że jest wprowadzone pojęcie tokenu, transformacji na tokeny. W przykładzie Financial Timesa, żeby był najprostszy, tokenizer był po słowach. Natomiast modele sztucznej inteligencji trochę oderwały się od słów i słowa zamieniają na typowe zlepki lub fragmenty. I to jest taki przykład z tokenizera OpenAI, w którym najczęściej będziecie mieli styczność. I jak przeczytacie, o co tutaj chodzi, jest tutaj dosyć kluczowa informacja. Zobaczcie. Jeśli chodzi o procesowanie do tokenów. Process text using tokens, which are common sequence of characters found in a set of text. Czyli wasze zdania, wasze słowa zaczynają być zmieniane na najprawdopodobniej albo na typowe sekwencje znaków. Co zobaczymy tutaj? Zobaczcie. Pisałem w zdaniu Hej, co u Ciebie słychać? I zobaczcie. Mieliśmy 24 znaki. Zostało zmienione na 11 tokenów. I te kolorki oznaczają tokeny, które AI sobie dalej będzie przetwarzać. Ten transformer w AI. Później ten self-attention i tak dalej, prawdopodobieństwa i co występuje po czymś. Podsumujmy to. Jeden token, zazwyczaj po angielsku to są 4 znaki ze spacją. znaczy jeden token to są cztery znaki bez spacji a powiedzmy 1500 sub to jest 2048 tokenów tak to się mniej więcej przelicza i dlaczego to jest super ważne? bo w każdej waszej akcji wy po prostu zapłacicie za tokeny dodatkowo wyszukiwarka stosuje transformery w swojej architekturze do oceniania waszych treści, więc wasze treści bezpośrednio również zostaną zamienione na token dlatego to jest ważne jest to również ważne, ponieważ kodowanie języku angielskim będzie tańsze niż kodowanie na tokeny w języku polskim. Przez co na przykład rozmawiając ze sztuczną inteligencją, promptując ze sztuczną inteligencją, jeżeli będziecie robić to w języku angielskim, to po prostu będzie tańsze aniżeli w języku polskim. Także Damian Wam to wytłumaczy w części dotyczącej prompting engineeringu. Ja tylko zaznaczam, że to jest ważne i totalnie odrywa się od warstwy słów. Dalej. 2018 rok. Google wprowadza Berta. Jest to algorytm, albo transformer, zasadniczo, bo to jest bi-directional transformer, dwustronny, który obecnie rządzi wynikami wyszukiwania. I to właśnie BERT ocenia Wasze strony internetowe. I to właśnie BERT ocenia Wasz content. W kolejnych częściach pokażemy Wam, jak lepiej się dogadywać z tym BERTem i dlaczego warto po prostu pamiętać o tych tokenach i o budowie zdań. No i zobaczcie. W tym samym roku powstaje pierwszy GPT od OpenAI. 2018 rok. 6 lat temu. Natomiast niewiele się wydarzyło w tym czasie. Google zapowiedziało pierwsze podrygi multimodalności, czyli tego, że zrobimy Wam search overview albo AI overview, albo AI search w wynikach wyszukiwania w 2021 roku. Chodziło o to, że Google myślałoby wyszukiwać głosem, wyszukiwać obrazem, natomiast w tamtym czasie nie było technologii, żeby to zrealizować. No i zobaczcie, dwa lata temu powstał czas GPT i wszystko przyspieszyło. I obecnie jesteśmy, jakby miejsce, w którym jesteśmy tu i teraz dzisiaj, czyli GPT-40 i O-1, czyli 2024 rok. Także całość procesu takiego akceleracji w sumie od 2018 roku, a ostatnie dwa lata to istny pociąg Pendolino, a raczej Shinkansen w rozwoju tej technologii. No właśnie, i w ten sposób Google zostało, nie Google, tylko OpenAI, zostało opycenione na 157 miliardów dolarów. Jest to jedna z największych wartości prywatnych firm na świecie. I to jest doskonały przykład właśnie. Zobaczcie, wartość, to jest wartość OpenAI, 157 miliardów. I firmy takie jak Zoom, Warner Music Group, Snapchat, Dominos, Duolingo, The Vs, The New York Times są mniejsze, a raczej ich suma dopiero może się złożyć na OpenAI, które przyspieszyło ten rozwój od dwa lata temu. No i cześć, słuchaj, witam Cię w 2025 roku. Powiedzieć, że AI się rozwija, to nic nie powiedzieć. Powiedzieć, że dużo się zmieniło, to również nic nie powiedzieć. Powiedzieć, że postęp jest wykładniczy, to też nic nie powiedzieć, bo postęp jest wykładniczo-wykładniczy. Przez chwilę pokazałem Ci wycenę. Tą część nagrałem w zeszłym roku. Wycena sięgała 157 miliardów dolarów. W 2025 roku, po kilku miesiącach, tamtą część kursu nagrywałem pewnie koło września albo października, w kwietniu tego roku OpenAI uzyskał już wycenę 300 miliardów dolarów, czyli co do zasady wycena się podwoiła. Na poziomie federalnym Stanów Zjednoczonych również jest gigantyczne wsparcie dla sztucznej inteligencji. Powstały programy, które finansują rozwój sztucznej inteligencji do poziomu 500 miliardów dolarów, więc jest niesamowicie. W 2025 roku mówimy już o Ricksoningu, czyli mamy tutaj modele Gemini 2,5, mamy O3, pojawiają się modele klasy O4, więc jest niesamowicie i modele zaczęły osiągać inteligencję przewyższającą powiedzmy średniego człowieka, jeśli można mówić o średnim człowieku, no ale jeżeli to jest rozkład normalny i średnio na świecie człowiek ma IQ na poziomie 100, no to w tym momencie modele w 2025 roku osiągają już wartości na poziom 130 i przekraczają właśnie dzięki temu reasoningowi, o którym będziemy bardzo dużo mówić w tym kursie, bo jest niezwykle przydatny w kontekście SEO i w kontekście marketingowym. Dla porównania Albert Einstein miał IQ na poziomie 167, także jeszcze troszeczkę brakuje modelom do poziomu Ale wiem, że to się wydarzy w takim tempie pewnie w tym roku, a jeżeli nie, to na pewno w następnym. Otrzymaliśmy super fotorealistyczne zdjęcia od OpenAI. Totalny detal, możliwość generacji totalnie fajnych napisów, które są napisami do tej pory w zeszłym roku. Raczej napisy były przypadkowymi znakami albo jakimiś krzaczkami. W tym momencie jesteśmy w takim miejscu, gdzie to naprawdę już zaczyna fantastycznie wyglądać. Powstało narzędzie Ideogram, które polecam. Mateusz Godzic w swojej ostatniej części tego kursu na temat grafiki i wideo na pewno pokaże Ci to narzędzie. Jest fantastyczne do tworzenia zdjęć. Bardzo dużo się wydarzyło przez te kilka miesięcy w tym kontekście, ale również w kontekście wideo. W zeszłym roku prezentowałem Ci, albo członkom kursu, którzy byli w zeszłym roku, być może nie Tobie, wideo, które nie wyglądało tak, jak to, co Ci pokazuje. Absolutny detal, kontrola kamery. Dostaliśmy również informację, że Oscary zostaną rozdane w kategorii AI i firmy AI zostaną dopuszczone do Oscarów. Filmy są generowane również w formie storytellingu, czyli można opowiadać historię na wielu filmach. Filmy generują również dźwięk, na przykład w tym przypadku, jak mamy chłopca, który porusza się po lesie, być może gdzieś jesteśmy w stanie wygenerować, na przykład jakiś tam szelest lasu, czy jakiś tam dźwięk, jakieś zwierząt, które się znajdują za tym drzewem. I absolutna multimodalność w każdym wertykalu. W zeszłym roku mówiliśmy o multimodalności dotyczącej zdjęć, w tym momencie mamy dźwięk, pełne wnioskowanie po zdjęciach, rezonning po zdjęciach, czyli jesteśmy w stanie na przykład wrzucić kreację reklamową i zapytać szóstej inteligencji, co poprawić w celu lepszego performance w mojej kampanii reklamowej, albo wrzucić wszystkie zdjęcia, przypominam, że Facebook ma ten marketplace reklamowy, gdzie możemy podglądać konkurencję, wrzucić wszystkie kreacje reklamowe naszej konkurencji i zapytać sztucznej inteligencji, co tam najlepiej działa, a później to wygenerować w sztucznej inteligencji, więc jesteśmy w stanie nie tylko wnioskować po treści, po tekstach, ale również po obrazach na dzień dzisiejszy, także tutaj mamy bardzo duże przyspieszenie rozwoju. I żeby zarysować to na osi czasu, mniej więcej, no nie na osi czasu, na perspektywie rozwojowej, to w zeszłym roku raczej mówiliśmy o tym Gen AI, czy Generative AI, generowanie słów, tekstów, zdjęć, powoli wideo. W tym momencie widziałeś, jak to wygląda niesamowicie. Jesteśmy gdzieś tu. Za każdym razem, jak pokazuję ten slajd, przesuwam odrobinkę, tą strzałeczkę, bo już jesteśmy w erze agentów. Natomiast jeszcze wymaga to chwileczkę czasu, żeby one faktycznie funkcjonowały. W tym kursie również będziemy opowiadać o agentach, jak tworzyć agentów autonomicznych. Kolejnym krokiem będzie physical AI. Więc jak spotkamy się w Sensei Akademii, być może w 2026 roku ta strzałeczka będzie już dużo wyżej po stronie Physical AI, ponieważ trening trwa. To jest prezent zarządu NVIDI, który zdaje się w tym roku, gdzieś koło lutego, poinformował, że istnieje coś, co się nazywa NVIDIA Cosmos. Jest to świat równoległy do treningu właśnie Physical AI, czyli czego? Czyli samochodów autonomicznych. Bo na świecie nie ma wystarczająco dużo filmów z jazdy samochodów, na których można by samochody autonomiczne trenować. Te samochody są trenowane właśnie w tym NVIDIA Kosmos, czyli świecie równoległym, gdzie dane treningowe są generowane na potrzeby treningu. Humanoidy. Jest masę informacji, że np. humanoidy w Chinach będą produkować iPhony. Tego nie było w zeszłym roku. W tym świecie wirtualnym, w Kosmosie te humanoidy właśnie są trenowane. Czy też prezentacje tej klasy, to jest polska firma z Wrocławia, która buduje humanoida, jakby to ładnie powiedzieć, mięśniowo-szkieletowego, który ma odwzorować człowieka, mięśnie i sposób poruszania się, nie tylko robocik, ale tej klasy. Ja nie chcę wiedzieć, co ludzie będą z tym robić za kilka lat. Natomiast takie rzeczy również się dzieją w 2025 i jestem bardzo szczęśliwy, że się dzieją akurat w Polsce, we Wrocławiu. Czy też neuoprotezowanie mózgu. Naukowcy z wykorzystaniem sztucznej inteligencji są w stanie wszczepić protezę człowiekowi do mózgu i dzięki temu on odzyskuje mowę w jakimś tam stopniu, czy jest w stanie się komunikować dzięki falomózgowym, które sztuczna inteligencja rozumie. Dodatkowo w tym roku również naukowcy Google DeepMind podali informację, że są w stanie, czy w procesie dekodowania języka delfinów. Mamy masę informacji z medycyny, na przykład diagnostyki gruźlicy, przewidywanie nowych leków. Tak więc od zeszłego roku, od poprzedniego Sensei Akademii wydarzyło się bardzo, bardzo dużo. W tym kursie będziemy omawiać wszystkie kluczowe informacje dotyczące SEO i marketingu z najnowszą technologią, także zapraszam Cię do kolejnej lekcji, powoli przechodzimy do konkretu w kolejnej lekcji opowiem Ci, czym są modele językowe, bo głównie na nich będziemy się skupiać jakie są ich wyzwania jak sobie z tym radzić, no i co? zapraszam Cię dalej, cześć!

---

### Lekcja: Zrozumieć Modele Językowe

Cześć, witam Cię w kolejnej lekcji. W poprzedniej zapoznaliśmy się z historią modeli językowych. Jesteśmy już w 2025 roku. Pomówmy jeszcze przez chwileczkę o podstawach modeli językowych. I chciałbym Ci opowiedzieć, czym są modele językowe, jakie mają wyzwania, jak się poruszać w tym świecie, jak je rozpoznawać, jak je oceniać, tak żeby w kolejnych lekcjach płynnie poruszać się w tym świecie i dobierać tak model, żeby spełniał Twoje zarówno oczekiwania, jak i cele czy procesy biznesowe. Okej, pierwsza sprawa. Modele językowe nie mają kreatywności. Modele językowe zostały wytrenowane na zestawie danych. Często, gęsto te dane są nieaktualne i powiedzmy wiedza modeli językowych jest gdzieś odcięta w 2023, być może w 2024 roku, więc modele nie mają kreatywności żadnej. Modele, jeśli mają podyskać jakąkolwiek kreatywność, muszą być karmione w formie feedbackowej. Tak są tworzone leki, tak są tworzone nowe proteiny. Czyli jest coś przewidywane przez model językowy, weryfikowane i po prostu jakaś baza danych budowana. Natomiast co do zasady, one są, jakie są, nie mają kreatywności, przez co nie wytworzą nowych rzeczy. To są procesory językowe, które trzeba nakarmić. Będziemy to bardzo, bardzo szeroko omawiać w tym kursie. I dopiero wnioskować i wytwarzać, powiedzmy albo nowe rzeczy na podstawie karmienia, albo syntezować na podstawie karmienia, ponieważ co do zasady one po prostu są procesorami językowymi, które przetwarzają wejście, dając nam jakieś wyjście. Ok, jak działają modele językowe? Prosty wzór na model językowy. LLM, czyli Large Language Model, równa się Deep Learning plus dane i proces treningowy. Deep Learning, żebyś zrozumiał, to jest na tej zasadzie. Machine learning, masz dane, to jest twoje zadanie, wykonaj zadanie. Deep learning to są, masz dane, zrób coś z nimi. Czyli model po prostu dostaje ogromny korpus danych i po prostu się na nich uczy w sposób taki, że nikt mu nie dał tego zadania. Jak ma się uczyć. Skąd pochodzą dane? Bazy danych, organizacje typu Common Crowd, to jest taka organizacja, która w cudzysłowie ściąga cały internet na dyskietkę, zapisuje i można te dane, czy korpus internetu pozyskać. Książki, dokumenty, wszystko, na czym nie ma prawa autorskich i można pozyskać. Wasze strony internetowe, na szczęście moje też, więc jakby cieszymy się i tak dalej, i tak dalej. Czyli powiedzmy, wyobraźmy sobie gigantyczny, wielki odkurzacz, który jest w stanie wciągnąć cały internet, to tym właśnie jest AI w procesie treningowym. Na początku wciągnęli odkurzaczem cały internet i dali, masz, zrób coś z tym. W 2025 roku Elon Musk, powiedział, że jesteśmy w miejscu, w którym dane treningowe, te realne ze świata ludzkiego powiedzmy, skończyły się. Dlatego w tym momencie dochodzą również dane syntetyczne. Przedniej lekcji mówiłem ci o treningu samochodów autonomicznych. To właśnie są trenowane na danych syntetycznych. Jak to rozumiemy? Mamy zestaw danych, który tutaj widzimy przed sobą i na podstawie tego są tworzone syntetyczne dane, czyli AI tworzy sobie większy zestaw danych, na podstawie których jest dalej trenowany. No tak to działa. W związku z tym mamy kilka problemów. Okej, w tym roku, w 2025 problemy są coraz powiedzmy mniejsze albo są redukowane, ale ciągle występują i muszę Ci o tym powiedzieć, ponieważ z perspektywy SEO czy marketingu to jest najważniejsza rzecz, którą będziemy zarządzać w tym kursie. Czyli pierwsza sprawa. Nie do końca wiemy, jak to działa. Modele mają charakterystykę probabilistyczną, czyli z jakimś prawdopodobieństwem, ci odpowiadają. Nie do końca wiemy z jakim. Znaczy wiemy, jesteśmy w stanie sterować, ale musimy na tyle dobrze zasterować tym modelem, żeby na przykład uzyskać odpowiedni format albo odpowiednią odpowiedź, która nas interesuje, ponieważ jeśli nie, to model nam coś po prostu odpowie. Także sorry, ja się zajmuję tym powiedzmy profesjonalnie, też nie wiem. W zeszłym roku powstało badanie, które pokazywaliśmy, czyli ten AI Brain Surgery, tak to się dumnie nazywa, czyli naukowcy w jakiś sposób wzięli najmniejszy możliwy model na świecie i wykazali, że jeżeli zadają mu zadanie przetłumaczenia treści na język rosyjski, to jakiś element modelu, możemy sobie wyobrazić, to jak połączenia w mózgu zaczyna realizować to zadanie. Jeżeli temu samemu modelowi dają zadanie, powiedzmy, sumaryzacji treści, to zupełnie inna część tej sieci neuronowej zaczyna realizować to zadanie, więc troszeczkę możemy sobie to wyobrażać jak ludzki mózg na takiej zasadzie, że lewa półkula pełni inna funkcja niż prawa półkula, natomiast dwie są potrzebne, żebyśmy żyli. Szczęśliwie pojawiło się kolejne badanie. Poprzednie również stworzył Antropik. To badanie również stworzył Antropik. Chciałbym Ci pokazać krótki filmik, który omawia, do czego dochodzą naukowcy, jakie są wnioski i jak działa model językowy. Ciągle traktujemy to jako black box, czyli wsadzamy coś, wyjmujemy i w środku coś dzieje. Naukowcy próbują to opisać. Zapraszam Cię na krótki filmik, który troszeczkę nam rozjaśni, co się dzieje. Jest to filmik z 2025 roku. Także dwie minutki. Zapraszam. You often hear that AI is like a "black box." Words go in and words come out, but we don't know why it said what it said. That's because AIs aren't programmed, but trained. And during training, they learn their own strategies to solve problems. If we want AIs to be as useful, reliable, and secure as possible, we want to open up the black box and understand why they do things. Ale nawet nie jest bardzo pomocny to form logical circuits. Let's take a simple example where we ask Claude to write the second line of a poem. The poem starts, he saw a carrot and had to grab it. In our study, we found that Claude is planning a rhyme even before writing the beginning of the line. Claude sees a carrot and grab it and thinks of rabbit as a word that would make sense with carrot and rhyme with grab it. Then it writes the rest of the line. His hunger was like a starving rabbit. We look at the place that the model was thinking about the word rabbit, and we see other ideas it had for places to take the poem. We also see the word habit is present there. Our new methods allow us to go in and intervene on this circuit. In this case, we dampen down rabbit as the model is planning the second line of the poem, and then ask Claude to complete the line again. His hunger was a powerful habit. We see that the model is capable of taking the beginning of a new poem and thinking of different ways it could complete it and then writing it towards those completions. The fact we can cause these changes to occur well before the final line is written is strong evidence that the model is planning ahead of time. This poetry planning result, along with the many other examples in our paper, only makes sense in a world where the models are really thinking w ich własnej way, about what they say. Just as neuroscience helps us treat diseases and make people healthier, our longer-term plan is to use this deeper understanding of AI to help make the models safer and more reliable. If we can learn to read the model's mind, we can be much more confident it is doing what we intended. You can find many more examples of Claude's internal thoughts in our new paper at anthropic.com slash research. No właśnie, tak jak słyszeliśmy, model to jest ciągle black box, który, tam było bardzo ważne zdanie, nie został zaprogramowany, tylko został wytrenowany w taki sposób, że sam rozwiązuje problemy. Naukowcy wykazali, że modele są w stanie nie tylko przewidywać następny token, mówiłem o tym w pierwszej lekcji na przykładzie Financial Timesu, ale są w stanie planować to, co się wydarzy w przyszłości, czy w następnym zdaniu. Tam był przykład z rymami, habit, rabbit, okej? Więc też musimy zapanować troszeczkę nad tym, jak on zaplanuje, to co ma tam dalej nam odpisać, prawda? To będzie Damian pokazywał w swojej części dotyczącej prompt engineeringu właśnie, jak nad tym zapanować. No i co? Na dzień dzisiejszy tyle wiemy o modelach językowych, więc przewidywanie następnego tokenu, planowanie i trening, który po prostu jest taki, że model sam się uczył i sam rozwiązywał problemy. W związku z tym mamy największe wyzwanie SEO, halucynacje, ponieważ to jest ciągle prawdopodobieństwo i ciągle planowanie z jakimś prawdopodobieństwem następnego słowa, tokenu, rymu, jak na przykładzie Rabbity Habit i to jest bardzo ciekawe. największe wyzwanie. Jeżeli dotychczas powiedzmy, szedłeś do chat-a GPT i pisałeś, napisz mi arcykuł na temat chwilówki, to na pewno dostawałeś ten arcykuł, oczywiście. Natomiast z dużą pewnością nie działał, prawda? Powiedzmy, Google nie było zainteresowane tym arcykułem w żaden sposób i przez ostatnie miesiące serwisy, które jakby nie rozwiązały, czy osoby nie rozwiązały problemu halucynacji, no jakby utraciły widoczności i będą tracić przy kolejnych kurach update'ach, więc w tym kursie bardzo mocno się pokłonimy. I zobaczcie coś ciekawego. To jest bardzo ciekawa rzecz dotycząca halucynacji. Po prawej stronie widzimy Gemini 2.0 Flash. Jest to model, który zapewne rządzi AI Overview w Polsce i na świecie. Dlaczego? No bo jest najtańszy, najszybszy, bardzo dobry przy okazji, ale widzimy, że bez dostarczenia danych charakteryzuje się poziomem halucynacji 60%. Czyli wyobraź sobie, że 60% twoich treści potencjalnie, jeżeli używasz Gemini 2.0 Flash, może być shalucynowane. No i zobacz, jeżeli oni mają wyzwanie odpowiadania na świecie ludziom w AI Overview i tworzenia tych wyników AI-owych, ale mają taką charakterystykę halucynacji, no to jest to gigantyczny problem. Gdzie tutaj mamy czata? No powiedzmy GPT-4O. Jest to najpopularniejszy model czatowy i mocno wykorzystywany w kontekstach SEO. 57% halucynacji. Czyli co drugie zdanie może być fałszywe w tym przypadku. Ale zobacz jedną bardzo ważną rzecz. Powiedziałem już. chyba w poprzedniej lekcji, że modele są to procesory językowe, być może w tej. Jeżeli ten sam model Gemini Flash, który charakteryzuje się 60% halucynacją, zostanie nakarmiony, ten sam model charakteryzuje się halucynacją 0.7, karmieniem modeli językowych. I tego właśnie poszukuje Google. Google wie, że sam model co do siebie, sam black box, który wystawia nam, halucynuje. Jak zostanie nakarmiony faktycznymi danymi z Twojej strony internetowej, z mojej i tego właśnie poszukuje AI Overview, co będziemy bardzo szczegółowo omawiać w tym kursie, ten sam model już zbliża się do zera, jeśli chodzi o poziom halucynacji. Więc da się ten problem rozwiązać i da się skutecznie budować content z wykorzystaniem modeli językowych, jak zostaną nakarmione w taki sposób, jak Google karmi AI Overview. 0,7% po nakarmieniu. Przypominam, 60 bez. Także to jest ważne, bardzo ważne i musimy to zaadresować w tym kursie i zrobimy to. Kolejny problem. Stronniczość. No, to wynika z tego, że modele nie zostały na przykład wytrenowane na danych polskich. Jeżeli dane polskie stanowią jakiś korpus treningowy, no to jest to pewnie jakiś procent albo poniżej jednego procenta po prostu gdzieś coś po polsku się znalazło. Przez co będą stronnicze zapewne do języka angielskiego. No bo tam pewnie korpus językowy jest absolutnie największy. szczęśliwie pewnie niestronnicze do rosyjskiego, bo pewnie tam nie było języka rosyjskiego w procesie treningowym, albo był w małym stopniu, więc jakby to jest taka dobra informacja. Ale dlaczego to jest problem? Jeżeli pójdziesz do czata i każesz mu napisać artykuł na jakiś temat, to z dużym prawdopodobieństwem zostanie napisany z perspektywy amerykańskiej. Ze stuprocentową pewnością zostanie napisany z interpunkcją amerykańską, typu na przykład duża litera po dwukropku. Listowanie zupełnie inaczej się tworzy w języku polskim niż w języku amerykańskim. W języku amerykańskim czy w angielskim, co do zasady. Pierwszy element listy jest w dużej litery, drugi w dużej litery, trzeci w dużej litery i ostatni z dużej litery w listowaniu. W języku polskim, i to mi zawsze zwracali proofreaders czy edytorzy językowi, profesjonaliści w języku polskim, że tylko pierwszy element w języku polskim jest w dużej litery i kończy się przecinkiem, każdy następnie z małej litery i kończy się kropką. W języku angielskim tak się nie dzieje. Tak więc jeżeli pójdziesz do wczoraj, przepraszam, nie definiując zależności językowych, czy sposobu pisania, otrzymasz tekst sformatowany po amerykańsku po prostu. I wystarczy na niego spojrzeć, już wiesz, że on jest z czata, więc jakby to jest właśnie stronniczość i też będziemy tym zarządzać. Niedeterministyczne zachowanie. O tym już mówiłem. Model ci coś odpowie. Z jakimś prawdopodobieństwem trzeba tym zarządzić i bezpieczeństwo danych. To jest też ogromne wyzwanie modeli językowych na dzień dzisiejszy, bo twoje dane są wysyłane gdzieś. Na przykład do Stanów Zjednoczonych. Pewnie tam nie ma RODO takiego, jakiego jest w Unii Europejskiej. I OpenAI obiecuje nam, ale twoje dane nie będą brane w procesie treningowym pod uwagę. No okej. Tak jest w API. Jeżeli powiedzmy dzwonisz bezpośrednio, rozmawiasz z modelem. Natomiast w czacie, jak sobie czatujesz, Twoje dane mogą być brane pod uwagę w kolejnym procesie treningowym. Czat czasami ma te takie okienka feedbackowe, gdzie pytać się, w jaki sposób ci odpowiedział, co jest lepsze, co jest gorsze. I wyobraź sobie sytuację. Chcesz robić jakieś projekcje finansowe w swojej firmie, wrzucasz dane finansowe do czata, bo chcesz sobie coś policzyć, chcesz sobie policzyć EBITDA czy tam inne historie. Twoje dane są wzięte pod uwagę w procesie treningowym, bo tak się może wydarzyć. A ja później przychodzę i piszę, podaj mi wynik finansowy twojej firmy. No i na podstawie danych finansowych, które ty sobie trenowałeś, EBITDA czy tam jakieś forecasty finansowe, no możemy się dowiedzieć ile zarobiłeś pieniędzy, nie? Więc jakby tutaj musimy uważać, rozwiązanie są modele open source'owe, które również będziemy omawiać w tym kursie, ponieważ na dzień dzisiejszy jesteś w stanie postawić model nawet na swoim laptopie czy w swojej firmie. Bardzo dobry model i być w pełni bezpieczny. Możesz też wybrać firmy, które działają na terenie Unii Europejskiej, są trochę lepiej regulowane. Oczywiście są też firmy chińskie, no i tam już po prostu nie wiemy, co się z tym wydarza. Co do zasady brak pamięci? To jest kolejne wyzwanie modeli językowych. Jeżeli rozmawiasz z modelem językowym w sposób, powiedzmy, profesjonalny przez API, tworzysz proces, on nie ma pamięci, on jest jaki jest, on to jest black box. Wsadzasz, wyjmujesz, wsadzasz, wyjmujesz. Przychodzisz jutro i on nie pamięta, o czym z tobą rozmawiał ostatnio. Też będziemy tym zarządzać, tak żeby modele zyskały pamięć. Oczywiście, systemy czatowe typu chat GPT, Klode, Grok, one... wewnętrznie budują sobie pamięć w środowisku czatowym. To się zaczyna dziać, pamiętają twoje konwersacje, pamiętają kontekst, możesz zarządzić to projektowo, pokażę ci to w następnych lekcjach. Natomiast one działają na takiej zasadzie, że to jest pamięć w narzędziu. To nie jest pamięć modeli, bo model jest jaki jest, jest czarnym skrzynką. Pamięć jest w narzędziu czata, czyli właśnie oni obudowali go pamięcią. W tym kursie również pokażemy ci, jak taką pamięć zbudować sobie wewnętrznie. Kontekst window. W 2024 roku był to ogromny problem. W 2025 roku, zgodnie z naszymi przewidywaniami, z moimi przewidywaniami i Damiana, kontekst window się rozszerza, przestaje to być wyzwanie. W 2024 roku 128 tysięcy tokenów, czyli wyzwanie ciągle, bo nie możesz wsadzić ile chcesz. W tym roku jesteśmy na poziomie miliona tokenów, więc można tam wsadzić naprawdę bardzo dużo, ale pamiętaj, że to jest ciągle limitowane. To nie jest tak, że wsadzisz tam ile chcesz, masz limit. Więc ten kontekst window też jest istotny i często jest limitowana odpowiedź, czyli dobra, ok, w 2025 roku wsadzisz milion tokenów, czy pewnie dużo książek na przykład i ok, masz myśl, a ciągle odpowiedź jest limitowana po drugiej stronie na przykład do 16 czy 32 tysięcy tokenów, więc to nie jest tak, że ci napiszę książkę na jeden raz. Tutaj z tego wynika. No i parametry. Porozmawiamy o parametrach. To już nie są wyzwania, to nie są problemy, natomiast tak są opisywane modele językowe parametrami. Zrozummy to. Parametry, tak jak mówiłem o tym AI Brain Surgery, to jest właśnie to, że lewa półkula odpowiada za coś, prawa półkula odpowiada za coś. Możemy sobie wyobrazić to jak połączenia w mózgu, czy połączenia jakiejś tam sieci, mniej więcej. Im więcej parametrów, tym mądrzejszy model, co prezentuje następny slajd. Działa to mniej więcej na takiej zasadzie. Są małe modele, teraz raz pokażę, one mają mało parametrów i są duże modele. Małe mniej potrafią, bo mają mniej połączeń mózgowych, duże modele, teraz mówi się o modelach, które mają powiedzmy 600 miliardów parametrów, czyli 600, wyobraźmy sobie to jako połączeń mózgowych, po prostu potrafią dużo więcej. I widzimy, tutaj model właśnie rośnie i w momencie, kiedy osiąga powiedzmy te 540 miliardów parametrów, nagle on umie tłumaczyć żarty albo jakieś tam logiczne zadania, czego mały model nie umiał. Więc jakby tak się modele dzielą. Małe dure. Do tego jeszcze przejdziemy. Każdy model językowy jest opisany swojego rodzaju benchmarkami. To też się zmienia w czasie, bo okazuje się, że benchmarki, którymi modele językowe były opisywane jeszcze w zeszłym roku, przestają być aktualne. Dlaczego? Bo wszystkie modele zaczęły osiągać górne granice typu 89, 91, 95. nie było tego miejsca już od 91% na przykład do 100%, żeby rosnąć, dokładnie dywersyfikować, opisywać ten modelem, bo już są takie dobre po prostu, ale powiem Ci, jak dotychczas się to opisywało, bo jest to całkiem istotne. Kilka kluczowych benchmarków. Multitask Language Understanding, Graduate Level Google Proof, Math Work Problem, Evil Coding, Multilingual Grade School Math, Discrete Resonning Over Paragraph. To są przykładowe benchmarki. Więc większość modeli językowych, z którymi mamy do czynienia w 2025 roku, w każdym z tych benchmarków osiąga maksymalne wartości. Co tu jest istotne? To są zadania tekstowe, to są multiselekty często gęste, na przykład ten discrete reasoning over paragraph, typu model dostaje ileś paragrafów treści i ma na tej podstawie wnioskować. czy ten Human Evil Code Generation podświetle go. Jest to na przykład benchmark, który mówi o tym, jak dobrze model koduje. Albo jakieś zadania matematyczne, co widzimy wyżej. Albo jakieś zadania właśnie wnioskowania po języku i tak dalej. To są zawsze zadania tekstowe. W 2025 roku ja ten kurs nagrywam, tę lekcję nagrywam w zdaje się 24 bądź 5, to dzisiaj jest 25, 25 kwietnia. Dwa tygodnie temu dostaliśmy również informacje, że aktualne modele językowe rozwiązują test Turinga. Test Turinga działa w taki sposób, że jest powiedzmy AI, czy komputer, nazwijmy to po prostu komputerem i człowiek i sędzia. Sędzia ludzki, człowiek. I komputer daje odpowiedź i człowiek daje odpowiedź, a sędzia próbuje zgadnąć, co zostało wygenerowane przez komputer. No w tym roku już jakby test Turinga modele językowe zdają, człowiek się pomylił, zdaje się, w 75% przypadków na korzyść komputera. I każdy model jest opisany takimi benchmarkami, ale jest jeden najważniejszy, który wysuwa się na prowadzenie. Dlaczego? Ponieważ często gęsto jest wyścig między firmami, że o, ja mam taki ten human evil, ja mam coś takiego, ja mam coś takiego, ja mam coś takiego i przez te modele językowe zostały trenowane pod benchmarki po prostu na zasadzie, okej, trenujemy cię na konkretny zestaw zadań, żeby być wysoko w benchmarkach, ale one niekoniecznie są takie po prostu super, realnie. Dlatego powstał Humanity Last Exam. Coś takiego. To jest bardzo, bardzo ciekawa sprawa, bo tak jak powiedziałem, benchmarki już nie dojeżdżają. Nie ma miejsca na ocenę modeli, ponieważ no tak jak masz 95% na przykład w jakimś benchmarku, no to no to nie ma miejsca, żeby oceniać. W związku z tym naukowcy z całego świata się powiedzmy skrzyknęli, to był taki program finansowany duży i naukowcy z Politechnik, z Uniwersytetów, z Akademii z całego świata wymyślali zadania logiczne, na które nie da się odpowiedzieć w jeden prosty sposób. Jest to zestaw, powiedzmy, nie pamiętam ilu, ale pewnie, nie wiem, 200-300 pytań. Naukowcy za stworzenie takich pytań byli wynagradzani finansowo i to są pytania klasy takiej, że model musi się bardzo mocno zastanowić, wnioskować, poszukać informacji na temat tego, żeby rozwiązać zadanie. Możemy sobie wyobrazić jakieś odjechane zadanie, na przykład z fizyki kwantowej, jakiś powiedzmy, nie wiem, ruch elektronów, w jakiejś tam próżni, w jakiejś tam studni potencjałów, takie zadania miałem na mojej Politechnice na przykład. No tego się nie da w proze odpowiedzieć po prostu, bo to jest tak trudne. I właśnie tak wygląda Himanetielas exam. I modele na szczęście zaczynają być już tak opisywane. To nam troszeczkę dywersyfikuje jakby zrozumienie i to jest przykład Gemini 2.5 Pro. Obecnie chyba najlepszy model na świecie. No, czy nie wiem, kiedy oglądasz tą lekcję, być może już się dużo zmieniło, bo co tydzień są ruchy. Modele osiągają teraz w tym Humanity Lasexam poziom 18%, co do zasady prawie 19% poprawnych odpowiedzi na te super trudne pytania logiczne wymyślone przez naukowców i to powoduje, że jest dużo miejsca jeszcze na ten wzrost. Co ważne, to są modele, one same w sobie, na poziomie swojej wiedzy osiągają takie takie wyniki, więc jeżeli chcesz ocenić model językowy w sposób niezmanipulowany, no to właśnie patrzymy na Humanity Last Exam jako ten kluczowy benchmark. 18-8% 25 kwietnia. Tak to wygląda. Na pewno to się będzie rozpędzać. Też jest ciekawa nazwa. Humanity Last Exam. Ostatni egzamin ludzkości. Czyli co? Jak osiądniem 100%, to co będzie? No nie wiem, zobaczymy. Istnieją areny, takie leaderboardy. Możesz sobie wpisać w Google chatbot, arena, chatbot, leaderboard, AI leaderboard i oni te wszystkie benchmarki unifikują do jakiegoś jednego skoru, do jednej metryki. W ten sposób też jest dosyć łatwo się poruszać, no jak widzimy właśnie ten Gemini 2.5 Pro, który osiąga to 18-8% w Humanity Last Exam jest najlepszy na dzień dzisiejszy na świecie, więc możemy sobie w ten sposób łatwo ocenić, że okej, to jest dobry model do moich zadań albo najlepszy ogólnie, jeżeli chcemy to zrobić w klasy no-brainer. Ten jest najlepszy, bierzemy go. Możemy oceniać na zasadzie właśnie, na przykład jeżeli mamy konkretne zadania, bo jeden sobie poradzi w logice, czy w rezoningu, okej, to robimy to w ten sposób. Jeżeli mamy wyzwanie na przykład programowania, no to już spójrzmy na ten Human Evil, że na przykład, nie wiem, Cloud 3.7 Sonnet na dzień dzisiejszy będzie w tym najlepszy, no bo się po prostu w tym specjalizuje, jakby w zeszłym roku tego nie widzieliśmy. W 2025 roku widzimy zdecydowaną specjalizację modeli w jakimś tam kierunku, albo firm wręcz, OK, Antropic wyspecjalizuje się w programowaniu, a na przykład OpenAI będzie się specjalizować w czymś innym, takie rzeczy obserwujemy. Dobra, podzielmy te modele w jakiś tam sposób, żebyśmy lepiej się odnaleźli w tym świecie i umieli doskonale w nim poruszać. Przede wszystkim są modele małe, to są te ilości parametrów, które modele mają, czyli modele małe będą potrafić troszkę mniej, ale do niektórych zastosowań mogą być doskonałe. Małe modele też są dużo tańsze w utrzymaniu, dużo tańsze w promptowaniu, więc jakby to jest istotne, że na przykład, ok, mamy ogromne zadanie. Wyobraźmy sobie zadanie, nie wiem, klasyfikacji polskiego internetu. No, brzmi jak drogo. Ale mały model może się okazać do tego doskonały, żeby na przykład skategoryzować strony internetowe, bo to potrafi i będzie to na przykład 10 razy tańsze niż w przypadku modeli dużych. Takim najlepszą reprezentacją na dzień dzisiejszy małych modeli jest Gemma 3 od Google. Jest to model open source'owy, który możesz ściągnąć na swój komputer. tuneować, dostrajać jest za darmo i to jest najlepsza reprezentacja małego modelu, który jest świetny. Zobacz, to są właśnie te benchmarki i widzisz, już masz my tutaj ten score, jeden, tę zunifikowaną wartość. Nie mówimy, że to ma 40 benchmarków, nie, mamy jeden score i model, który ma 27 miliardów parametrów. To widzimy na dole, ja mam taki fajny pilot, pokażę ci to, to patrzymy tutaj, gdzie to jest, o, 27 miliardów parametrów. Praktycznie dorównuję modelowi, który ma 371 miliardów parametrów. Oczywiście są jakieś drobne różnice w tym skorze, ale małe modele zaczynają powiedzmy dojeżdżać w tym przykładzie DeepSea i to jeszcze DeepSea reasoningowego, o którym zaraz będę mówić, a raczej o reasoningu, więc jakby nie zamykajmy się na małe modele, no najlepsza na świecie jest na dzień dzisiejszy gamma. Jak oni to osiągnęli? Oni osiągnęli to w procesie kwantyzacji. Nie będziemy dokładnie tego omawiać w tym kursie, natomiast mniej więcej chodzi o to, że w trakcie treningu model był kwantyzowany, czyli powiedzmy liczby zmienną przecinkowe, tam 32-bitowe zostały na 8 bitów. Sorry. Po prostu był zmniejszany na tyle skutecznie, że nie utracił jakości, miał mniejszej ilości połączeń w mózgu, więc nie zamykamy się na małe modele. Gemma jest najlepszym przykładem, że warto je brać pod uwagę do jakichś zadat. No i modele duże. Modele duże, no to są te wszystkie, które widzimy. O3, O4, GPT-4O, Sonety, Gemini, to są modele duże. One mają setki tysięcy, setki miliardów parametrów. I jak rozpoznajemy duży, mały? Pokażę Ci. To jest bardzo proste, ale jak chcesz się poruszać w tym świecie, musisz to wiedzieć. Zawsze model jest opisany w ten sposób. 1B, 1 miliard parametrów, 27B to jest 27 miliardów parametrów. W ten sposób są opisywane, więc jeżeli musisz wybrać sobie model do swojego zadania, no to możesz w ten sposób, właśnie jak Ci pokazałem, zweryfikować jego jego rozmiar. Niektórzy producenci modeli językowych nie podają rozmiaru, na przykład OpenAI nie podaje rozmiaru, bardziej piszą mini, czyli GPT-4-O duży, GPT-4-O mini mały. Także tak możemy się poruszać w tym świecie, a ten 4-O mini jest też również bardzo ciekawy. Komercyjne i open source'owe. Komercyjne płacimy, są pozamykane OpenAI, Gemini, ale Gemma jest już od Google open source'owa, czyli możemy ściągnąć na swój komputer i po prostu sobie zainstalować w swojej organizacji, zapewnić bezpieczeństwo danych, zoptymalizować koszty. Będą na ten temat kolejne lekcje. Modele są również opisywane w ten sposób. Czat i Instruct. Czat, model skonfigurowany, żeby z Tobą rozmawiać, wymieniać opinie, czatować, odpowiadać Ci. Model Instruct jest to model instrukcyjny, który porusza się po Twoich instrukcjach tam zrealizować zadanie powiedzmy profesjonalne. Żeby Ci to lepiej wytłumaczyć. I to jest najczęstszy błąd, jaki podpieniają ludzie, którzy na przykład idą w świat open source'u. Wybierają sobie jakiś model, powiedzmy Lame, czy whatever sobie wybierają, i wybierają model czatowy. I tam chcą sobie zrobić na przykład keyword research albo jakąś ekstrakcję. No to się nie wydarzy, bo ten model jest skonfigurowany do rozmowy z Tobą, więc wymieni opinię z Tobą na temat keyword research'u. Model instrukcyjny jest to model profesjonalny, więc w ten sposób możemy jeszcze modele rozróżniać, jeżeli chcemy się lepiej poruszać w tym świecie. No i zobacz, tak też są opisywane. Ta stara lama, tutaj co ją widzimy z zeszłego roku jest 8b, czyli czatowa. Jak nie ma napisać czat, to jest czatowa zazwyczaj. No instrukcyjna, czyli skonfigurowana do wykonywania instrukcji. Więc jeżeli nie ma tutaj czata, no to to będzie czatowe. Co prawda w tym roku już coraz więcej się mówi o takich modelach uniwersalnych, ale ciągle chciałem Ci to powiedzieć, żebyś umiał się poruszać w tym świecie. Okej. Mamy rozwój modeli wizyjnych. Modele już powiedzmy widzą. Modele wnioskują po tym, co widzą. W 2025 roku możesz rzucić zdjęcia i kazać modelowi na przykład wnioskować, co tam się znajduje na tym zdjęciu, jak coś zmienić, albo rozmawiać ze zdjęciami. Pokażę Ci bardzo fajną lekcję na koniec tego panelu tego tygodnia dotyczącą notebook LM. Co tam zrobiliśmy właśnie z wykorzystaniem tej wizyjności, tego, że model widzi. No i najważniejsze, modele reasoningowe. Tak też się dzielą modele. Nie wszystkie mają funkcję wizji, ok? Nie wszystkie mają funkcję reasoningu, czyli tego myślenia, przemyśliwania tematu, rozbijania tematu na małe części, w myśl zasady, dziel i rząd, czyli podziel duży problem na zestaw małych problemów, suma małych problemów stanowi duży problem. Tak działają modele rezonningowe, to są te właśnie IQ 132, czy też Gemini, którego pokazywałem. Tak też dzielimy modele. I zobacz, to jest ten leaderboard, który Ci pokazałem wtedy w kontekście tego ArenaScore, a teraz jakby się przyjrzeć, najlepsze modele na świecie właśnie mają tą charakterystykę rezonningową. Czy coś pominąłem? Nie. Wszystkie, które zaznaczyłem, no to na dole, O4, O1 również są rezonningowe. Czy mają zastosowania PSEO? Mają. Ogromne, na przykład do tworzenia nagłówków, do filtrowania nagłówków, do ustawienia hierarchii nagłówków, do jakiejś logiki, treści i tak dalej. Do generowania treści, no nie za bardzo, ale do przygotowania wsadu pod generowanie treści, w pełności tak, to pokażę Ci również w tym kursie, mają gigantyczne zastosowanie dla programistów. Ponieważ programiści mają problemy logiczne właśnie, albo mają jakiś kod, który trzeba, nie wiem, napisać jakieś bardzo skomplikowane zapytanie do bazy danych, czy coś takiego. Tam znajdują super zastosowanie właśnie modele klasy rezonningowej. No i powoli zbliżając się do SEO, chociaż SEO będzie za jakiś czas w tym kursie, jakie mamy wyzwania? Po stronie modeli językowych mamy wyzwanie halucynacji, wyzwanie właśnie biasu, czyli stronniczości, mamy problemy z danymi, a raczej z danymi historycznymi i ok. Jeśli chodzi o wyzwania SEO, znaczenie danych historycznych. Coraz bardziej w SEO Google bierze pod uwagę dane historyczne i sygnały ludzkie. Jest to potwierdzone przed kongresem amerykańskim, jest to potwierdzone w Google Leaków, że Google bierze to pod uwagę. No i teraz my, jako specjaliści SEO, czy też osoby, które chcą być, nie wiem, wysoko w Google, pewnie mają wyzwanie, jak na treści AI zdobyć dane historyczne, skoro, czy nikt nie chce tej treści czytać, albo mało czytać, albo treść jest shalucynowana, no to jak na treści shalucynowanej, czy niskiej jakości ma pozyskać pozytywne dane historyczne, kiedy cały internet jest rozpędzony w Polsce od, nie wiem, 15-20 lat, już te dane historyczne ma. Ogromne wyzwanie ze strony SEO. Po stronie SEO będziemy również tym zarządzać. Kolejne. Inflacja treści. Treści w internecie jest po prostu za dużo. Jeżeli potrzebujesz usmażyć naleśniki, wystarczą ci dwa przepisy. Jeden z wodą mineralną gazowaną, drugi bez wody mineralnej gazowanej zazwykłą. Na przykład. Co do zasady tyle. To jest taki przykład, który zawsze podaję. Po prostu treści jest za dużo, treści jest trochę za darmo. Ekonomia treści jest taka, że kurczę, cena idzie do zera, więc przestaje być po prostu copywriting czy treść jakimś gigantycznym wyróżnikiem, jeśli mamy jeszcze modele językowe, które mają wyzwania halucynacji, no to jest to chyba jedno z największych wyzwań, które stoi przed specjalistami SEO i w tym kursie również tym będziemy zarządzać i koszt przetwarzania. To wynika bezpośrednio z inflacji treści, bo Google za każdą twoją stronę internetową, za każdy nie wiem, crawl, indeksację płaci. Więc trzeba sobie zadać pytanie, dlaczego ma zapłacić za treść z czata GPT, która ma halucynacje, inne problemy, albo masową publikację treści, dlaczego ma zaindeksować. No i właśnie obserwujemy w tym roku problemy z indeksacją, jakby nie jest przewagą konkurencyjną masowe generowanie treści po prostu w czacie, bo to fajnie, ale cała konkurencja to robi, albo wpadł na ten genialny pomysł, że tak będzie budować przewagi, a Google za to nie chce płacić, tym również zarządzimy w tym kursie w kolejnych tygodniach dotyczącym kontentu. No i koniec. Co się dzieje? AI Search. Musimy myśleć o w jednej strony wyszukiwarkach przyszłości, ale one trochę już istnieją. Nie wszystkie funkcje są jeszcze w pełni dostępne w Polsce, no ale tak może wyglądać wyszukiwarka produktowa na świecie. No tak robi to Perplexity i tak to będzie za jakiś czas wyglądać również w Polsce, w Google, więc pewnie to jest doskonały moment, żeby się zastanowić, jak tam się znaleźć w przyszłości. O tym również opowiemy w tym kursie. No i to, co stało się faktem kilka tygodni temu, czyli AI overview, no AI już jest w searchu, prawda? Google już odpowiada bezpośrednio w wynikach wyszukiwania. Gigantyczne wyzwanie SEO, żeby A, tam się znaleźć, B, żeby odebrać z tego jakiś ruch, zoptymalizować starą treść, na przykład inflacja treści. Pokaż kotku, co masz w środku. Jakbym zadał Ci pytanie, co jest na Twoim blogu 5 lat temu, to pewnie nie wiesz. Więc na tym będziemy się również zastanawiać i pokazywać, jak tam się znaleźć, jak to działa. Bo zobacz, z jednej strony Google zabierze Ci ruch, zabierze. Są badania ze Stanów. 30-37% CTR-u, czyli ruchu z AI Overview. ale masz narzędzie, które pozwala Ci rosnąć 10 razy szybciej, więc jakby też sobie pozastanawiamy się na ten temat w tym kursie, w kolejnych lekcjach. No i tyle. Zapraszam Cię do kolejnych lekcji. W kolejnej lekcji pokażę Ci podstawowe narzędzia, jak się po nich poruszać, co tam jest istotne, jakie są fajne funkcje, jak się odnaleźć w tym świecie. I na końcu podsumuję mój prywatny ranking, czego ja używam i co Ci polecam. Do zobaczenia. Cześć.

---

### Lekcja: Podstawowe Narzędzia AI

Cześć, witam Cię w kolejnej lekcji. W tej lekcji przejdziemy przez zestaw podstawowych narzędzi AI, tak żeby każdy z Was umiał poruszać się w tym świecie. Na pewno poruszacie się w czacie i znacie czat. To, co Wam pokażę, to przede wszystkim będzie czat. Poklikamy sobie po jego funkcjach, żeby lepiej zrozumieć, co to tam się dzieje. Pokażę Wam Perplexity, który jest bardzo przydatnym narzędziem w kontekście SEO, marketingu i ogólnie to jest po prostu wyszukiwarka przyszłości, więc przyszłości. Przyszło jest dziś. Więc jakby wyszukiwarka wiedzowa bardzo przydatna. Przejdziemy sobie przez Klodę, przejdziemy sobie przez Groka, którego osobiście uwielbiam i stosuję go pasjami. Przejdziemy sobie przez Gemini i skończymy na Google AI Studio, jako ten ostatni, ostatnie, ostatnie narzędzie, chyba o niczym nie zapomniałem. Czat. Czat każdy zna. Jeżeli nie zna, to zachęcam. Podstawowe narzędzie i najpopularniejsze narzędzie AI-owe na świecie. To, co jest super istotne, to są tutaj modele i wybieranie, umiejętność wybierania odpowiedniego modelu, ponieważ oferta i nazywnictwo w czacie, tutaj, że jak się rozwiniemy, będzie ich więcej, jest dosyć zawiła, a w związku z tym, że publikowane są kolejne modele co miesiąc, co miesiąc, co miesiąc, Powoli ciężko się odnaleźć w tym świecie, więc pokrótce Wam mniej więcej powiem, jak się tutaj nawigować i na co zwracać uwagę. GPT-840 jest to model, powiedzmy, na dzień dzisiejszy podstawowy do zadań typowych, prostych, szybkich podsumowań, szybkich generacji. Jest to model, który również został wypuszczony w zeszłym roku, więc na dzień dzisiejszy jest to, powiedzmy, starość. ale ciągle do zastosowań prostych możemy się pokusić o 4.0. Na końcu jest model 4.5, który jest modelem gigantycznym, arcydrogim, tam milion tokenów kosztuje na poziomie 150 dolarów. Ten model charakteryzuje się naprawdę potężną wiedzą i powiedzmy mocą w cudzysłowie, więc to są bardziej skomplikowane na operacje i jeżeli nie jesteśmy heavy user, chyba limit dzienny wynosi 50 zapytań do tego modelu, stosujmy go, bo jest największy, nierizoningowy i radzi sobie całkiem, całkiem. To, co widzimy poniżej, modele z tym przedrostkiem O, są to modele klasy reizoningowej, czyli to są modele, które powiedzmy, tak w cudzysłowie, myślą, wnioskują przed udzieleniem ci odpowiedzi, przemyślą temat, udzielając dopiero odpowiedzi. To ma ogromne zastosowanie np. dla programistów, gdzie mają np. skomplikowaną logikę algorytmu jakiegoś, albo jakiś problem logiczny, bo jakieś zapytanie SQL-owe, które trzeba przemyśleć, zoptymalizować, te modele znajdują tam największe zastosowanie. W kontekście SEO, gdzie widzę zastosowanie dla nich, na przykład do sortowania nagłówków, układania w logiczną całość nagłówków, wyciągania esencji z treści, czy czegoś takiego głębokiego, gdzie to nie jest przypadkowy ciąg znaków ze spasją, tylko trzeba by się na chwileczkę zastanowić, na przykład jakie nagłówki dobrać do kontentu, albo jaka wiedza jest kluczowa, albo jakiegoś porównania. mamy stronę A, stronę B, porównajmy ją. Nie w sposób przypadkowy, tylko zastanów się nad tym. Co ciekawe, ten model O3 również ma możliwość wnioskowania po zdjęciach, czyli wrzucimy mu jakiś obrazek i będziemy rozmawiać z obrazkiem i wnioskować po obrazku. Co się znajduje na obrazku, co jest kluczowe na obrazku, co zmienić na obrazku i tak dalej, więc jakby to jest bardzo fajne. I zobaczcie, ten model 4,5, który jest modelem właśnie nierizoningowym, czyli nie ma możliwości myślenia, powiedzmy w cudzysłowie, jest oceniany na poziom 100 IQ. Zamianym się zawsze śmiejemy, że średni człowiek ma 100 IQ, więc jakby to jest tego typu pomysł, tego typu, powiedzmy, no, klasa inteligencji, jeśli można mówić o średnim człowieku. O3 wykazują naukowcy, że ma poziom 132 IQ. Właśnie przez ten rezonik, przez to myślenie, więc jakby tutaj jesteśmy już na poziomie naprawdę jakiegoś nieco geniusza, ale przynajmniej jest dobrze. Te modele O4 są jeszcze w klasie mini, czyli to są modele, które dopiero się pojawiły i są to modele małe. Można stosować, ja akurat nie stosuję ich, natomiast doszukuję się zastosowań gdzieś przy bardziej programowaniu. Co jest ciekawe, w historii właśnie nazewnictwa modeli przez OpenAI, a jest duża oferta modelowa, Pominęli model O2, ponieważ jest to najpopularniejsza sieć telefonii komórkowej na Wyspach Brytyjskich. W związku z tym nie chcieli pozwów o znak towarowy czy coś tego typu. Więc było O1 i O3 od razu, O2 zostało pominięte w ramach ciekawostki. Co my tutaj widzimy? Okno czata, wszystko jasne. Natomiast jest tutaj trochę ukrytych rzeczy i trochę ukrytych funkcji, które warto sprawdzić. Jeżeli damy slash, to nam pojawi się taka sztuczka, czyli okno, gdzie mamy po prostu rozwinięte jego funkcjonowanie. funkcjonalności, które są naprawdę bardzo ciekawe, na przykład wyszukiwanie informacji, czyli możemy dzięki temu, mamy pewność, że model pójdzie poszukać informacji w internecie na jakiś konkretny temat, na przykład jaka dziś jest pogoda w Warszawie i w tym momencie on sobie gdzieś pójdzie do internetu, przeszuka sieć i mi odpowie faktycznie, bo wiemy już, że największym wyzwaniem modeli językowych jest alucynacja i problem braku aktualnej wiedzy. Te modele często mają wiedzę odciętą rok, dwa, lata temu. A tu widzimy jasno, że dzisiaj jest 24 kwietnia, czyli wiecie, kiedy nagrywam ten kurs. Wiemy, jaka jest pogoda, czyli za oknem, czy za tą ścianą mamy 22 stopnie Celsjusza, więc jest fantastycznie. I to jest prawda. Mamy taką funkcję wyszukiwania. Więc fajnie. Wracamy do początku. Co my tu jeszcze mamy? Mamy funkcję stworzenia obrazów. To są te najnowsze modele do generacji obrazu. One akurat chyba działają na 4O. Stwórz obraz. To są te obrazy, które trendowały w internecie w kontekście Gilbi i to zrobiło ogromny wiral dla czata. Stwórzmy jakiś obraz. Powiedzmy, stwórz obraz AI Ninja w studio nagraniowym. Także niech mnie stworzy. Zobaczymy, jak mnie widzi wszuszczą inteligencję na podstawie takiego promptu. Niech to się dzieje w tle. A my otwieramy sobie kolejne okno, później wrócimy do tej funkcji. Czyli jeżeli chcemy tworzyć zdjęcie, stwórz obraz. To, co jest arcyprzydatne dla marketerów, specjalistów, jest tutaj opcja Canva. Jest to opcja edytora. Jak sobie wejdziemy jeszcze raz slash, damy Canva i piszemy napisz mi prosty skrypt php. Otworzy nam się edytor. Ojojoj, nie chce mi się otworzyć kredytor. Ktoś korzysta z mojego czata. Otwórz. Canvas. Możemy tak w ten sposób mu kazać to otworzyć. Oj, oj, oj, oj, oj, oj. Ach, ten Canvas. Jeszcze raz. Banowali mnie do tego, że po prostu jestem na firmowym koncie i wstyd się przyznać, ale ktoś korzysta teraz z konta. Ale chwali się, 1620 ktoś pracuje. Jeszcze raz. Dobra, szanowni państwo, mieliśmy mały problem techniczny na moim koncie chat GPT. Po prostu w Vestigio jeszcze aktywnie pracują i mam obcięte konto, ale Mateusz użyczył mi swojego laptopa, więc jesteśmy na jego koncie czata. Pokażę Wam teraz właśnie tą kanwę, jak otworzyć. Czyli pierwsza sprawa, jeszcze raz ten slash, mamy opcję canvas i w tym momencie będziemy mieli właśnie, zobaczycie co, bardzo przydatny edytor, który Wam się przydatny w codiennej pracy. Bierzecie prawie ten slash, kanwaj, piszecie na przykład napisz mi bardzo prosty skrypt PHP. i zobaczcie teraz co się dzieje otwiera się okno, to u mnie nie działało chat GPT teraz ma tak, że jak dużo ludzi korzysta z jednego konta, a u mnie to jest możliwe, że tak się dzieje to automatycznie obniża funkcjonalność do niższej no tak jest no ale ok, co my tutaj widzimy weszliśmy w kanwasy, czyli dostajemy od nich edytor i możemy na przykład zepsuć kod i mu napisać naprawę napraw mi kod. Bardzo przydatna funkcja właśnie debugowania, jeżeli coś się zepsuło, możemy sobie rozmawiać z naszym kodem, albo jak tu będzie plan na główku, zaraz sobie to otworzymy, to będziemy mogli pracować z treścią, rozwijać ją, rozmawiać o treści i kazać mu pracować. W kontekście programowania również znajdujemy tutaj trochę fajnych funkcjonalności, bo teraz jesteśmy przy pisaniu kodu, bardzo przydatne funkcje. Możemy dodać komentarze do kodu, możemy dodać jakiś tam dziennik zmian, naprawić jakieś tam błędy, jakiś port do języka. Czyli port do języka polega na tym, że mamy napisane w języku PHP, a chcemy mieć w Pythonie. No i jak go odpalimy, no to zaraz nam przepisze ten kod z PHP na Pythona. Również bardzo przydatne. Ja obecnie przesiadam się z PHP właśnie na Pythona i często wykorzystuję AI do przepisywania po prostu kodu. O, tutaj coś u Mateusza widzę, też jest mały lag. złośliwość, złośliwość, złośliwość. Prosty coś tutaj napisało. Prosty skrypt Python. A, tutaj trzeba było kliknąć. Jest przepisane. Okej. Mamy przepisane na Pythona i co mamy tutaj ciekawe? Czat ostatnio również udostępnił możliwość uruchamiania kodu w sobie, czyli jesteśmy w stanie na przykład edytować jakiś tam element strony internetowej i sobie robić podgląd. Więc jak damy uruchom, no to mamy, uruchomiona konsolę, że witaj, witaj świecie, czyli to co mieliśmy mieliśmy uzyskać, czyli świecie i bitaj, prawda? Czyli uzyskaliśmy ten kod. Bardzo przydatna rzecz. Jeszcze czemu? Możemy się tym podzielić. Czyli mamy jakiś tam fragment kodu, czy coś nad czym pracujemy, to mamy funkcję udostępnienia i naszemu współpracownikowi jesteśmy w stanie wysłać link i on będzie pracował na tym samym, co my. Wracając do kontekstu SEO, pewnie fajnie będzie tworzyć jakiś content z wykorzystaniem czata, nie? No i do tego Canvas znajduje również doskonałe zastosowanie. Napiszmy mu napisz mi plan nagłówków dla artykułu o kortyzolu. Mój ulubiony kortyzol. No i dobra. Otrzymujemy jakiś plan nagłówków. Nie wchodzimy w to, czy on jest jakby profesjonalny, czy taki ma być, czy nie, czy on ma sens, czy to nie ma znaczenia. dla nas. Otrzymujemy tekst i co do tekstu dostaniemy zupełnie inny zestaw funkcjonalności, który nam daje Canvas. Widzimy tutaj, możemy jakby jakieś edycje prowadzić, dostosować długość, poziom odczytu, udoskonalać, dodawać emoji, więc bardzo przydatne. Do tego wszystkiego czatować z naszą pracą i udostępniać, powielać. Fantastycznie. Dobra, wróćmy dalej do czata. Co on jeszcze potrafi? Czat również ma GPT-sy, czyli możliwość konfiguracji modelu do Waszych konkretnych zastosowań. Nie będę tego omawiać w tej lekcji. Macie dedykowane dwie następne lekcje dotyczące GPT-sów i dwa ćwiczenia dotyczące GPT-s. Tam będę to pokazywać. Co my tutaj jeszcze mamy? Coś dosyć ciekawego, ale chyba już poproszę Mateusza, żeby przepiął mi komputer. No dobra, jesteśmy już na moim laptopie. W międzyczasie możemy zobaczyć, jak wygląda Ninja. w studiu nagraniowym. To też zostało zrobione w czacie. Oczywiście tutaj na co warto zwrócić uwagę, no są litery. Do tej pory była plastelina, jeśli chodzi o modele graficzne. No to, a co, to chyba się nie udało. No ale co do zasady, dostaliśmy jakieś zdjęcie dużo lepsze niż to było do tej pory, więc czat potrafi to robić, ale też potrafi na przykład dokonywać edycji, czyli wrzucasz mu zdjęcie i możesz coś dokleić do tego zdjęcia. Zaraz wam to pokażę. Ponieważ w czacie pojawiła się taka opcja typu biblioteka bibliotece mamy wszystkie zdjęcia, które zostały w jakiś sposób wygenerowane. I tutaj mamy przykład zdjęcia, które moja siostra wygenerowała w czacie, czyli wrzuciła zdjęcie psa rasy Beagle, wrzuciła swoje szelki, którymi handluje w swoim sklepie internetowym i nałożyła szelki na psa. Więc takie rzeczy potrafi robić czat, a my mamy punkcję biblioteki, czyli po prostu podglądu tego, co robiliśmy do tej pory. Co tutaj mamy więcej w czacie? Mamy możliwość projektów, czyli na przykład jakiś tam Szymon, to jest jeden z programistów w Estigio ma swoje projekty, gdzie sobie tutaj widzimy moduł skrapowania, skrapowanie treści, jakby realizuje jakieś swoje projekty. Ja mam jakieś moje programowanie, jakieś Roberty, także możemy sobie poukładać w projekty nasze czaty. Niestety nie ma to takich dobrych funkcjonalności jak w innych narzędziach, które zaraz Wam pokażę, ponieważ projekt stanowi co do zasady tego katalog, gdzie jesteśmy w stanie porządkować. Projekt nie ma jakiejś dodatkowej wiedzy, więc cała historia wszystkich czatów zapewne będzie brana pod uwagę w tym konkretnym projekcie, chyba to nie działa w ten sposób, że następuje jakakolwiek separacja. To, co jest jeszcze warte podkreślenia, to są GPT-sy, nie będziemy tworzyć, ale na pewno mamy marketplace z GPT-sów, gdzie możemy się poruszać i stosować gotowe rozwiązania skonfigurowane do danych czynności, których potrzebujemy. I chyba tyle, tu jest jeszcze jedna funkcja zbadaj głęboko, ale o niej będziemy rozmawiać w osobnej lekcji, więc nie będziemy tego prezentować teraz. Także tyle. Chat GPT fajnie się rozwija i z tego co widać jako trend narzędzie bardziej staje się produktem i feature'ami i funkcjonalnościami, aniżeli nowymi modelami, mimo że cały czas dostają. Działa coraz fajniej, generuje fajne obrazki. Canvas jest rewelacyjny do pracy z treścią, do pracy z kodem, może troszkę mniej, ale ciągle dla małych zastosowanie jest rewelacyjne. Także mocna polecajka i na pewno entry level jak najbardziej. Kolejnym narzędziem, którym chciałbym Państwu przedstawić jest Perplexity. Jest to stricte wyszukiwarka AI-owa, która pozyskuje wiedzę z internetu, aktualną wiedzę i syntezuje do zjadliwej formy. jest bardzo, bardzo korzystne z perspektywy SEO, chociażby po to, żeby rozpocząć pracę w kanbasach w czacie nad naszym artykułem, pójdę do Perplexity, wyszukać aktualnej wiedzy, kontekstu informacji i wrócić do czatu, przepraszam, żeby dalej z nim pracować. Co tutaj mamy ciekawego? Przede wszystkim badania, czyli zaopansowaną analizę tematyczną, ale to sobie omówimy w kolejnej lekcji. I mamy tutaj opcję wyszukiwania, mamy opcję włączoną Pro. Perplexity ma to do siebie, że on sam wybierze nam tryb. W zależności od złożoności problemu na chwilę obecną, wcześniej mieliśmy możliwość konfiguracji, jakiego trybu chcemy użyć. W tym momencie on sam nam dostosuje tryb na przykład do trudności naszego zapytania, więc potencjalnie możemy zapytać, jaki powerbank można zabrać na pokład linii lotniczej lot. Pewnie jest to typowe pytanie. Pewnie jest to pytanie, gdzie nie chce nam się czytać artykułu, na czyjejś stronie internetowej, jak i Powerbank, możemy zapakować. Więc Perplexity jest doskonałym narzędziem, żeby tą robotę wykonać za nas. Jak widzimy, poszli na stronę lotu, przeczytali stronę lotu i dostajemy syntezę wszystkich informacji. Oczywiście z cytowaniem jest bardzo ciekawe. I zachęcam, żeby na przykład tak pozyskaną wiedzę skopiować i pójść z tym do czata i w ten sposób pracować ze swoją treścią, żeby dane były zawsze, uwaga, faktyczne i prawdziwe. Co my tutaj jeszcze więcej mamy? Oczywiście mamy możliwość wyboru modelu, no ale tych modeli jest tyle, że właśnie zrobili to, czego brakuje w czacie, po prostu wybierz najlepszy, więc jakby to jest spoko, ale bardzo mi się podoba to, że oni są w stanie wybierać modele, które nie są ich, bo w czacie mamy tylko OpenAI, wszędzie w każdym innym środowisku będą modele tylko danego producenta. Tutaj możemy wybrać Antropika, możemy iść do OpenAI, możemy wybrać Googla, możemy wybrać Groka, czyli XAI, bardzo fajnie, czyli mamy wybór, możemy bierać modelek, na których chcemy pracować. Bardzo miłe. Mamy obrazy, więc tutaj możemy wyszukiwać sobie informacje, które nam są potrzebne jako obrazy, odpowiedzi, więc jakby co do zasady tyle. Oczywiście naszego Perplexity, czy nasz wynik jesteśmy w stanie udostępnić i to też ostatnio udało mi się wykorzystać i było bardzo przydatne, ponieważ musiałem komuś wytłumaczyć, czym jest format Mastermind z formą HotSeed i nie chciało mi się tego pisać, więc zapytałem Perplexity co to jest, zobaczyłem, że odpowiedź jest taka, jaka powinna być i po prostu sobie wysłałem link. Masz, tu masz informacje, co ja od ciebie chcę, ja chcę, to przeczytaj sobie. I efekt tego ćwiczenia był taki, że osoba mi odpisała, że bardzo dziękuję za wyjaśnienie, nareszcie to rozumie, więc jakby tutaj jest w porządku. Dobra, co jest fajnego w Perplexity? Zaraz sobie tego poszukamy, tylko to jest na stronie głównej. Perplexity ma swoje API. Tutaj nie wiem, czy na tym komputerze jesteśmy zalogowani, czy nie, to nie ma znaczenia, natomiast te modele, które stoją za Perplexity, za tym głębokim reasoningiem i za innym deep researchem, są dostępne. Więc jakby to jest bardzo fajnie, to nie jest temat dzisiejszej lekcji, ale wiemy, że są te modele dostępne. Jeżeli chcemy zbudować nasz proces profesjonalnie w oparciu o Perplexity, tak jest to możliwe, tak jest to bardzo kuszące. Będziemy się tym bawić w kolejnych lekcjach. To my tutaj jeszcze mamy w tym Perplexity. Wiadomo, pogoda, aktualne newsy, jakieś przestrzenie. No fajna zabawka. Raczej odkrywanie, czy jakieś newsy. Perplexity ma zakusy, żeby być po prostu takim, no powiedzmy, portalem, gdzie ludzie poszukują wiedzy, informacji. I fajnie. Pojawiają się sygnały, że Perplexity chce przejąć amerykańskiego TikToka, więc jest to bardzo ciekawe, jak się wydarzy. To bardzo może zmienić search. Podsumowanie Perplexity. Wyszukiwarka, synteza wiedzy, Synteza wiadomości, ściąganie aktualnej wiedzy, budowanie aktualnej wiedzy w serczu i w kontekście, w którym się znajdujemy i fajnie ją stąd zabrać i przenieść do naszego ulubionego edytora, do czata, czy gdziekolwiek pracujemy. Do tego wszystkiego jeszcze możliwość API, tak żeby włączyć Perplexity w nasz proces. Fajnie. Dobra, szanowni Państwo, wracamy do kolejnego narzędzia. Tym narzędziem teraz będzie grog, czyli produkt od X. nie dajcie się zmylić, jestem na moim profilu X-owym. Natomiast tutaj znajduje się zakładka GROK i jest to fantastyczny zarówno model, jak i narzędzie, którego pasjami wykorzystuję do programowania, jakby się przyjrzeć mojej historii. To tu jest pisanie, tu jest pisanie, tu jest pisanie kodu po prostu i działa z tym fantastycznie. Jeżeli jesteście programistami albo potrzebujecie pisać kod, to jest super. Co do zasady mamy jeden model, GROK. To jest interfejs w X-ie. ma oczywiście ten deep search, to w następnej lekcji ma opcję thinkingu, czyli głębokiego myślenia o danym temacie. Bardzo przydatne. Jak ja mam problem logiczny, polegający na przykład na przetworzeniu jakichś skryptów, czy stworzeniu jakiegoś systemu, to połącza mu thinking, żeby się mocno nad tym zastanowił i to fajnie działa, edycję zdjęć. Natomiast to jest interfejs w X. Ja mam konto premium na X, w związku z czym zostałem obdarowany grokiem w wersji SuperGrog w domenie grog.com. I tutaj znajduje się więcej opcji, które są bardzo ciekawe. Co do zasady to jest tak, że praktycznie wszystkie te narzędzia mają bardzo podobne funkcjonalności, natomiast grok zaczyna z czymś, czego nie ma zarówno w Perplexity, jak i nie ma w czacie GPT i od razu do tego przejdę, czyli do Workspaces. Czyli jesteśmy w stanie sobie robić jakby, powiedzmy nie wiem, obszary robocze, w których będziemy pracować tylko i wyłącznie w danym kontekście. czyli na przykład mamy jakiś tam kontekst naszym kontekstem będzie napisz mi prosty skrypt w Python pokazujący cyfr dobra, starczy i w tym momencie mamy ten workspace który już będzie umiejscowiony na jakimś konkretnym kontekście w tym kontekście mamy akurat pisanie w Pythonie i każda kolejna iteracja a każdy kolejny proces będzie już umieszczony w kontekście z historią, z poprzednimi skryptami. To jest niesamowicie korzystne. Dodatkowo możemy do tego dokleić na przykład dane. Te dane to jest na przykład nasz dotychczasowy system albo dotychczasowe pliki, albo dotychczasowy content, albo dotychczasowe, nie wiem, wytyczne redakcyjne, czy cokolwiek sobie dokleimy i to będzie nam pracować w tym jednym systemie. Jak założymy sobie nowy, to mamy jakby separację i kolejny kontekst, czyli potencjalnie z perspektywy agencji SEO ja mogę mieć 40 klientów, 40 workspaców i w każdym workspacie pracuję tylko na danych dotyczących danego klienta, jego kontentu, jego wytycznych, jego innych historii i w ten sposób bardzo fajnie jestem sobie w stanie to segmentować. Tego nie ma w innych narzędziach, znaczy jest w klodę, zaraz pokażę, natomiast to mi się bardzo spodobało w Groku. Dodatkowo możemy do każdego workspacu dodać wytyczne, czyli powiedzmy jakieś wytyczne systemowe, jeśli to jest na przykład programowanie, to wrzucam tam wytyczne dotyczące danego systemu, jak oczekuję. Jeżeli to jest na przykład aspekt kontentowy w kontekście SEO danego klienta, jakieś wytyczne dotyczące kontentu SEO, priorytety, niepriorytety, tone of voice, brand i tam inne historie, więc możemy sobie to skonfigurować i bardzo fajny sposób tutaj z tym sobie współpracować. Więc jakby polecam, to są jakby historie konwersacji, więc jakby to jest bardzo przyjemne. Wracając na stronę główną groka, oczywiście mamy to, że jakieś są persony. Ja tego nie stosuję, natomiast jeżeli ktoś chce mieć lojalnego przyjaciela, no to powiedzmy będzie miał lojalnego przyjaciela, prawda? Edycję zdjęć, to jest fajne. Wrócamy zdjęcie, zmień coś, działa. Tworzenie zdjęć, okej, ale nie jest tak dobre jak w czacie. I research. Research jest bardzo ciekawy. Łacza się opcja deep researchu i deeper searchu, która będzie nam przeszukiwać zasoby bardzo głęboko w celu jakby syntezy wiadomości. O tym będzie następna lekcja. No i co? Można mu włączyć search, ma search, plus co jest fajne, ma search wewnątrz X, czyli platformy społecznościowe. Co tam trenduje, co tam się dzieje, on już to wie, jest to w jego bazie wiedzy, to go mocno odróżnia od konkurencji. I na przykład jeżeli jesteśmy w kontekście politycznym, albo coś tego typu, no to ta wiedza jest na bieżąco, typu o czym napisał dzisiaj Donald Tusk, czy ktoś inny to już to tam jest i co do zasady tylko tam. Więc podsumowanie. Grok. Super. Ja jestem zachwycony osobiście grokiem. Model groka, jeżeli byśmy sobie wpisali powiedzmy LM Leaderboard to pewnie nie powinien czat bodaryna. No na dzień dzisiejszy znajduję na raz, dwa, trzy, na czwartym miejscu przez dłuższy okres czasu był Na pierwszym miejscu teraz jakby mamy powrót do OpenAI i do Google, do którego zaraz przejdziemy, natomiast to jest bardzo, bardzo wysoko. Polecam. Tutaj mamy opcję również thinkingu i deep researchu. Już ma jedną opcję, ma canvas, tylko że ta opcja została udostępniona na świecie kilka dni temu i raz mi się włączy, raz mi się nie włączy. Dobra, dajmy temu szansę. Jeżeli chcemy, na przykład napisz mi prosty skrypt PHP, otwórz w Canvas. Być może zadziała, być może nie. Dzisiaj mi nie zadziałało, wczoraj mi działało. No jeszcze to nie działa, więc to się narzędzie cały czas akurat rozwija i są dokładane nowe funkcje. Jest ogłoszone, że jest udostępnione, ale widocznie jeszcze w Stanach Zjednoczonych i do nas musi to przyjść. Dobra, chodźmy do kolejnego narzędzia, które jest jakby bardzo podstawowe, ale również bardzo przydatne. Jest to Cloder. Bardzo przydatne narzędzie, w szczególności w kontekście programowania i wszystko wskazuje na to, że firma, która stoi za Clodę, czyli firma Anthropic, zaczyna się w tym wysoce specjalizować. Modele są też tak tuningowane. To są twórcy tego protokołu MCP, również dla programistów i taki też jest Cloder. Doszukuje się to głównie takich zastosowań, ale oczywiście świetnie sobie radzi z tekstem, z dadaniami SEO, natomiast z programowaniem zdecydowanie najlepiej. Co tutaj mamy? Modele. Co do zasady mamy, no po sumie będziemy używać jednego, czyli tego 3,7 Sonnet. Mamy jakieś starsze modele, ale jakby na dzień dzisiejszy nie widzę dla nich zastosowań, bo to jest po prostu najlepsze. Standardowo okno czatał i kilka rzeczy, których nie widzieliśmy wcześniej. Pierwsza rzecz. Take screenshot. OK, tam nie było, tam było wrzuć plik, tutaj możemy zrobić screena, miło, ale to, co jest ważne dla programistów, i tu widać właśnie specjalizację, możemy dodać GitHuba, możemy dodać nasze reprezentacje na GitHubie i one zostaną wciągnięte i będziemy z nimi czatować, programować. To jest fantastyczna funkcja, przeleciała troszeczkę poniżej radaru mainstreamowego, ale to jest bardzo fajnie. Możemy też dodać nasze pliki, zsynchronizować się z Google Drive'em, przez co będziemy w stanie pracować na naszych plikach, raportach, istniejącym kontencie, czy co tam mamy na Google Drive'ie. Możemy też jakby robić to w projektach, do których zaraz przejdziemy. To, co mi się podoba w Cloudę, to są tak zwane artefakty. Czyli jesteśmy w stanie, powiedzmy, napisz mi prosty licznik HTML i JS liczący od 10 do 0. On to wykona jako model specjalizujący się w programowaniu. Powinien mi otworzyć taki artefakt, który znajdzie się po prawej stronie. Drafting Artefact. No i zaczyna nam to po prostu pisać w osobnym okienku, czyli to, co dla programistów jest bardzo korzystne. Programowanie po prawej, kodowanie po lewej, więc bardzo fajnie. I właśnie to, co chciałem Wam pokazać, czyli automatycznie interpretacja. Jesteśmy w stanie widzieć w klodę to, nad czym pracujemy i dewelopować sobie live. Mamy na przykład tutaj przycisk reset. Możemy mu napisać zmień na czerwony i on przepisze nam kod i zaraz go nam zaprezentuje po prawej stronie. O, zmienił. Widzicie, zmienia style. I zaraz nam zaprezentuje ten kod. To, co wprawne oko właśnie zobaczyło, a ja wam wytłumaczę, to jest to, że dotychczas modelek, które były stosowane do programowania, i to jest kolejna przewaga, Clode, mamy czerwony przycisk, jeżeli dałeś mu zadanie zmienić coś w kodzie, to on przepisywał cały kod od góry do dołu. A Clode zrobiło tylko edycję w konkretnym miejscu. To właśnie świadczyło ich specjalizacji w programowaniu, że nie muszą przepisać całości, tylko jednak edytują kluczowe miejsca. Okej, mamy jakąś opcję publish, więc jesteśmy w stanie się z tym share'ować z naszym teamem, jesteśmy w stanie to kopiować, downloadować, jesteśmy w stanie wersjonować. Dla programistów jest to rzecz naprawdę bardzo, bardzo przydatna i na dzień dzisiejszy w mojej firmie Vestigio dużo kodu tutaj powstaje. Dodatkowo jak ja używam kursora, o którym również będziemy mówić w tym kursie, to zazwyczaj jest on zasilony właśnie tym modelem od Antropica, czyli 3,5 sonet. Dobra, kolejna rzecz, która tutaj wydaje się być bardzo fajna, to są również projekty, które w kontekście programowania wydaje mi się, że są bardzo ciekawe, ale również w kontekście pracy SEO, tworzenia treści. Zobaczcie, oczywiście mamy tutaj czatowanie, czatowanie, czatowanie, ale mamy tutaj też instrukcję, czyli jeżeli chcemy pisać na przykład stronę internetową o czymś albo konkretną instrukcję systemową, mamy na przykład jakiś konkretny system, nad którym pracujemy, tworzymy i on ma konkretne wytyczne co do funkcjonalności i innych historii, wrzucamy tutaj i system już będzie pamiętał, co my w ogóle piszemy, prawda? Daniel będzie pokazywał to w swojej części na przykładzie tworzenia stron w Lowy Bull. To są właśnie te instrukcje. Dodatkowo jesteśmy w stanie dorzucać wiedzę i po raz kolejny, to czego nie ma nigdzie, Google Drive, synchronizacja z naszym drivem i GitHub, czyli konkretnie nasz projekt stworzony tutaj, pracujemy nad stroną internetową Sensei Academy, podpiętą do GitHub'a, synchronizacja i pracujemy na konkretnym produkcie, a nie jak sobie po prostu wymyślamy jakieś tam rzeczy. No i oczywiście tutaj też jakieś tam synchronizacje, jakieś tam konfiguracje, możliwość używania stylów. Co ciekawe, jesteśmy w stanie stworzyć swój styl. Jeżeli byśmy chcieli pisać treść, no to powiedzmy, nie wiem, nasz brand voice, tone of voice charakteryzuje się jakimś konkretnym stylem i tak chcemy go utrzymać, więc sobie tutaj definiujemy swój własny styl czy tone of voice. No i tak też będzie tworzony content dla nas, więc jakby bardzo przydatne narzędzie. Chyba w szczególności przez te projekty i te możliwości synchronizacji, których nie prezentuje konkurencja. Mocna polecajka co do Klode. Na pewno dla programistów, na pewno dla osób, które chcą pracować w kodzie. No i jeżeli chcesz mieć fajnie poukładane projekty, odseparowane z instrukcjami, no to grog ci to da. Klody też, co Ci da. ChatGPT, Perplexity Ci tego nie da, więc jakby dla profesjonalistów również bardzo mocna polecajka. No i dobra, chodźmy do następnego narzędzia. Następnym narzędziem jest Gemini. Jak widzę, mam wersję Advance, bo mamy Westigio, mamy tą chmurę Google, Gmaila i wszystkie inne rzeczy, więc widocznie dostaliśmy za darmo. Co tutaj mamy? Kurczę, to samo co wszędzie, ponieważ teraz jest trochę wyścig na funkcjonalności, a praktycznie każdy ma te same, chociaż tutaj nie ma tego, co mi się najbardziej spodobało w poprzednim Groku i Klode. No nie mamy projektów, więc nie jesteśmy w stanie się separować, no ale powiedzmy, jeżeli to jest dedykowane do konta Gmail, czy G Suite, lepiej mówić, no to każdy ma swój, no ale ciągle tych kontekstów tam będzie dużo, więc nie ma nie ma jakiejś tam wielkiej separacji tych. No ale okej. Co my tutaj mamy? Wrzucanie plików i wrzucanie zdjęć. No miło, ale widzicie tutaj, no tu widzicie, no muszę sobie włączyć, administrator musi mi włączyć Google Drive'a, a w CloudEich jakby to mam, plus GitHub, więc funkcji mniej, Deep Research, który w następnej lekcji, no i Canvas, mam nadzieję, że tutaj mi się chociaż otworzy ten Canvas w Gemini, napisz mi prosty skrypt PHP o, coś tam rozkminił, dobra mamy Canvas, mamy Canvas więc chyba w pierwszy raz się udało dzisiaj otworzyć go u mnie. Co ciekawe, tutaj jest jakieś wersjonowanie. Dalej tego nie widziałem w innych rozwiązaniach. Oczywiście możliwość czatowania z moim kodem, kolorowanie składni, kopiowanie. No ale widzicie, nie ma opti-sharowania, jakichś takich rzeczy. To są okej, to są mega proste rzeczy. No ale czasami są przydatne. Tak jak na przykładzie tego klode projektów albo share'owania jakichś rzeczy w canvasach Choto GPT, no tutaj niestety tego tego nie widzę. Co my tutaj mamy? Oczywiście mamy dostęp do przeróżnych modeli i teraz tak, to co mam wybrane, ten 2,5 Pro Experimental to jest obecnie najlepszy model językowy na świecie i on jest tutaj w pakiecie, więc jakby jeżeli ktoś potrzebuje do głębokiego rezoningu, myślenia, no to jest to bardzo przydatne i pewnie tutaj bym postawił po stronie Gemini na jakieś naprawdę skomplikowane rzeczy, typu chcesz stworzyć dokumentację techniczną, oprogramowania, czy jakieś coś poważnego, czy przetworzyć jakieś dane, albo się mocno zastanowić, no to pewnie bym tutaj przyszedł, przez to, że ten model na dzień dzisiejszy jest najlepszy na świecie, więc pewnie tutaj uzyskamy jakiś pi razy, czyli najlepszy możliwy efekt. Deep Research to pokazujemy w następnej lekcji. No i co? No i Gemini jak Gemini. No Rocket Science. Kolejny czat. Nie za dużo funkcji. Na szczęście Google bardzo szybko się rozbija, więc zakładam, że tutaj przybędzie. Natomiast od sieczą przychodzi dla nas również Google AI Studio. Jest to rzecz, której pewnie się nie będziecie zbyt często pojawiać, ale być może będziecie. No dobra. Gemini jest to proste zastosowanie, czyli ja mam w swoim koncie Google G Suite Gemini Advanced, mam dostęp do modeli, ja sobie czatuję, klikam i sobie używam i generalnie jesteśmy zadowoleni. Google AI Studio jest to trochę bardziej zaawansowane środowisko, w którym przede wszystkim jesteśmy w stanie konfigurować tego Gemini'a, to Damian będzie omawiać w swoich lekcjach jakby znaczenie tych funkcji temperatury i tak dalej. Natomiast mamy trochę większy wachlarz modeli, bo możemy korzystać z modeli Gemma, modeli 1,5. No zapewne w większości przypadków będziemy korzystać z tego modelu, ale co do zasady mamy tutaj większy wachlarz dostępnych modeli. Mamy opcję konfiguracji, to wyprzedzając Damiana Wam powiem jak to wygląda. Wartość 0 jest to 0 kreatywności, wartość 2 to jest kreatywność klasy LSD. Wartość 1 to jest po prostu kreatywność, powiedzmy. No i tak sobie możemy tym żonglować. Czyli chcemy na przykład zrobić AI Studio profesjonalne zastosowanie, na przykład ekstrakcję słów kluczowych albo coś takiego, dajmy temperaturę na zero, a chcemy pisać kreatywne treści, no to dajmy na wyżej. Czyli tutaj jest pierwsza taka możliwość konfiguracji. W żadnym innym środowisku nie mieliśmy takiej możliwości. Mamy też trochę narzędzi. Mamy Structure Output, czyli możemy strukturyzować odpowiedź, że na przykład chcemy odpowiedź w JSON-ie, w jakimś tam formacie, więc jakby tutaj mamy. Mamy egzekucję kodu, wykonywanie kodu. Widzieliśmy to w klodę, na przykładzie tego lecznika, tutaj widocznie też jest. Function calling, czyli tak zostanie ustrukturyzowana odpowiedź, żeby przekazać to do następnej funkcji, wywołać następną funkcję. Zaawansowane ustawienie, nie będziemy tego stosować, ale to, co możemy zastosować, to jest grounding with Google. I to polega na tym, że AI Studio daje Wam możliwość przeszukiwania Google'a w celu uzupełnienia odpowiedzi faktyczną wiedzę. To jest bardzo ciekawe, bo tak działa AI Overview. Google przeszukuje wiedzę i stosując te modele, które mamy w AI Studio, syntezuje ją do postaci AI Overview. I tutaj jest ta funkcja wystawiona, więc możemy sobie wykonać takie ćwiczenie. Tu jest troszeczkę bardziej zaawansowanych funkcji, nie będziemy w nie chodzić. Ta funkcja grounding wydaje się ciekawa. No i na przykład możemy wrzucić jakiś prompt. Napisz mi plan nagłówków dla frazy. Kortyzol. No i teraz powinien się wydarzyć thinking. Okej, mamy thinking, czyli ten najnowszy model. Powinien wydarzyć się grounding. Mam nadzieję, że się wydarzy. O, tu sobie myśli teraz o tym temacie. Ale chyba jeszcze nam nie poszedł do searchu. O, niestety nie poszedł nam do searchu. Poszedł nam tylko do do thinkingu. No dobra, ale to możemy na przykład zadać inne pytanie. wyszukaj mi wszystkie o jest Google Search Suggest Display Google Suggest nie, to są sugestie wyszukaj informacji na temat kortyzolu i popraw plan no i teraz być może nam się to uda nie dostaliśmy statusu o tym groundingu, czyli wyszukiwaniu, być może on to realizuje w tle, nie prezentując tego statusu, ponieważ tutaj nam pokazał to jest, search sources, czyli na potrzeby, o, i są nawet cytowania, czyli robi to w tle. Na podstawie wyniku Google stworzył nam ten plan nagłówków, podając źródła 14 stron internetowych, które zostały wzięte pod uwagę w procesie właśnie tego groundingu, więc chyba to dla mnie jest największa przewaga Google AI Studio. Dodatkowo widzimy tutaj troszeczkę więcej jakby zastosowań, czyli możemy z nim rozmawiać, czyli bawić się modelami głosowymi. Video Generation, więc jakby to jest ten najnowszy model VO2 tutaj dostępne zapewne. Tak jest. Mamy VO2. Możemy sobie generować wideo. No, jakieś tam aplikacje od Gemini, więc jakby bardzo fajne narzędzie dla troszeczkę bardziej zaawansowanych użytkowników. Największa jego przewaga jest to grounding with Google Search. No i pewnie wideo, bo w poprzednich jakby nie widziałem takich zastosowań. Być może to, że jest dostępny najlepszy model na dzień dzisiejszy na świecie. To tyle. Mam nadzieję, że to była taka prosta, ale przynajmniej troszeczkę ciekawa podróż po podstawowych narzędziach AI-owych, z którymi będziecie się zderzać. Jeśli ja miałbym ocenić po moim uważaniu, albo jak ja stosuję, to na pewno na pierwszym miejscu na dzień dzisiejszy jest dla mnie grok. Ze względu na jakość modelu, na jakość programowania, bardzo mi się podobają te workspacy. w groku. Plode, fantastyczne narzędzie przez projekty i możliwość synchronizacji danych. Chat GPT, szybkie akcje, ale jakby nie jestem jakimś heavy userem. Google Gemini, ciekawostka jako narzędzie, modele stosuję z pasjami w profesjonalnych zastosowaniach. No i Perplexity, do szybkich akcji, do wyszukiwania wiedzy w internecie. Także tak wygląda mój ranking. No i co? Dzięki za to lekcje i do zobaczenia w kolejnej. Cześć!

---

### Lekcja: Pozyskiwanie wiedzy i deep research

Cześć, witam Cię w kolejnej lekcji, fajnie, że jesteś. Przed chwilą omówiłem wszystkie podstawowe narzędzia AI-owe, pokrótce pokazując najciekawsze funkcje i podając moją ocenę. Kilkukrotnie w tych narzędziach mówiłem o funkcji Deep Research, czy też Deep Search. Przyjrzymy się jej nieco bliżej, ponieważ wydaje się ona być bardzo... No okej, ciekawa, ale też istotna w kontekście wykorzystania sztucznej inteligencji, czy tych modeli, nie modeli, rozwiązań na potrzeby SEO. Już o tym mówiłem. Modele nie mają aktualnej wiedzy. Modele mają wiedzę odciętą 2-3 lata temu zazwyczaj. Może rok temu niektóre, ale ciągle nie jest to wiedza aktualna. Ty pisząc artykuły musisz mieć wiedzę faktyczną w kontekście polskim, taką jaką Google docenia, prezentuje w AI Overview i tego czego Google de facto poszukuje. to musi być wiedza faktyczna. Więc twoim zadaniem jest jakby pozyskać ją. To, co widziałeś, Perplexity, czyli narzędzie do wyszukiwarka. Wyszukiwarka i synteza wiedzy na pewno jest świetnym rozwiązaniem. Nie będę się nad nim skupiać, aż tak nad tym typowym Perplexity. Zadanie jest proste. Idziesz do Perplexity, wyszukujesz aktualnej wiedzy, kopiuj wklej do czatu na przykład, czy do twojego ulubionego narzędzia, edytora i już jesteś w stanie z tym pracować. Odbijać się dalej w kierunku na nagłówki, odbijać się dalej w kierunku na tej treści, czy cokolwiek uzupełnianie i tak dalej. To jest jasna sprawa. Pomyślmy chwilę o deep researchu. Zaczynając po kolei, chodźmy do czata. To, co już widzieliśmy, to jest funkcja wyszukiwania, czyli wyszukiwarka, która przeszukuje faktyczne wiadomości. Zapewne czat jest, czy OpenAI, częścią Microsoftu w taki czy inny sposób, na pewno Microsoft jest inwestorem, więc pewnie jest to zasilone bingiem, zakładam, że tak to jest. Natomiast jest to funkcja zbadaj głęboko. I funkcja Zgadaj Głęboko działa mniej więcej w taki sposób. Zadamy jedną frazę, czyli zadamy po raz kolejny frazę kortyzol. Faza kortyzol zostanie rozpisana do szeregu fraz powiązanych. To się nazywa query expansion. Daleko, daleko w tym kursie będziemy bardzo szczegółowo to umawiać. Pokrótce. Mamy frazę kortyzol. Tym sobie rozpiszemy na wszystkie frazy i konteksty powiązane, żeby zrobić głębokie uczenie, żeby nie pierać naszej operacji na dziesięciu stronach internetowych, tylko na dziesięciu frazach, dziesięciu stronach internetowych czy potencjalnie na puli stu. w celu zebrania pełnego obrazu danego tematu. To jest bardzo korzystne w kontekście podyskiwania wiedzy. Tak więc piszemy zbadaj zbadaj mi temat kortyzolu. I tutaj jakby nie wykonał mi jakiegoś głębokiego badania. Spróbuję jeszcze raz. Zobacz, że on już ma tą wiedzę na temat kortyzolu. dobra, niech się produkuje, zapewne może sobie bada w tym momencie głęboko. Perplexity również ma takie badania, czyli ma tą funkcję badania i tutaj mamy właśnie zaawansowana analiza na każdy temat, mają tutaj podpięte modele wnioskujące i właśnie ten już głęboki deep research, czyli dajemy frazę kortyzol i to, co tutaj się zacznie wydarzać w Perplexity, zaraz zobaczymy właśnie to query expansion, jak są rozpisywane konteksty kortyzolu na na tematy, o widzicie, tu się zaczyna rezonik, gdzie on, i will search, coś tam, coś tam, kortyzol i trzy więcej, czyli tu się wykonała funkcja tego query expansion, o którym już powiedziałem, czyli transformacji jednej frazy, zestaw fraz powiązanych i wnioskowanie na tej podstawie. Czytanie stron internetowych, to teraz aktualnie obserwujemy, czyli perplexity dokonuje funkcji odczytywania Dalej. OK? I na podstawie wnioskowania po tych stronach uznał, że brakuje mu jeszcze kolejnych kontekstów typu kortyzol badania i kortyzol normy i szuka kolejnych stron internetowych, które analizuje, zbierając cały czas esencje do kupy. Zaraz przeczyta kolejne strony internetowe, zastanowi się czego mu brakuje w kontekście kortyzolu, rozpisze dodatkowe frazy w celu poszukiwania i pójdzie szukać dalej najprawdopodobniej. I will search, bla bla bla. No i jeszcze nie ma frazy kortyzol definicja, czyli cały czas rozszerza nam temat w celu zaprezentowania całego badania danego tematu. I tak będzie postępować do momentu, w którym jakby wysyci temat i udzieli Wam konkretnej odpowiedzi, czyli doskonała rzecz, żeby zasilić model językowy. O, widzimy, jest kolejne analizowanie tematu i zobaczymy, czy już jest usatysfakcjonowane ilością wiedzy, czy rozpisze kolejne tematy i poszukiwania. Dobra, w tym momencie już wiemy, że ma chyba całą wiedzę. Przeszukuje źródła, analizuje źródła i zaraz zacznie nam drukować pełne podsumowanie. Niech sobie pracuje. Perplexity. O, i mamy opracowanie. Użył do tego 36 źródeł plus 37, 8, 9. 39 źródeł w celu opracowania kontekstu kortyzolu. bo tutaj widać, że się coś przycina albo wolno chodzi, albo jest duże obciążenie perpleksji, no i dostajemy pełną esencję wszystkiego, co istnieje w kontekście kortyzolu w polskim internecie. Co robimy? Idziemy z tym kopii wklej do czata na przykład, czy do kanbasa, czy do naszego edytora i tutaj pracujemy. Hej, podałem Ci całą wiedzę w kontekście kortyzolu, zaplanuj mi artykuł na ten temat. Albo hej, podałem Ci całą wiedzę na temat kortyzolu, czego brakuje w moim artykule, co dopisać. To jest właśnie to zadanie, które możemy w ten sposób zrealizować. To jest arcy, arcy, arcy przydatne, jak widzimy. Naprawdę tutaj tej wiedzy będzie bardzo duża. Ja tutaj często rozwiązywałem w ten sposób problemy, których nie byłem w stanie rozwiązać w prozy w Google, typu jak podpiąć jakąś analitykę w systemie Envato, Marketplace, coś tam, coś tam, coś tam, coś tam. Nie ma takiej informacji. Natomiast tutaj Perplexity poszło tak głęboko, że mi znalazło tą informację, jak to zrobić. De facto to nie działało. ale koniec końców znalazło, wykopało spod ziemi to, czego ja potrzebowałem, żeby się wydarzyło. Jakieś opracowania bardziej poważne możemy w ten sposób sobie tutaj zbudować. Co tam w czacie? Czat też nam coś tutaj porobił, pohalucynował, ale po raz kolejny, moi kochani pracownicy z Festigio działają na naszym koncie, więc jakby nie będziemy de-brysearchu opierać o czat. Co do zasady w czacie działa to identycznie. Grok. Grok ma to samo, tylko Grok ma tą unikalną funkcję, że Grok ma wiedzę z X, czyli to, co się dzieje live, nie? Jak jesteśmy w jakimś kontekście, nie wiem, krypto, czy kontekście inwestycyjnym, na przykład cały świat inwestycyjny dzieje się na X, czy krypto, więc tutaj pewnie tą wiedzę będziemy mieli najlepszą. No i tutaj sobie możemy zaznaczyć opcję Deep Search, albo coś takiego, co się nazywa Deep Search. To jest bardzo, bardzo ciekawe. Spróbujmy tego Deep Searcha i wpiszmy kortyzol. I zobaczmy, co nam zacznie tutaj robić grok. No widzimy, jest to samo, albo przynajmniej w podobny sposób, jak zaprezentował to nam Perplexity. Mamy jakieś tam wnioskowanie po jednej frazie, wyszukiwanie rezultaty, sprawdzanie i tutaj widzimy cały tok myślowy, jak Perplexity wyszukuje cały internet, łącznie z XM w Wikipedii w kontekście kortyzolu. Zaraz do tego sobie wrócimy, bo jakby deeper brzmi jak coś długiego, to sobie wrócimy. Na chwilę obecną grok podaje nam 11 źródeł. Perplexity zrobiło to na źródła 39. Także to jest taka rzecz. Idziemy dalej. I teraz tak. Najlepszy deep research ze względu, że to jest Google, ma Google, czyli najlepsze deep research ma Gemini. Ta pojemność i ilość źródeł, które Google przeszokuje w celu zbudowania tej syntezy i pełnego obrazu wiedzy w Google jest największy. Przynajmniej tak mi się wydaje. Zaraz zobaczymy ile zrobi tego grok. Ale do tej pory Google zawsze zawsze dawał mi najwięcej. więc pewnie już czujesz to, że środowiska zaczynają uzupełniać siebie czyli jeżeli nam się okaże, że w Google zaraz zobaczymy będzie najwięcej przeszukanych źródeł i najdokładniejsza wiedza to warto będzie tam poszukać wiedzy i z nią pójść po raz kolejny do naszego ulubionego środowiska edytora i tak sobie wybierać środowiska do zastosowań, tak to niestety działa na chwilę obecną, to się znajduje w Google tutaj mamy opcję Deep Research i w tym momencie damy frazę kortyzol, czyli ciągle jesteśmy w tym samym kontekście, żeby mieć jakby odniesienie i porównanie. No i Google zacznie nam przeszukiwać cały internet, po raz kolejny tworząc to QR Expansion, rozpisując kortyzol na bardzo dużą ilość fraz. O, tutaj mamy jakieś tam definicje, napisał nam jakiś tam wstępny plan, no i robimy start research. No i ruszył. Na początku nam rozpisał temat, w tym momencie jakby już dokonuje już procesu przeszukiwania całego internetu i zbierania dla nas wiedzy. Więc zobaczymy, ile tutaj nam poda wyników. Jak widzimy tutaj, w tym momencie dzieje się proces jakiegoś rezoningu. O, zobaczcie, czyli wymyślił sobie, co on potrzebuje, jak potrzebuje, jakie tam są najważniejsze rzeczy, jakie są następne stepy. Sam sobie to jako agent rozpisał. No i tu mamy ilość źródeł, które już sobie wyłuskał. Co jest ciekawe, zobaczcie, tu są strony amerykańskie. To jest w zupełności okej, żeby pójść do największego internetu na świecie i tam pozyskać wiedzę, na której dalej w Polsce będziemy uzupełniać graf polskiego internetu. Zobaczycie w którymś z moich następnych tygodni, jak bardzo to jest istotne. Ile mamy źródeł w Groku? Bo już nam odpowiedział. Grok nam odbił odpowiedź na podstawie 32 źródeł. No i co do zasady, ok, mamy po angielsku, translate to Polish, wiecie o co chodzi, chociaż język nie ma znaczenia. No i mamy głęboki research dotyczący kortyzolu. Chwilkę to trwało, kluczowe cytaty, więc mamy faktyczną wiedzę, zsyntezywaną z 32 stron internetowych. Bierzemy do udłowionego edytora i odbijamy się ze strategią. Nagłówki, planowanie, optymalizacja, inne historie. Co tam w Gemini? Gemini nie przestaje analizować. Jak widzimy, ten rezonik trwa. ile tu mamy źródeł, nie podaje nam ilości źródeł, mam nadzieję, że na końcu będzie podsumowanie ale powiedzmy to jest raz, dwa, trzy, cztery cztery razy, raz, dwa, trzy cztery, pięć, sześć, siedem osiem, dziewięć, dziesięć, no dobra, Gemina już wygrała tu jest czterdzieści, cztery razy dziesięć więc jakby w tym pierwszym rzucie już mamy czterdzieści źródeł, które bierze pod uwagę czterdzieści jeden, czterdzieści dwa czterdzieści trzy, czterdzieści cztery czterdzieści osiem, już mamy pięćdziesiąt i on nie skończył, raz, dwa, trzy cztery, raz, dwa, trzy, cz 5, 20, 23. No dobra. Szkoda liczyć. Gemini wygrał i jeszcze nie skończył liczyć. Cały czas nam wnioskuje w kolejnych tematach i w kolejnych kontekstach, których mógł nie pokryć w temacie kortyzolu i pewnie będzie dobierać źródeł. Ten Deep Research tu jest najdłuższy, ale chyba wiecie dlaczego. Dlatego, że tych źródeł już mamy tutaj naprawdę sporo, już nawet nie wiem ile musiałbym policzyć jeszcze raz, ale na pewno więcej niż 40 i to zdecydowanie więcej niż 40 więc no to potwierdza to, co powiedziałem. Po deep research idziemy do Gemini, bierzemy ze sobą tam, gdzie potrzebujemy i budujemy na tej podstawie strategię, nasz content, tak, żeby był faktyczny, prawdziwy i chyba to jest najlepsze rozwiązanie właśnie stosować ten deep research. W kontekście semantyki i planowania też Wam pokażę dużo zastosowań w kolejnych lekcjach z wykorzystaniem właśnie tego query expansion i deep researchu. Także dajmy temu jeszcze chwilę i zobaczymy finalnie ile jest źródeł. No dobra, minęło kilka chwil z Mateuszem, skomentowaliśmy Gemini sobie w tle, że robi wrażenie. Jeszcze nie skończył, proces myślowy u niego trwa. Na chwilę obecną jeszcze nie skończył, ciągle poszukuje wiedzy w internecie. Na chwilę obecną zweryfikował dla nas 73 strony internetowe, więc jakby jest to mocarna ilość w porównaniu z konkurencją i i proces trwa od świeżego, żeby się upewnić, czy nic tam się w tle nie wydarzyło. No nie, no proces jeszcze trwa, jeszcze myśli, więc jakby ogromna ilość wiedzy. No, zdecydowanie Gemini wygrywa konkurs. Nawet bez wiedzy, ile tego będzie na końcu. No, już mamy 73. Pewnie doskonałym pomysłem jest również zabranie tego z Gemini do Notebook LM, o którym opowiem w ostatniej lekcji z tego tygodnia. No cóż, dzięki za tę lekcję. Myślę, że już rozumiecie koncepcję debrysearchu, przynajmniej po co to robimy. W kolejnych tygodniach wytłumaczę Wam, jak to robimy i jak możecie zrobić swój debrysearch na swoje potrzeby. No i co? Używajcie, bierzcie wiedzę faktyczną, bo tego poszukuje Google, tego oczekuje AI Overview. No i pracujcie w ten sposób. Dzięki, cześć!

---

### Lekcja: Modele Open Source w AI

Cześć! Witam Cię w kolejnej lekcji. Dzisiaj omówimy sobie, teraz omówimy sobie aspekty modeli open source. Uważam, że jest to świat, którego nie wolno przeoczyć i de facto w tym świecie dzieje się dużo więcej niż wśród modeli komercyjnych. W tej lekcji porozmawiamy o tym sobie, o tym czym przede wszystkim są modele open source i dlaczego warto. Pokażę Ci źródła wiedzy i miejsca, w których możesz znaleźć nowości w tym świecie oraz jak się w tym świecie poruszać, nawigować, gdzie szukać wiedzy, gdzie szukać nowości i jak to wykorzystać. Dodatkowo pogadamy o infrastrukturach, w tym o infrastrukturach alternatywnych, w których będziecie mogli zarówno zakupić moce obliczeniowe, wykorzystując modele open source, ale również pokażę Ci, w jaki sposób odpalić model open source na swoim komputerze lokalnie. Jest to szalenie istotne, jeśli mamy w naszej organizacji na przykład dane wrażliwe, czy też po prostu chcemy zoptymalizować koszty. Ja odpalę sztuczną inteligencję powiedzmy w cudzysłowie na moim MacBooku. I na końcu pokażę Ci jak to zrobić zdalnie, na przykład na jakiejś tam infrastrukturze cloudowej, czy na serwerze, gdzie wykupimy sobie maszynę wspólnie i pokażę Ci jak ten model odpalić, jak się z nim komunikować. W kolejnych częściach również pokażę Ci jak orkiestrować modele open source'owe. Zacznijmy sobie od pierwszego głównego pytania. Czy warto? Uważam, że zdecydowanie na rynku mamy kilku liderów, na przykład Meta, czyli Facebook, NVIDIA, Google, Microsoft oraz Mistrala na przykład. Wszystkie z tych organizacji udostępniają te modele za darmo, więc jeżeli Facebook, który płaci za to setki milionów dolarów, czy Google, czy Microsoft daje nam za darmo, należy z tego korzystać, albo przynajmniej mieć świadomość, że taki świat istnieje. dodatkowo OpenAI w ostatnim czasie zmieniło charakter swojej organizacji z organizacji non-profit na organizację for-profit więc warto mieć alternatywę, a przynajmniej ją znać technologia również rozwija się na tyle szybko, że z tygodnia na tydzień jak z Damianem organizowaliśmy, organizujemy AI Launch co tydzień wychodzą nowe modele, które już doświadczają swoją doskonałością te największe komercyjne modele językowe by pokazać Ci sytuację tu i teraz na dzisiaj przejdźmy na stronę internetową, gdzie znajduje się tak zwany leaderboard. W materiałach dodatkowych znajdziesz adres tej strony internetowej. Jeżeli wpiszesz sobie w Google po prostu spróbujmy znaleźć tę stronę wspólnie AI leaderboards i pierwszy wynik, myślę, że to jest dobry wynik znajduje się tutaj tablica z najnowszymi wszystkimi modelami AI językowymi na świecie po której można się po prostu nawigować i patrzeć z jednej strony, co jest nowe, z drugiej strony porównywać z innymi. W poprzedniej lekcji dowiedziałeś się, jak czytać benchmarki modeli językowych i w jaki sposób one są porównywane. Podsumujmy, są to testy i zadania na przykład logiczne, które modele językowe spełniają i w zależności, wiecie, próba Turinga, jak się model pomyli, tak są to oceniane. Ale chodzimy do tego leaderboardu. OK, czyli to, co wiemy i co jest oczywistością, OpenAI jako najlepszy model językowy i też najlepsza infrastruktura na świecie oczywiście jest na pierwszym miejscu, natomiast są to modele komercyjne, w których zapłacimy zarówno za dostęp czy to do jakichś usług premium, czy też po prostu zapłacimy za tokeny. Natomiast to, co widzimy poniżej i to jest bardzo istotne, są to modele od Mety, czyli od Facebooka. Facebook wszystkie swoje modele udostępnia za darmo i jest to bardzo kuszące. Wokół tego tworzy się ogromna rzesza deweloperów, którzy tworzą niesamowite rozwiązania z wykorzystaniem właśnie modeli Mety. Kilka dni temu odbył się nawet w Singapurze hackathon, w którym Meta rozdawała granty dla deweloperów, którzy wykorzystują te modele. Można było tam spotkać rozwiązania do zarządzania finansami gdzieś w Azji, więc to jest dosyć ciekawe. Kolejnym z graczy, który jest oceniany najwyżej, zgodnie z benchmarkami, czyli z parametrami tych modeli, będzie Google. Część tych modeli jest darmowych, na przykład Gemma, pokażę Wam gdzie ją znaleźć i w jaki sposób akurat nie będziemy jej odpalać, natomiast one są darmowe. Gemini są to modele płatne, dostępne w Google Cloudzie. One będą też stosowane do zasilenia sztucznej inteligencji w wynikach wyszukiwania, więc to jest dosyć ciekawe, że Google, który co do zasady powinien być gdzieś w okolicy lidera rynku, jest dopiero na trzecim miejscu i też udostępnia modele za darmo. Kolejne są modele Antropica, które są płatne, ale poniżej są modele Mistral, które również są darmowe i możesz je wykorzystywać w swoich strategiach, infrastrukturach firmowych czy procesach. są całkiem niezłe. Także w ten sposób czytamy leaderboardy. Tu oczywiście mamy jakieś tam indeksy znormalizowanej jakości tych modeli. Co do zasady nie wchodząc w głębsze szczegóły, te indeksy potrafią nam ocenić, które są najlepsze. W poprzednich lekcjach również nauczyłem się czytać, co znaczą te wartości. Przypomnijmy sobie, są to rozmiary modeli. Czyli ten 450 miliardów parametrów będzie modelem największym, który jest za darmo i będzie doskonały na przykład generowania treści. Nawet sobie radzi w języku polskim? Te mniejsze modele raczej będą do szybkich akcji, typu na przykład chatbot, albo coś innego. Meta również udostępnia modele wizyjne, do których zaraz przejdziemy, czyli te modele zaczynają widzieć. To też jest bardzo ciekawe, to się wydarzyło stosunkowo niedawno. Okej, powiedziałem o mecie. Spójrzmy na rodziny. Na rodziny tych modeli. Na dzień dzisiejszy mamy dwa, znając prędkość rozwoju technologii pewnie zaraz będą kolejne. Rozróżniamy dwie rodziny. 3-1 są to modele, które służą do generowania treści i operacji logicznych. Jak się przyjrzymy, ten największy model, o którym przed chwilą wspomniałem, 405 miliardów parametrów, jest porównywalny, a w niektórych benchmarkach nawet lepszy, aniżeli modele od OpenAI to GPT-4, czyli troszeczkę starszy model, ale realizuje doskonale Nematronem, to jest model od NVID, czy też Sonetem, czy też z GPT-4O Mini. I jest za darmo. Dodatkowo mniejsze modele, gdzie mówiłem o na przykład o GEM-ie, czyli o Google, również rywalizują ze sobą i te modele okazuje się, że są najlepsze od mety. No i model, który 75-70 miliardów parametrów wydaje się być najbardziej kuszący z zastosowań SEO w porównaniu do konkurencji, czyli Mixtralla i GPT-4, GVT 3,5 turbo, bo wtedy tak, gdy były modele oceniane, wypada najlepiej. To jest pierwsza rodzina. Podsumujmy. Jeżeli chcemy mieć super jakość, na przykład naszej treści końcowej, wybieramy ten model największy, 405 miliardów parametrów, radzi sobie całkiem nieźle z językiem polskim. 70 miliardów parametrów jest to model, który jest do takich zastosowań normalnych, powiedzmy, jakiegoś tam przetwarzania szybkiego albo jakichś takich zastosowań niekrytycznych. Ten najmniejszy 8 miliardów parametrów jest to model, który możecie zastosować na przykład do szybkich akcji, chatbotów, klasyfikacji, kategoryzacji, czegoś szybkiego, lekkiego, gdzie nie potrzeba takiego rozwiniętego, powiedzmy, mózgu. Drugą rodziną modeli językowych od mety są modele oznaczone numerem 3,2. Co tu jest ciekawe, to jest to, że te modele zaczynają być wizyjne. Co znaczy? Znaczy tyle, że modele zaczynają widzieć. I okazuje się, że również darmowe modele wizyjne od MET-y są lepsze niż na przykład modele od Antropica Cloud, Cloud Haiku, czy też GPT-4O Mini, które również mają możliwości wizyjne. Do czego możecie zastosować to w swojej strategii czy w swoich aplikacjach? Chociażby do skanowania faktur, chociażby do sprawdzania jakichś statystyk, czy odczytywania zdjęć, czy dokumentów PDF-ów, czy dokumentacji wewnętrznej, która na przykład była zbudowana w PDF-ie albo wydrukowana. Do tego te modele mogą zostać zastosowane. Kolejną rodziną modeli, które sobie sprawdzimy, są to modele ze stajni Mistral. Mistral jest to francuska firma, która również udostępnia swoje modele za darmo i również zaczęły całkiem nieźle komunikować się w języku polskim. Rodzina jest mniej więcej taka sama. Modele małe, które widzimy po lewej, czyli ten Mistral 7B, Model duży, odpowiednik tej mety 70 miliardowej 8x777b. No i największy model, który będziemy stosować do generowania treści, jak widzimy, tutaj mamy napisane fluent in English, French, Italian, German, Spanish and strong in code. Dokładnie tak. Całkiem nieźle radzi sobie po polsku, możemy to stosować. I również Mistral ostatnimi dniami, ostatnimi czasy zaprezentował darmowe modele wizyjne, czyli po raz kolejny to, co zrobiła Meta, modele językowe zaczynają widzieć i możemy się z nimi komunikować obrazem i na przykład klasyfikować faktury, czy też inne rzeczy. Wartym podkreśleniem jest również to, że na polskim rynku całkiem dużo się dzieje w obszarze modeli językowych. Mamy polskiego bielika. Jest to inicjatywa, która nazywa się Spichlerz. Zebrało się kilku inżynierów, innowatorów. Stworzyli polski model językowy. Uwaga, oparty na polskich danych treningowych. Zostało to zrobione we współpracy, jeśli dobrze pamiętam, z akademią górniczo-chutniczą. Jak widzimy przed oczami, pamiętamy takie superkomputery na uniwersytetach, to właśnie na takim superkomputerze ten model został nauczony w Polsce. Na polskim korpusie językowym, również w tej lekcji pokażę Ci, jak ten model odpalić na laptopie. Testowałem go, dobrze sobie radzi z klasyfikacją języka polskiego. także bardzo interesujące z perspektywy zastosowań SEO czy też zastosowań marketingowych całkiem nieźle radzi sobie z tworzeniem treści w języku polskim, więc chyba szkoda byłoby przejść koło bielika obojętnie aktywnie udzielają się chłopaki na Linkedinie, co chwilę anonsują nowe wersje warto na niego patrzeć, oczywiście on nie dojeżdża jeszcze swoim rozmiarem i jakością do modeli tych największych, powiedzmy, ze stajem OpenAI, ale to nic. Dlaczego? Dlatego, że w korpusie treningowym, GPT-4 na przykład, ilość danych o języku polskim, czyli ilość danych korpusa językowego polskiego, stanowi pewnie jakieś może 1%, a może mniej. Nie mamy takiej wiedzy, ale na pewno są to marginalne ilości. Jeśli chodzi o Bielika, to cały korpus językowy jest po prostu polski, przez co Jako procesor logiczny w języku polskim sprawdzi się dużo lepiej, więc fajnie mieć go na oku i być może stosować w waszych strategiach, procesach, budowie. W kolejnych lekcjach pokażę ci, jak orkiestrować agenty AI, pokażę ci także, jak to zrobić z Bielikiem. No i nowość, mamy taki model Plum. Plum jest modelem, który jeszcze nie został stworzony, natomiast polskie uczelnie, zdaje się Politechnika Wrocławska oraz Politechnika Poznańska pozyskały bardzo duży grant na budowanie kolejnego polskiego modelu językowego na poziomie akademickim, który będzie wykorzystywany w sektorze publicznym, ale też w sektorze prywatnym. Więc to się dzieje, jeszcze to nie działa, będzie działać. Miejmy na oku pluma, to jest kolejny model. Jeszcze nie wiem, czy będzie open source, ale chciałem to pokazać jako kontrę do Bielika, który jest modelem polskim. OK, poszukajmy sobie źródeł wiedzy i w jaki sposób nawigować się w tym świecie. Istnieje społeczność modeli open source, nazywa się HuggingFace. HuggingFace.co, w Google jak sobie wpiszemy, HuggingFace znajdziemy. Okej, kilka zakładek, które nas powinny interesować. Przede wszystkim zakładki z modelami. Znajduje się tu powiedzmy kolejny leaderboard, czy też to jest posartowane po trendingu. Ten leaderboard, który Ci pokazałem na początku, jest posortowany po jakości. Tutaj mamy sortowanie po, przepraszam, pozamykam trochę niepotrzebnych zakładek. O, bielik. Właśnie zaraz sobie do niego jeszcze wrócimy. To sobie przewinę w ten sposób. Po trendzie. Co w danym momencie trenduje i co społeczność modeli open source, czy też deweloperów na świecie w danej chwili wykorzystuje. I widzimy, co teraz jest powiedzmy w gazie. Dzisiaj nagrywam ten film, jest chyba 10 czy 11 października. mamy OpenAI, Whisper Large jest to model, który przetwarza dźwięk w tekst. On jest również za darmo więc całkiem kusząca perspektywa, w której dzięki temu za darmo możemy na przykład sobie podsumowywać YouTube'y czy pożyskiwać treści z telewizji czy z YouTube'a, no nie? Kolejnym trendującym jest od NVIDIA Flux. To Damian pokazywał w kontekście generowania zdjęć. Ten model jest również za darmo wiesz, nie mówiłem na samym początku w kontekście Mety czy Mistrala o tego typu modelach, ale Flux też jest za darmo. Apple również daje modele za darmo. W jednym z AI Launchy mówiłem o tym, są to modele do mierzenia głębi obrazu, czyli jeżeli wyobrazimy sobie samochód autonomiczny, który jedzie i ma zdjęcie z kamery z przodu, to ten model jest w stanie określić głębiej, co tam się znajduje z tyłu. Być może pomoże to samochodem autonomicznym się poruszać. No i widzimy to, co się dzieje właśnie, czyli lame od byśmy się tutaj troszeczkę poscrollowali w dół. Znajdziemy tego dużo, dużo więcej. Możemy też poszukać naszego bielika. Bielik oczywiście się również tutaj znajduje. Zobaczmy. Półtora tysiąca ludzi już sobie ściągnęło bielika, więc pewnie fajnie się tym pobawić i również na tym się skupimy. To, co jest ciekawe, to są robisz datasety. To jest zaawansowane. O tym możemy rozmawiać przy fine-tuningu, natomiast możemy tutaj pozyskać na przykład polskie datasety do fine-tuningu, czy do pracy z modelami językowymi po polsku, ponieważ modele open-source, co na końcu podsumuję, również mogą być fine-tuningowane, czyli dostrajane do języka polskiego, czy też dostrajane do jakiegoś tam zadania. Kolejna ciekawa rzecz, jest to troszeczkę już w obszarze infrastrukturalnym, ale będąc na Hugging Face'ie, chciałbym o tym Ci powiedzieć. Zobacz, jeżeli mamy na przykład model Mety, Lame, to to jakiś tam najmniejszy, uczymy się czytać, powtarzamy, 1 miliard parametrów, malutki model, proste zadania, teksty, proste akcje. Możemy te modele również tutaj u nich odpalić. To jest całkiem ciekawe. Czyli wybieramy sobie z listy na przykład, okej, chcemy korzystać z mety, chodzimy sobie Diplo i proszę bardzo, mamy możliwość infrastruktury u nich bezpośrednio, mamy możliwość sobie kupić jakąś tam dedykowaną infrastrukturę, Azure'a, Cloud'a, nawet w Amazonie możemy to postawić, więc co do zasady, ten Hugging Face jest całkiem obszerne miejsce, w którym nie z jednej strony znajdziemy modele, poczytamy o nich, ale jednocześnie możemy je odpalić. Więc Hugging Face jest super i polecam Ci go pod rozwagę. Wróćmy teraz jeszcze chwileczkę do Bielika. Widzimy, tutaj jest strona główna bielik.io, 11 miliardów parametrów, dostępny na Hugging Face. Super. Chłopaki mają, jeśli dobrze pamiętam, społeczność na Discordzie, można z nimi rozmawiać o tym modelu, śledzić newsy i tak dalej. Dobra, co my tu mamy dalej? Kolejne źródło wiedzy, OpenRouter. OpenRouter, adres openrouter.ai Z jednej strony jest to infrastruktura, o tym opowiem troszeczkę później, ale z drugiej strony jest to miejsce, w którym możemy po raz kolejny poszukać modeli, czy sprawdzić, co w trawie pisz, czy jakie są fine tuningi modeli, czy kto czego używa. No i mamy, powiedzmy, weźmy sobie browse, czyli podzimy w jakiś tam katalog i zobaczmy, tutaj mamy całkiem niezłe filtrowanie. Po tych modelach możemy się poruszać na przykład na serię modeli, czyli ten Gemini, lama i tak dalej, poszukamy, czego sobie potrzebujemy. Co jest ciekawe, tam są gotowe fine tuningi, czyli na przykład możesz sobie poszukać, okej, że jeżeli wiesz, że twoim zadaniem jest na przykład kategoryzacja jakiejś tam akcji, to te fine tuningi zazwyczaj tam się znajdują, no nie, czyli będziesz miał na przykład model mety przygotowany do konkretnego zadania, którego ty potrzebujesz. Znajdziesz to na Hugging Face'ie i znajdziesz to również na Open Routerze, do czego cię zachęcam. Wejdźmy na przykład sobie na tą na tą gemmę. Mamy informacje, gdzie one się znajdują. Fantastycznie infrastruktury. Pokażę Ci dalej, jak tego korzystać. To, co Cię może zainteresować, to jest również informacja o tym. To jest taki grubszy temat, ale powiedzmy okej. To jest informacja o tym, ile tokenów dany model przetwarza w tej infrastrukturze. Im więcej, tym jest części użytkowany, więc sobie można posortować i w ten sposób wybrać, co w danej chwili drabieloperzy na świecie używają w największym wolumenie. no to pewnie to jest jakby taki szybki, tak jak na Allegro, sortuj po popularności, kupujesz pewnie pierwsze, drugie, trzecie, bo tam dużo ludzi kupiło, czyli jest dobrze. Podsumujmy, open router, kolejne miejsce do poszukiwania modeli do rozglądania się za gotowymi fine tuningami, ale również gotowa infrastruktura, gdzie sobie dzisiaj ten model przetrenujemy. Dobra, co my tam mamy dalej? Olame. Kolejna świetna rzecz. Z jednej strony jest oprogramowanie, które pokażecie, jak dzisiaj odpalić, na komputerze, jak zrobić model językowy. Z drugiej strony po raz kolejny jest to źródło wiedzy o modelach językowych, open source'owych, które od razu możesz zainstalować u siebie na komputerze. Czyli mamy oczywiście tutaj troszeczkę mniej informacji o tych, mniej filtrów, natomiast mamy tą lamę, o której mówiłem z modelami wizyjnymi, detale, informacje, benchmarki, jak odpalić, parametry, to jest spoko i jeżeli sobie wpiszemy bielik, no to my go znajdziemy. Także fajnie, że w tych światowych rozwiązaniach, bo chyba O-Lama jest największym na świecie tego typu rozwiązaniem do utrzymywania modeli językowych, znajdują się już również polskie produkcje. No niestety to widzimy na Hugging Face'ie, było półtora tysiąca pobrań, na Lamy jest 249. No ale okej, Polacy są dostępni na tego typu infrastrukturach. Wróćmy do prezentacji. Przed przejściem do infrastruktur chyla podsumowania. Wiemy czym są modele językowe open source. Wiemy, jak to są liderze rynku, jak się poruszać w tym świecie, gdzie poszukiwać, co znaczą parametry. Chodźmy do infrastruktur. Bo to jest troszeczkę tak. Jeżeli idziemy do OpenAI, czyli do GPT-4, czy do wolnego modelu, który tam stosujecie, oni wam dają infrastrukturę. Modele open source charakteryzują się tym, że wy musicie gdzieś umieścić. To jest dobre i niedobre. W opcji open AI jest to no-brainer. Idziesz, dzwonisz, masz, płacisz, koniec. End of story, załatwione. Tu też tak możesz, zaraz ci pokażę. Ale to jest ciągle wysyłanie danych gdzieś. Możesz te modele utrzymać również na własnych maszynach, przez co twoje dane, dane wrażliwe, dane biznesowe, GDPR i wszystkie inne historie, są po prostu bezpieczne. Jest kilka fajnych infrastruktur, które używam. Pokrótce przeklikamy się przez nie bez głębszego wejścia. Może wejdziemy w jedno. Wszystkie są podobne. Różnią się co do zasady pieniędzmi, które będziemy płacić po raz kolejny za tokeny, które wiemy, co to są tokeny i dlaczego trzeba promptować w języku angielskim, żeby płacić mniej. Można też płacić za jakieś tam godzinę. Najfajniejsza infrastruktura, bo przynajmniej bardzo przyjazne, jest to AnyScale. Gdzieś powiedziałem, nie mam mieć otwartą. Platforma AnyScale, jak widzimy tutaj ogromne firmy, Intel, Airbnb, OpenAI, nawet tam kupują od nich zasoby. My również możemy. Tam znajdują się modele, głównie z Facebooka, ale nie tylko. Sporo różnych rozwiązań. Zachęcam sobie wejść na AnyScale i po prostu nawet się zapoznać, bo uważam, że tego typu firmy, Jest to przyszłość startupów na świecie, bo tych infrastruktur będzie rosnąć wraz z ilością zapotrzebowania na moc obliczeniową na świecie. Mamy tam wiele możliwości, fine-tuningu modeli, stosowania tych modeli, deploya modeli. Mamy taki sam playground praktycznie jak w OpenAI, gdzie możemy te modele testować, więc jakby any scale polecam się tym zainteresować. Na moich slajdach, do których nie będę wracał, następną pozycją jest Together AI, jest to tożsama infrastruktura różni się co zasady ofertą natomiast jak widzimy na tym kolażu tutaj znajdują się wszystkie rzeczy, których potrzebujemy mamy Lamy, najnowsze modele mamy Mistralę mamy, co my tu jeszcze mamy? Snowflake, Stable Diffusion czyli mamy zdjęcia, kod Lama co do zasady tam praktycznie jest wszystko co nowego się pojawia, tam się pojawia kupujemy sobie od nich moc obliczeniową mają również playground, jest to całkiem tanie, więc Together AI jest kolejną polecajką, którą można wziąć pod uwagę szukając infrastruktur do modeli open source na świecie. Następną jest perplexity, czyli to jest to fajne narzędzie, które omawiamy do przeszukiwania treści. Oni też dają swoją infrastrukturę, gdzie można utrzymać te modele i zdecydowanie obniżyć koszty. Jak sobie poszukacie perplexity AI, macie możliwość API, tu są wszystkie informacje, których potrzebujecie oraz perplexity ma całkiem niezłą dokumentację. Widzimy tutaj karty, wiadomo, okej, ale mamy też playground, gdzie możemy się tymi modelami pobawić i zobaczmy. Tutaj po raz kolejny z Perplexity mamy modele Lamy, Sonoro Large i to chyba jest istotna informacja. Cała Perplexity, które jest cudowne z perspektywy SEO do pozyskiwania danych, przetwarzania danych, doszukiwania danych, jest oparte o swoje fine tuningi Lamy i działa fantastycznie, więc jakby też możecie z tego używać. Zapytamy, hi, how are you? no i w Perflex City akurat to jest jakaś dziwna historia, a jakiś kontekst ma przytrzymane, no nie bardzo działa i grok jest ostatnią taką infrastrukturą, którą chciałbym podkreślić, dlaczego? dlatego, że na dzień dzisiejszy mówię na dzień dzisiejszy, bo dużo się dzieje i dużo się anonsuje, jest najszybszy na świecie więc jeżeli wasze aplikacje wymagają bardzo szybkiego przetwarzania, wyobraźmy sobie dzwoni telefon i jest chatbot czy jakiś tam asystent, który dzwoni. To musi dziać szybko i właśnie grok dostarcza tego typu rozwiązania. Zalogujmy się tam. Mamy możliwość glock platform, jeśli jesteśmy duzi i chcemy tam sobie troszeczkę wykupić. Mamy możliwość właśnie platform i zaraz się gdzieś tutaj zalogujemy. Gdzie oni mieli ten link? Właśnie. Product overview platform i oni tu wicielibili ten konsol. Start loading now. Jest. OK. I mamy tożsame playground z OpenAI, gdzie możemy z nimi rozmawiać. Jak spojrzymy na... To jest takie podsumowanie już w tym momencie, jakby wychodząc z infrastruktur. Wiemy, że meta jest najlepsza. Mistra, Google daje. I spójrzmy, jakie modele dają infrastruktury na świecie. Gemma. Google. Z Groka. To są fine tuningi, mety, czyli to samo, co Perplexity robią. Dokładnie to samo. Z Hugging Face'a Whisper, czyli to, co już pokazałem do przetwarzania dźwięku. Jak dzwonimy czy coś takiego, to oni to robią najszybciej. Meta. Mistral. Open AI, czyli po raz kolejny to samo. I koniec. I tak będzie na większości infrastruktur. Modele, które spotkacie, to właśnie będą to największe. I okej. Działa. Piszmy sobie jakiś tam user prompt. gave me informations about bloggers. To nie jest lekcja z finding, z prądu inżynieringu. Jak widzieliśmy, momentalnie pojawiła się odpowiedź. To jest właśnie charakterystyka tych modeli. Spójrzmy jeszcze raz, że jest odpowiedź. Jeżeli twoje aplikacje potrzebują działać naprawdę szybko, to grok jest dla ciebie najlepszy w rozwiązaniu. Dobra, wróćmy do prezentacji. Ja już to sobie ładnie omówiłem. Infrastruktura jeszcze jedna, czyli ten open router. Wróćmy do niego. Open router AI. Teraz open router w kontekście infrastrukturalnym. Na początku rozmawialiśmy jako źródło wiedzy, poszukiwania, modeli. OK, fajnie. Natomiast oni zrobili coś, co w mojej opinii jest przynajmniej warte uwagi, jeśli nie rewolucyjne. Znajdźmy jakiś większy model, powiedzmy, wpiszmy, co byśmy tutaj sobie wpisali blablabla lama metalama instruk, okej, nikt będzie zobaczmy, znaleźliśmy sobie ten model powiedzmy będziemy rozmawiać z lamą to nie jest to o tym co znaleźliśmy natomiast to co oni wam dostarczają z punktu widzenia infrastruktury to jest to, że oni wiedzą że ten konkretny model czyli nasza lama znajduje się na dzień dzisiejszy w raz, dwa, trzy, czterech infrastrukturach i na przykład ma wolne moce obliczenia. I teraz tak, zobaczmy. Znaleźliśmy model i mamy infrastruktury. Powiedzmy, że on jest w Deep Infra, w Together AI, które wam pokazałem, Fireworks, Hyperbolic. Tych infrastruktur naprawdę jest dużo na świecie i będzie jeszcze więcej. I teraz tak, oni potrafią skumać w ten sposób, że jeżeli na przykład, załóżmy, ok, Hyperbolic ma najtaniej w danej chwili, bo ma wolne moce obliczeniowe, to oni wyślą was do hyperbolik. No nie? W momencie, w którym uznają, ok, hyperbolik jest obciążony, bo coś tam albo zaczyna być drożej, to oni to samo wyślą wam do deep infra. Czyli to, co oni stworzyli dla infrastruktur i modeli open source'owych, to jest taki jakby jeden ustandaryzowany pośrodku konektor, miejsce, load balancer, no nie wiem. Jeden standard, który rozsyła po całym świecie tam, gdzie są wolne moce obliczeniowe i tam, gdzie jest najtaniej. Więc to jest całkiem fajne. zalogujmy się tam, ja to pokażę trochę szerzej przy orkiestracji agentów AI i zobaczcie, gdzieś tutaj mamy credits, ja tutaj jestem najbogatszy na świecie, wiadomo, 9,5 dolara, okej, ale zobaczmy usage, jak się tym bawimy jakiś czas temu, zobaczmy, mamy tą metę, jakoś tam buduje system do tworzenia artykułów lifestyle'owych wokół tego i zobaczmy, część powiedzmy promptów poszła do infrastruktury hiperbolic, bo hiperbolik w danej chwili miał moc obliczeniową, a w tym samym procesie część pochodzą do Dibi, Infra Dibi, Infra Nowita, Avian i on tak miesza. Bardzo kuszące rozwiązanie, bo zdejmuje nam ciężar z głowy wyboru, wybiera tam, gdzie jest moc. Przyszłość takich rozwiązań wyobrażam sobie trochę jak wiecie, zielona energia na przykład z fotowoltaiki jest jak świeci słońce, czyli potencjalnie można by wysłać nasze żądania AI do miejsc, gdzie świeci słońce w danej chwili, potencjalnie jest tam zielona energia z fotowoltaiki, na pewno takie coś będzie. To jest 100%. I ostatnia infrastruktura, tutaj też pokażę troszkę więcej, to jest RunPod. W RunPodzie mamy dwie opcje, dedykowaną, gdzie sobie odpalimy zaraz nasz model językowy, ale również możemy też wykupić jakieś tam, powiedzmy, end pointy, czyli miejsce, gdzie nasz model będzie utrzymany i to jest już totalnie prywatne w tym momencie, bo te hiperboliki, DB Infra, Together AI, AnyScale i tak dalej, to gdzieś to idzie, a tu mamy zamknięte tylko dla nas. Czyli właśnie tutaj mamy aspekty bezpieczeństwa. Zaraz sobie do tego dojdziemy. Podsumujmy. Infrastruktury alternatywne służą przede wszystkim do tego, żeby obcinać koszty. Pojawiają się rozwiązania takie jak open router, które już dystrybuują nam moc na świat i pozyskują chociażby, no załóżmy, że powiedzmy tą zieloną energię z fotowoltaiki i tam akurat nam nasze modele utrzymują. I to działa. I wierzę w to, że to będzie ogromny trend. Natomiast zastanówmy się do czego warto, nie? Bo takie proste myślenie, ok, idę do OpenAI, oni są najlepsi na świecie, ok, spoko, ok. Ale jednak być może warto. Pierwsza rzecz, która dla mnie jest dosyć istotna, są to operacje klasy backend, czyli powiedzmy, ok, naszym zadaniem jest kategoryzacja słów kluczowych. Naszym zadaniem jest dopisanie produktów do kategorii w sklepie internetowym. Czy naszym zadaniem jest, nie wiem, klasyfikacja produktów, czy ten produkt, na przykład ta sukienka ma czerwony kolor, czy tam zielony, czy tam jakiś tam tego, tego. To się nie dzieje z przodu, czyli tego człowiek nie widzi. To się dzieje w naszej infrastrukturze, w naszych systemach. To zdecydowanie modele open source można tam stosować w zupełności. Są wystarczające, świetnie sobie radzą, przez co obcinamy koszty, mamy aspekty bezpieczeństwa i wiele innych historii. Do tego na pewno warto. Kolejna rzecz. Przygotowanie danych do GPT-4. W kolejnych lekcjach pokażemy Wam, jak budować treść z wykorzystaniem modeli językowych i każdy profesjonalny powiedzmy, nie wiem, workflow czy sposób generowania treści, jest to wieloetapowe przetwarzanie danych przez sztuczną inteligencję, gdzie na samym końcu znajduje się najlepszy obecnie istniejący model na świecie po to, żeby ta treść była najlepsza. End of story. Dosyć proste. Natomiast po środku znajduje się proces, który może mieć na przykład 10-15 kroków i tam już nie trzeba tak doskonale, no nie? Więc tam te modele mają zastosowanie w zupełności wystarczą. Ja właśnie tam je widzę i tam stosuję. Optymalizacja kosztów. Jeżeli na przykład mamy do kategoryzacji, wyobraźmy sobie Senuto. Senuto ma do kategoryzacji powiedzmy, nie wiem, 100 milionów słów kluczowych w Polsce. Więc jakby kupując tą moc od OpenAI to jest nierealne. Używając modeli open source to praktycznie może nic nie kosztować. Więc jeżeli wasze procesy są drogie albo chcecie obniżyć koszty, bo już zaczyna być droga, to na pewno jest to dobre rozwiązanie, żeby w tym kierunku pójść. Ogromna skala przetwarzania, to jest kolejna rzecz. Czyli to już co powiedziałem, 100 milionów słów kluczowych w Senuto, czy nie wiem, 50 tysięcy produktów w waszych sklepie internetowym, razy ileś wariacji, czy to operacji, czy tak dalej. Na pewno te modele się do tego sprawdzą. I ostatnia rzecz, bezpieczeństwo danych. Po prostu. Na dzień dzisiejszy wysyłając cokolwiek do infrastruktur zamkniętych, czyli do OpenAI, do Antropica, czy powiedzmy do Google, to gdzie, gdzie, gdzieś. I obiecują, że nic z tym nie zrobią, ale wiemy jak z tym jest. Nie mamy jeszcze regulacji w Unii Europejskiej, dopiero się pojawią. i jeżeli wasze dane są kluczowe i nie możecie sobie pozwolić, że na przykład jakiś model będzie na niej trenowany, to się może wydarzyć. No właśnie, to musicie sobie odpowiedzieć sami. Na pewno te modele dają wam walor bezpieczeństwa, co wam pokażę, jak zaraz zrealizować. Kolejna rzecz. Point Tuning. Możecie wziąć dowolny praktycznie model open source'owy i go przystosować do konkretnego zadania. Czyli na przykład wiemy, że mamy lamę od mety, ona tak sobie średnio po polsku, to my ją douczymy, nie? Po polsku. Wrzucimy na przykład artykuły z Forbes'a, Damian pokaże w kolejnych sekcjach, jak fine-tuningować modele. Te modele wszystkie można fine-tuningować do konkretnych zadat, do języka polskiego. Fantastyczna opcja. I coś, co się pojawia, destylacja modeli. Nie widziałem tego jeszcze w kontekście modeli open-source'owych. To się pojawi. To się pojawi. Nie wiem, w którym momencie kursu jesteście, znaczy kiedy go oglądacie. Na pewno to się być może już pojawiło, albo dopiero pojawi. Czyli z dużych modeli open-source'owych będą robione malutkie modele do konkretnych akcji. Na przykład tego największego, modelu, lamy, zostanie stworzony jakiś malutki modelik, który będzie realizował jedną akcję, będzie super tani, będzie można go odpalić nawet na telefonie czy na zegarku. To się na pewno wydarzy. Okej, dzięki. W tej lekcji to wszystko. W kolejnej lekcji pokażę Ci, jak wykonać deployment lokalnie, czyli zainstalować model językowy na Twoim komputerze oraz zrobimy sobie ćwiczenie, w którym pokażę Ci, jak zainstalować model językowy na maszynie zdalnej, powiedzmy na chmurze czy na jakimś serwerze dzierżawionym. Do usłyszenia.

---

### Lekcja: Lokalne oraz zdalne uruchamianie modeli językowych

Dzień dobry. Do tego ćwiczenia posłużymy się narzędziem O-Lama. O-Lama w mojej opinii jest najlepszym narzędziem obecnie na świecie i najłatwiejszym do instalacji modeli. W poprzedniej lekcji pokazałem Ci, jak nawigować po tym środowisku, po O-Lamie, czyli przeglądać modele, wybierać te modele, wiemy jakie są najlepsze, czyli powiedzmy te odmety Lama, mamy dostęp również do modeli od Google'a, czy też dostęp do modeli od Microsoftu w Lamy. Okej, ale teraz skupimy się na tym, jak sobie odpalić model na swoim komputerze. Pierwsza rzecz, po prostu ściągamy. Jak widzimy tutaj jest informacja o tym, że to oprogramowanie jest dostępne na Maca, na Linuxa, na Windowsa, czyli co do zasady instalujemy wszędzie. Nie będę przekazywał procesu instalacji, on jest dosyć prosty. Ściągamy, instalujemy dalej, dalej, koniec. No i po zainstalowaniu Lamy, ja akurat pracuję w środowisku Macowym, ona się znajduje tutaj u góry, jest już odpalona, możemy sobie z nią pracować. I teraz tak, lama nie ma żadnego interfejsu, to jest bardzo lekkie rozwiązanie, w którym będziemy pracować w terminalu, nie bójcie się, to jest dosyć proste, command spacja, piszemy terminal i otwiera nam się matryk. Ja troszeczkę przybliżę tutaj, czy powiększę, żebyśmy lepiej widzieli. OK. Co piszemy? Po prostu OLAMA. OK. I tu mamy zestaw funkcji, które OLAMA będzie dla nas realizować. Czyli mamy jakieś tam informacje o tym, jak stworzyć modele, odpalić modele, zastopować modele, wylistować modele, usuwać itd. Jest to taka dosyć prosta konfiguracja. Ja na potrzeby tego ćwiczenia sobie już, tutaj mamy list, czyli w tym momencie podświetlę modele, które już mam zainstalowane na swoim komputerze. Ja na potrzeby tego ćwiczenia ściągnąłem je wcześniej na swój komputer, po to, żeby przyspieszyć tą lekcję. No i widzimy, że mamy zainstalowaną już na moim komputerze jakąś tam lamę 1B, najnowszą lamę. Mamy z tego spichlerza bielika, czyli ten polski model językowy, o którym już również wspomniałem. Zaraz się nim pomyjmy. No i jakąś kolejną lamę. To, co dla Ciebie tutaj jest ciekawą informacją, to jest jakby size, no nie? Czyli zobaczmy, czyli ten bielik, polski model językowy, waży 6,7 giga. Jest to dla Ciebie informacja, że tyle powinieneś mieć giga RAM-u w swoim komputerze, żeby ten model odpalić. Ja tutaj akurat na swoim komputerze mam 16 GB, więc te modele bez problemu sobie odpalimy. Chodźmy do jakiegoś innego. Pokażę Ci różnicę, jakie są rozmiary. Okej, jesteśmy w tej najnowszej LAM-ie. Zobaczmy. I największy model, V-Mall. Największe modele, tutaj akurat są te mniejsze modele. Chodźmy do tej starszej Lamy, czyli sobie wchodzimy w model. lama, to są większe modele, bo te najnowsze są wizyjne, one są troszkę mniejsze, te starsze modele z rodziny 3.1, o której mówiłem, są modelami większymi. No i zobacz, największy model, ten 405 miliardów parametrów, który konkuruje z GPT, plik waży, sam model waży 229 giga. Jest to informacja, ile potrzebujesz mieć pamięci RAM właśnie na swoim komputerze. Ten model, który jest w zupełności wystarczający do tego, żeby generować treści, waży 40 giga, czyli pasuje mieć Pewnie 64 GB RAM-u, najlepiej na karcie graficznej. Dla tego modelu, dla największych modeli, to odpowiednio 256 GB. To pokażę na RAM-podzie, jak sobie tym zarządzić. OK, czyli jesteśmy w miejscu, w którym mamy zainstalowaną LAM-ę. Jest ona już na naszym komputerze, jesteśmy w terminalu. Ja akurat mam ściągnięte modele, ale sobie zrobimy ćwiczenie, poszukamy na przykład, OK, skupimy się na tym modelu. Czyli chcemy sobie zainstalować na moim komputerze tę LAM-ę z rodziny 3.2 z 3 miliardami parametrów. Jak to robimy? Wyszukujemy w środowisku olamy nasz ukochany, ulubiony model, który chcemy sobie zrobić. Wybieramy jego rozmiar. Nas interesuje akurat 3 miliardy. I zobaczcie. Tutaj znajdują się wszystkie parametry modelu, rozmiary i tak dalej. Cały readme i informacje, co to jest, benchmarki. Także możemy sobie poczytać. Na potrzeby odpalenia interesuje nas co zasady to. Czyli mamy gotową komendę, czyli olama run i nazwa modelu. Zobaczcie. Jak sobie wejdziemy do naszego terminalu, po prostu robimy kopiuj, czyli tu. tutaj wklejamy, ok no i ten model nam już się odpala dlatego, że on był wcześniej przeze mnie ściągnięty na mój komputer no i mamy, działa, zobaczmy już możemy sobie z nim rozmawiać zapytajmy what is hamburger? no i mamy już sobie rozmawiamy, ten model jest na moim komputerze jak widzimy całkiem sprawnie działa, mimo że mamy tylko 16 giga RAM-u, to wynika z tego, że te modele są małe, dlatego uważam, że przyszłością właśnie są małe modele, lepiej wytrenowane, lepiej dostosowane do konkretnych zadań, bo po prostu koszty przetwarzania tych modeli będą dużo, dużo tańsze. No jak widzimy, odpowiada mi, czym jest hamburger. Dobra, jeżeli chcielibyśmy sobie odpalić bielika, czyli nasz ten polski model językowy, nic prostszego. Piszemy po prostu bielik. Okej. On się znajduje w najnowszej wersji. Jak widzimy, to są te wersjonowanie i tak dalej. To już sobie umiemy to czytać. To jest najnowsza wersja. No i I po raz kolejny. Jest na Olamie. Fantastycznie. Readme, wszystkie rzeczy. Kopiujemy sobie informację o Bieliku. Odpalamy w naszej Olamie. On się ładuje. To chwileczkę potrwa. W tym momencie mam trochę załadowanych tych rzeczy na swoim komputerze. Więc dajmy mu chwilę. On teraz sobie robi ładowanie. Dobra. No i mamy odpalonego na moim komputerze Bielika. Czyli model językowy stworzony przez Polaków na polskim korpusie językowym, więc zagadajmy z nim po polsku. Co to jest hamburger? Jak widzimy, on na chwilę sobie pomyśli, bo to jest troszeczkę większy model. No i leci. Bielik działa. Podsumujmy. Ściągamy lamę. Raczej o lamę. Instalujemy na naszym komputerze. nawigujemy sobie do modelu, który nas interesuje. To są polecane przeze mnie i przez nich, jakby skupmy się na nich. Kopiujemy, odpalamy, działa. W kolejnych lekcjach pokażę również, jak orkiestrować te modele w jakieś procesy czy w jakieś profesjonalne rozwiązania. Na chwilę obecną ta wiedza, jak to odpalić na swoim komputerze jest dla nas wystarczająca. Ok, teraz pokażę Ci, jak odpalić model językowy na, powiedzmy, serwerze czy w jakiejś tam infrastrukturze chmurowej. Do tego ćwiczenia posłużymy się runpodem. Jest to, jak widzimy, all-in-one cloud. Bardzo lubię to rozwiązanie, ponieważ oni mają dosyć ciekawe podejście, w którym zarówno mają swoje serwery i swoje procesory logiczne, procesory graficzne, ale również mają podejście marketplace'owe, w którym można kupić procesor, czy komputer, czy kartę graficzną od kogoś z rynku. Czyli załóżmy, że ty masz 10 kart graficznych u siebie w biurze i chcesz je sprzedać, no to tutaj jest właśnie taki marketplace dzięki temu możemy kupować to dużo taniej. Dodatkowo, jakie są różnice między podejściem klasycznym? Powiedzmy, idziemy do OpenAI i w OpenAI płacimy za tokeny, czyli pracujemy, pracujemy, pracujemy, pracujemy, pracujemy i za ilość przetwarzanych danych po prostu zapłacimy. W tym podejściu zapłacimy za godzinę, czyli jeżeli mamy ogromne zadanie, powiedzmy, naszym zadaniem jest, tak jak powiedziałem, powiedzmy przykład, skategoryzować 100 tysięcy czy 100 milionów fraz kluczowych w Senuta, no to pewnie płacąc za tokeny zapłacimy bardzo długo. A tutaj możemy odpalić maszynę na 7 dni, zapłacić za 7 dni użytkowania, wykonać nasz proces, wyłączyć, nie płacimy. Jest to bardzo kuszące rozwiązanie i w sumie też zamknięte, ponieważ te maszyny są tylko nasze, nikt tam nie ma dostępu i w sumie z tym pracujemy. Dobra, pokażę Wam, jak to odpalić. Powiedzmy, w tym momencie się zalogowaliśmy do Rampoda, Wy się zalogowaliście, podaliście dane Waszej karty i przeszliście weryfikację w standard. Ja mam jeszcze na koncie 7 dolarów, więc nam starczy. I mamy dwa podejścia. Pierwszym podejściem jest serverless, czyli jakby serverless w tłumaczeniu to jest po prostu bezserwerowe. I tam też możemy sobie odpalać nasze modele. Dosyć proste, nie będziemy się na tym skupiać, natomiast po prostu dajemy link do Hugging Face'a, który omówiłem w poprzedniej lekcji, czyli powiedzmy Hugging Face. Interesuje nas model, powiedzmy no to skupmy się na tej mecie już. Popiecmy ten. Kopijemy sobie adres, wchodzimy tutaj, jest adres skopiowany, Haging to jest token, tutaj musimy się zalogować, to jest akurat mój token, no i tam jakieś konfiguracje, i tak dalej, i tak dalej, kupujemy moc, dostajemy po prostu endpoint, czyli API, gdzie mamy już nasz model językowy tylko dla nas i sobie z nim pracujemy. Natomiast nie będę tego pokazywać, to jest bardzo proste rozwiązanie, skupimy się na rozwiązaniu pods. Rozwiązanie pods jest to rozwiązanie, w którym kupujemy jakby swoją maszynę gdzieś na świecie, gdzie ktoś ma wolne moce obliczeniowe i tam możemy utrzymać nasz model językowy tylko na nasze potrzeby, optymalizując koszty. Czyli co? Wchodzimy sobie w pods, dajemy sobie deploy, dzięki temu jesteśmy już w ich marketplace'ie. To, co widzimy na liście, jest to spis kart graficznych czy procesorów obliczeniowych, które są dla nas dostępne, czyli ten H100, na przykład to jest jakaś bardzo droga kartograficzna, RTX to jest jakiś tam GeForce RTX od NVIDIA, które możemy wynająć w cenie 0,69 dolara za godzinę. Czyli jak nasz proces jest w 24 godziny, to jedziemy pod sufit, realizujemy, wyłączamy i płacimy mało. To, co jest dla mnie fantastyczne, jest to właśnie to Community Cloud. Dużo tańsze. Są to procesory, karty graficzne u kogoś, przez co one są po prostu tańsze. I skupimy się na procesorze, powiedzmy ten H100. H100 jest teraz jedną z najlepszych kart graficznych procesorów obliczeniowych na świecie. I możemy sobie wybrać na przykład do naszych zastosowań dużo mniejszych. Jeżeli mamy mały model, to idziemy na przykład w jakiś tam tani procesor, powiedzmy ten RTX 3090, czyli to właśnie tam GeForce RTX 3090, to jest bardzo tani. Jeżeli mamy duże potrzeby, no idziemy do dużych procesorów. No to okej. Wybieramy sobie jakiś procesor, który nas interesuje i wchodzimy sobie w templatki. Zobaczcie. W templatkach mamy bardzo dużo możliwości instalacyjne, jakieś tam TensorFlow-y, jakieś tam PyTorche i wiele różnych rzeczy, które możemy zastosować, czy to do treningu modeli, czy do innych rzeczy, nawet serwer na Ubuntu postarimy. My się opieramy o Lame, czyli piszemy sobie Olama. I mamy Olama Teamplate, czyli tą Olamę, którą zainstalowaliśmy wcześniej na swoim komputerze, po prostu zainstalujemy na czyimś komputerze. Dobra, mamy wybraną Olamę. I zobaczmy, mamy dwie opcje. Jedną opcją jest On Demand. Opcja On Demand jest to opcja, w której ten procesor jest tylko i wyłącznie dla nas i nikt nam go nie zabierze i za niego płacimy. Ale mamy fantastyczną opcję Spot. Opcja Spot polega na tym, że jeżeli mają wolne moce, nikt tego nie używa, to sprzedadzą Wam za połowę ceny, mieszają te procesory. Oczywiście działa to w ten sposób, że jak się znajdzie klient, który zapłaci więcej, no to Wam wyłączą, czyli trzeba tym zarządzić, ale jeżeli może się okazać, że nie ma klienta w danej chwili, no to w ten sposób też możemy zoptymalizować koszty naszej pracy. My sobie zrobimy on demand potrzeby tego ćwiczenia. No i zobaczcie, mamy nasz setup, zapłacimy 3 dolary za godzinę przetwarzania modeli językowych, mamy tą najdroższą kartę graficzną, 251 giga RAM-u, 24 procesory i dysk. To nie jest istotne. Istotne jest to, że za te trzy dolce, które tutaj mamy, zobaczcie, ta karta graficzna kosztuje 160 tysięcy złotych. No to jakaś za stówę, tutaj za 160 tysięcy złotych, za 166 tysięcy złotych, czyli za 3 dolary za godzinę mamy kartę, która kosztuje 160 tysięcy złotych gdzieś na świecie. Ok, podsumujmy. Wchodzimy sobie w pods, dajmy deploy, bo chcemy stworzyć naszą maszynę. Ja jestem zwolennikiem Community Cloud, lubię takie inicjatywy. Bierzemy tą najdroższą kartę, bo why not? Mamy całe 7 dolarów, więc sobie możemy się pobawić. On demand. Wybieramy Olamę, którą przeciczyliśmy sobie wcześniej. Dajemy on demand. No i co za tyle. Deploy. Teraz gdzieś na świecie ta maszyna się tworzy dla nas. Zaraz zobaczymy, gdzie nam wylosuje miejsce i gdzie są dostępne moce. Dzisiaj nam wylosowało... Okej. Jesteśmy akurat w Kanadzie. Także tam stoją jakieś procesory, które są dla nas wolne. No i teraz trzeba chwileczkę poczekać. Teraz się instaluje system. Będzie to trwało dosłownie kilka minut. Jak widzimy, są ściągane jakieś tam pliki i tak dalej. Po prostu się instaluje system. Tak jak pamiętacie, instalowaliście kiedyś Windowsa, to na pewno to jest coś takiego. Poczekajmy chwilkę. Okej, jesteśmy. Maszyna się zainstalowała, trwa o to kilka minut. Jest to normalne, po prostu musi się zainstalować. Jesteśmy w miejscu, w którym mamy maszynę w Kanadzie, która ma ten najdroższy procesor graficzny NVID-H100 za 160 tysięcy złotych. Jesteśmy gotowi do akcji, działa. Dobra, co musimy teraz wykonać? Ona gdzieś sobie stoi dla nas w Kanadzie, trzeba by tam zainstalować model językowy. Klikamy sobie Connect i dla osób zaawansowanych mamy możliwość terminala, czyli w konsoli, z którym pokazywałem, możemy sobie tam porozmawiać z tą maszyną. My zrobimy sobie to w web terminalu, żeby było szybciej i łatwiej. Czyli klikamy sobie Start Web Terminal, Connect Web Terminal. I teraz łączymy się do tej naszej maszyny, która stoi dla nas przygotowana w Kanadzie. To zawsze chwileczkę musi potrwać, bo ona dopiero powstała. Za chwileczkę się połączymy. O śmiało, Kanada. Ok, jesteśmy. Dobra, jesteśmy połączeni, czyli ten terminal, który Wam pokazywałem na moim komputerze, teraz mamy po prostu w Kanadzie na tym procesorze logicznym. Tutaj już jest wszystko gotowe. Wszystko jest zainstalowane i możemy robić akcje z modelami językowymi. Czyli wracamy sobie do Lamy, czyli O-Lamy. No i jeszcze raz, powiedzmy nas interesuje, żeby sobie ściągnąć jeszcze raz ten model powiedzmy 3 miliardy parametrów, żeby to było szybciej. Kopiujemy tylko i wyłącznie tą komendę do web terminalu. Kopiuj wklej, ogień. I zobaczmy. Tego nie widzieliśmy wcześniej, bo na moim komputerze było już zainstalowane. W tym momencie model się pobrał. Zajęło to kilka sekund, no bo akurat on jest mały, więc jakby to jest okej. W tym momencie zaczynają się instalować dodatkowe jakieś tam manifesty, konfiguracje i są ściągane właśnie na tej maszynie naszej kanadyjskiej. Weryfikacja. No i zaraz będzie działać. Dobra, jesteśmy. Model jest zainstalowany na karcie graficznej za 160 koła w Kanadzie i możemy mu napisać. Jeszcze raz. What is sushi? Chyba mam ochotę dzisiaj na sushi, chociaż nie dzisiaj na żurek. Okej, let's go. No i mamy sushi, informacje o sushi i tak dalej. On sobie pracuje tutaj w Kanadzie. Fantastycznie. Wróćmy do run poda. Zobaczcie. Dodatkowo mamy tutaj connect to HTTP service. Czyli tutaj mamy już wystawiony nasz jakby API endpoint, gdzie możemy wysyłać z naszych orkiestratorów, co pokażę w kolejnych lekcjach, Żądania, czyli wpiąć do naszej aplikacji, czy korzystać z API. Jeżeli chcecie wiedzieć więcej, to jest bardzo, bardzo proste. Wchodzimy sobie na OLAM-ę, DOCS. W DOCS-ie się znajduje API Reference. I w API Reference mamy informację, jak tam sobie rozmawiać z tą LAM-ą, OLAM-ą, która jest tylko i wyłącznie nasza. Używamy po prostu tego adresu do komunikacji. Czyli on znajduje się tutaj, connect. No i sobie działa. OK. Czyli jesteśmy w miejscu, w którym już sobie odpaliliśmy model, przetestowaliśmy, że działa, możemy się do niego dalej połączyć, wykonywać nasze operacje. I załóżmy, wykonaliśmy akcję. 100 milionów operacji, kategoryzacji, słów kluczowych zajęło nam to, nie wiem, dwa dni. Płacimy za godzinę i uznajmy, okej, dobranie, płacimy, koniec. Maszyna się stopuje, czyli wyłączamy, przestajemy już w tym momencie płacić. A żeby nie płacić jeszcze za zajętość dysku, tak kilka centów za to chcą, to my sobie po prostu usuniemy. Koniec. Temat jest zamknięty, maszyna nie istnieje. Jeżeli chcemy zrobić kolejną, deployujemy, instulujemy inny model, podpinamy do naszych procesów, realizujemy masowe operacje, wyłączamy i jedziemy dalej. Dzięki za tą lekcję.

---

### Lekcja: Poznajemy GPTs

Cześć! Witam Cię już w ostatniej lekcji w tym tygodniu. Pokażę Ci łatwy, prosty sposób, jak stworzyć własne GPTC. O co chodzi? Chodzi o to, że w czacie mamy zastosowania jakieś tam typowe, czyli na przykład chcemy pozyskać wiedzę, chcemy napisać artykuł, chcemy coś takiego, ale możemy mieć na przykład zastosowanie jakieś konkretne, typu, że chcemy stworzyć na przykład metatitle, albo chcemy stworzyć, nie wiem, wpisy na LinkedIn, tylko i wyłącznie, albo chcemy na przykład humanizować treść, czy wiele rzeczy powtarzalnych w konkretny sposób, konfigurując konkretne zadanie. I do tego służą właśnie GPT-sy, czyli twoja własna konfiguracja czatu do twojej konkretnej potrzeby, którą masz. Dodatkowo możesz tam stworzyć połączenia do API, typu na przykład iść do Senuto i zapytać, jaka jest widoczność twojej strony internetowej. Czy też wiele innych opcji, które są kuszące, ciekawe. Chodźmy tam, pokażę ci, jak to działa. To jest super, super proste. Okej, czyli jesteśmy w głównym widoku czatu. I zobacz, możemy sobie tutaj kliknąć odkryj modele GPT. To są właśnie te GPT-sy. Zobacz, tutaj powstał taki trochę powiedzmy marketplace, w którym mamy gotowe rozwiązania, na przykład do pisania. Fajnie mamy rozwiązania do, nie wiem, badania analizy danych. Czy mamy jakieś tam zadanie edukacyjne, lifestyle'owe, także no spoko. Weźmy sobie jakiś przykładowy canvas. Na przykład nie canvas, tylko przykładowy GPT-sy. Co mnie tu interesuje? Coś prostego zróbmy. Powiedzmy, nie wiem, write for me. albo tego LinkedIna poszukajmy LinkedIna na pewno się znajdzie jakiś model, który się wyspecjalizował Viral LinkedIn Postformater wow, super, jedziemy z tym zobacz, wybraliśmy sobie jakiś tam model ma jakieś tam oceny, więc fajnie rozpocznijmy czatowanie ok, i zobacz, jesteśmy teraz już w tym GPT-sie czyli w modelu skonfigurowanym do tego, żeby napisać viralowe wpisy na LinkedIn zaraz Ci pokażę, jak je zbudować swój. Natomiast używamy na razie jakichś gotowych, jest tam masę rozwiązań do SEO, optymalizacji kontentu, humanizacji treści. No i powiedzmy, damy mu informację, napisz mi wpis na LinkedIn na temat szkolenia AI Akademii. I teraz jakby już mamy skonfigurowany konkretnie czat z zadaniami takimi, że wiesz, okej, no i mamy Sensei Akademii, widzicie, z emotkami, fajnie sformatowany, akurat jest po angielsku, no a jakbyśmy poprosili po polsku też nam sobie zrobię. Fundamentals to cutting edge techniques in machine learning NLP and more. Super. Także możemy sobie poszukiwać różnych GPT-sów pod konkretne nasze rozwiązania, czy zastosowania, możemy, nie wiem, do pisania treści inne, do tworzenia czegoś tam innym, do czegoś tam innym, do czegoś tam innym i tak dalej. Zróbmy swój, albo pokażę wam, jak zrobiliśmy jakiś nasz prosty GPT, totalnie, absolutnie prosty do generowania zdaje się opisów metadescription na podstawie jakiejś tam core, semantic, coś tam. Pokażę jak to jest skonfigurowane. Zobacz. Pierwsza rzecz. Jeżeli chcemy stworzyć swój GPT, dajemy po prostu jego nazwę, opis, co tam ma się wydarzyć i instrukcję. Zobacz. Tutaj mamy już konkretnie skonfigurowany model, który będzie nam po prostu na przykład twoim zadaniem jest coś tam, coś tam, verbalizacja. Założenia. 100, 150 znaków i tak dalej, tak dalej, i tak dalej, i tak dalej, i tak dalej, czyli go konfigurujemy do konkretnego zadania, czyli tworzenia metody description. Przeglądanie sieci, generowanie obrazu, konfigurujemy sobie po prostu swoje środowisko, w którym będziemy dalej pracować. I to, co jest super, jeżeli chcemy na przykład pozyskać jakieś dane z API, typu senuto od naszej widoczności, sobie przechodzimy tutaj na sam dół i mamy informację, utwórz nowe działanie i możemy na przykład pójść do senuto, ściągnąć dane do naszego GPT-sa i dalej na przykład na tej podstawie coś przedporzyć. Dobra, wykorzystajmy to. I powiedzmy, jestem już tutaj w skonfigurowanej wersji czata. Do konkretnego zadania, czyli do tworzenia meta description, powiedzmy, nie wiem, napisz meta description dla frazy pożyczki online. Przygodnie? No i mamy meta description przygotowany zgodnie z jakimiś tam naszymi założeniami dla frazy pożyczki online. Także tak to działa. Ale jak zrobić swój? Jesteśmy w głównym oknie czata. Klikamy sobie odkryj modele GPT i dajemy utwórz. No i mamy taki interfejs, w którym możemy już sobie konfigurować nasze GPT-sy. No i robimy konfigurację. Nazwijmy naszego GPT-sa powiedzmy LinkedIn Post. Opis. Generowanie wpisów na LinkedIn. Dobra. Teraz instrukcje. Co ma robić nasz konkretny GPT-sy? W następnych lekcjach poznasz dokładnie metody prompt engineeringu, jak to robić. Tutaj zrobimy to totalnie prosto, żebyś zrozumiał tylko koncepcję GPT-su. Czyli twoim zadaniem jest napisać viralowy wpis na LinkedIn, max 500 znaków, albo coś krótszego, zrobimy 300 znaków. Użyj emoji. Użyj Hashtagów. Co tam jeszcze możemy zrobić? Ja nic, niech będzie. Okej, możemy mu przesłać wiedzę. To jest fantastyczne, bo jeżeli na przykład masz dokumentację firmową, albo na przykład, nie wiem, jakąś książkę, na podstawie której chcesz, nie wiem, budować jakiś content z strony internetowej, czy coś takiego, możemy mu wgrać tutaj wiedzę i on na podstawie tej wiedzy będzie pracował już wewnętrznie tego GPT-sa i dostarczał Ci faktycznych informacji, na przykład z książki, albo z Twojej strony internetowej, też z dokumentacji produktowej. Więc możemy to przesłać. To jest tak zwany RAC, czyli Retrieval Augmented Generation w kolejnych częściach tego kursu dokładnie. Wytłumaczymy, co to tu się wydarzyło, nie? Otwórz nowe działanie i tak dalej, skonfiguruj i tak dalej. Dobra, jesteśmy. Dajemy otwórz i możemy sobie to share'ować. Zobacz, możemy wystawić naszego GPT-sa do sklepu GPT, czyli do tego marketplace'u, gdzie możemy sobie pobierać. Każda osoba z linkiem możemy dać naszym pracownikom, czy naszym przyjaciołom, czy naszym, nie wiem, w organizacji. I tylko ja na przykład, no to ja, ja, to na No tylko ja. I on jest tylko dla mnie. Możemy się też tym podzielić. Na przykład w naszej firmie wszyscy nasi pracownicy w dziale SEO czy w dziale marketingu będą tego używać. A jakieś linki i tak dalej. Jesteśmy już w naszym GPT-sie. Przypomnijmy, skonfigurowany model czata czy skonfigurowany czat do zadania konkretnego, którym jest napisanie wpisu na LinkedIn. Dobra, napiszmy. Napisz wpis na LinkedIn. i na... o szkoleniu. Co my mieliśmy w instrukcjach? Maksymalnie chyba tam 200 czy 300 znaków. OK. Użyj emoji i użyj hashtagów. Takie były nasze zadania do GPT-su. No i zobaczmy, co będzie. Mamy emoji. Mamy 300 znaków. Nie wiem, czy mamy, ale wygląda na krótki tekst, więc zakładamy 200-300 znaków. No i mamy hashtag, o którego poprosiliśmy. Czyli osiągnęliśmy efekt. Naszym celem było napisanie wpisu na LinkedIn. wiralowego, z hashtagami i tak dalej. I jak będziemy cały czas pracować w konkretnym kontekście, zawsze będziemy odzyskiwać przynajmniej podobne efekty do naszego działania, więc jakby to jest super fajne rozwiązanie, jeżeli mamy powtarzalne rzeczy, które chcemy po prostu w szybki sposób sobie robić. Nie wiem, humanizacja treści, wpisy na LinkedIn, wpisy na social media, kontent na naszą stronę internetową. Czy też możemy zastosować GPT-sy z Marketplace'u, albo możemy się po prostu podzielić z naszym teamem tymi rozwiązaniami. Myślę, że to jest super, super przydatne. Możemy na przykład, nie wiem, mamy zadanie skonfigurować WordPressa, czy coś zrobić do naszej strony. Na pewno jest WordPress. Zobaczmy, czy jest jakiś GPT do WordPressa. Na pewno jest, tylko coś wyszukiwarka mnie zawodzi. Albo tak dużo tych WordPressów jest. Mamy. WordPress Wizard. Proszę bardzo. Używamy. Rozpocznij czat. I w tym momencie jesteśmy w GPT-sie, który na pewno ma wgraną całą dokumentację WordPressa na 100%, czyli ta wiedza, to miejsce, co ci pokazywałem, że tam możesz wgrać swoją dokumentację, oni na pewno mają tam wgraną całą dokumentację WordPressa, przez to jest dostosowany do tego, żeby na przykład, nie wiem, write me code to add menu in footer. No i dobra, napisze nam jakiś tam kod, który doda nam menu footer, nie? Więc fajnie, krótka lekcja, szybka lekcja, Damian więcej opowie o promedie inżynieringu, co z czego wynika. Dzięki za dzisiaj i do usłyszenia w przyszłym tygodniu.

---

### Lekcja: Notebook LM

Cześć, witam Cię w kolejnej lekcji. Jesteśmy w sekcji dotyczącej przydatnych narzędzi AI i na moją szczególną uwagę zasługuje, na Waszą również, narzędzie Notebook LM, które do zasady na początku powstało jako narzędzie do tworzenia podcastów w danej tematyce, natomiast w kontekście SEO w mojej opinii znajduje również bardzo ciekawe zastosowanie ze względu na to, że A. Notebook LM od kilku tygodni potrafi wyszukiwać źródła informacji, co jest bardzo ciekawe, B. potrafi generować mindmapy, czyli grafy powiedzmy, no nie są to grafy wiedzy, ale przynajmniej mapy myśli w danym temacie, które w kontekście SEO wydają być całkiem przydatne, chociażby na potrzeby planowania treści, optymalizacji treści i tak dalej. Chodźmy do tego narzędzia. adres notebook.lm.google piękna domena, wypróbujmy notebook.lm. Tu jest jeden problem z notebookiem.lm, że zazwyczaj działa w języku angielskim, czasami działa w języku polskim, no ale jest jak jest, zawsze możemy się tym wspierać. Co możemy zrobić? Tworzymy nowy notebook, czyli właśnie to, co docelowo byłoby tym podcastem, czy czymś, co jest w stanie nam syntezować jakąś wiedzę i możemy w tym miejscu wgrywać naszą wiedzę, czyli na przykład dokumentacje firmy, wszystkie artykuły ze strony, które sobie pobraliśmy itd., ale to nie wiem, czy nam na tym zależy. Możemy skopiować po prostu tekst, fajnie. Możemy podać adresy stron internetowych, fajnie. Możemy się połączyć z naszym Google Drive'em. To też jest całkiem fajne. Na przykład mamy zestaw wszystkich artykułów, które napisaliśmy na naszą stronę internetową. Moją uwagę przykuwa przeciw tutaj Discover Sources, Czyli każemy Google'owi nijako z wykorzystaniem jakichś ich agentów, może Google Search'a, czy tego co rządzi AI Overview, może, poszukać dla nas wiedzy, tak jak szuka jej Google. No dobra, czyli zbliżamy się do analizy tematu w taki sposób jak analizuje temat i źródła Google. No bo to jest narzędzie od Google. Nie sądzę, żeby Notebook Alem miał swój Search, na pewno jest wpięty w jakieś API Google'owe. Pozostając w moim ulubionym kontekście, czyli hormonów i kortyzolu, piszemy sobie, że chcemy mieć wiedzę na temat, powiedzmy, kortyzolu. Dobra, powiedzmy kortyzolu, hormon stresu, wpływ kortyzolu na zdrowie. No i powiedzmy, chcielibyśmy pozyskać też wiedzę, na przykład przeciwdziałanie wysokiemu poziomu albo obniżenie. to są nasze tematy. Załóżmy, że piszemy artykuł o kortyzolu w tym momencie i potrzebujemy takich kontekstów, takiej wiedzy, żeby ten artykuł był pewny. Dajmy tutaj powiedzmy, żeby to było w polskim języku, ale to jest całkiem ciekawe podejście, żeby tutaj poszukiwać wiedzy międzynarodowej, prawda? Być może czegoś nie ma w Polsce, no ale dajmy, że Niestety tutaj w interfejsie nie znalazłem opcji konfiguracji językowej, więc jakby sprąptujmy mu ten język. No i dobra, zobaczmy co się dzieje. Znalazło nam 10 źródeł dotyczących kortyzolu. Jak sobie zaimportujemy, to on w tym momencie automatycznie ściąga nam zawartość stron internetowych, które sobie znalazł Google. Oczywiście też możemy dokleić ich więcej, prawda? Mamy tutaj opcję dodania źródła, czyli to, co już widzieliśmy. Mamy opcję discover, czyli ponownie możemy coś mu tam jeszcze dokręcić i powiedzmy obniżenie poziomu kortyzolu. Teraz na pewno znajdzie kolejne źródła, których niech znalazł wcześniej, być może inne. Teraz znalazł nam 8 źródeł. Dodajmy te źródła. Na pewno on sobie zrobi deduplikację tych źródeł. No ale cały czas rozwijamy mu wiedzę, którą on dla nas przetwarza w kontekście kortyzolu. I teraz tak, mamy możliwość czatowania już z wiedzą. Czyli pobrało nam Google automatycznie, no w tym przypadku 18 stron internetowych w kontekście kortyzolu minus duplikacja. Możemy z tym czatować. Kuszące może Ci podnapisać, odpowiedz mi na konkretne pytania, albo copywriter w ten sposób już ma zsyntezowaną wiedzę i może sobie lepiej po prostu pisać, no bo wiemy jak działają kopierajterzy. Rano ezoteryka, później opony letnie, a wieczorem na przykład turystyka, więc w tym narzędziu automatycznie już mają tą wiedzę, która jest powiedzmy specjalistyczna czy faktyczna, ale na moje szczególne to co mi się najbardziej podoba to jest mind map. Czyli na podstawie tej reprezentacji 18 stron internetowych, które Google nam pozyskało, nie jesteśmy w stanie stworzyć mapę myśli, reprezentację graficzną danego tematu. No i tutaj widzimy, ta mapa się właśnie generuje po prawej stronie i możemy sobie ją popodglądać. No niestety jest po angielsku, no ale ciągle lepsze to niż nic. Czyli mamy kortyzol i powiedzmy mamy jakieś funkcje i te funkcje coraz nam się tutaj rozwiną na jakieś tam, widzimy jakieś regulacje metabolizmu, immune response regulation, czyli regulacja systemu immunologicznego, czy jakiś blood pressure regulation i tak dalej. Są to jakby konkretne tematy, które trzeba pokryć w Waszym artykule, jeżeli chcecie mieć wszystko kortyzolu. No i po kliknięciu dostajemy automatycznie jakiś sad wiedzowy w tym kontekście, jak to się ma na przykład do tej regulacji. Jesteśmy przy temacie, jaki to jest temat? Discuss source about blood pressure regulation, czyli omówienie regulacji. Po raz kolejny ciśnienia. Więc po raz kolejny bardzo fajne zastosowanie dla copywriterów. Możemy sobie jakby rozwinąć ten graf i oczywiście go pobrać. niestety tylko do formy graficznej. Szkoda, że nie da się tego pobrać do jakiegoś planu, ale pewnie jakby korzystając z modeli multimodalnych, jakby wykonane takie ćwiczenie, że sobie rozwijacie mapę, macie wszystkie konteksty dotyczące kortyzolu, trochę to pomniejszyć, pobrać. Zaraz zobaczymy, w jakiej to rozdzielczości się pobiera. to jest to rozdzielczość na tyle wysoka żeby pozwolić sobie z tym iść do czata GPT żeby nam to przeczytał co tutaj jest napisane i na tej podstawie zbudował wam kontent więc troszeczkę sprytu gdzieś można to wszystko przemycić myślę, że bardzo przydatne gotowa synteza nie powiem, że to Pical Map ale przynajmniej brief w konkretnym temacie, który nas interesuje. Jeżeli chcielibyśmy posłuchać sobie na temacie kortyzolu, tak sobie klikniemy generate, to stworzy nam się podcast, w którym będziemy mogli posłuchać. To jest jakby takie zastosowanie, jeżeli potrzebujecie strawić jakąś konkretną dawkę wiedzy i nie chcecie czytać 20-40 artykułów w danym temacie, żeby być ekspertem, to możecie je wrzucić do tego narzędzia i posłuchać sobie rozmowy w formie podcastu na temat tego. Co to jest dla mnie jeszcze bardzo ciekawe, to jest to, że można tutaj wrzucać filmiki na YouTubie. Czyli możemy o tyle zastanowić się nad wyprzedzeniem konkurencji albo pozyskaniem jakichś, nie wiem, na przykład świeżych informacji, bo powiedzmy jesteśmy w obszarze giełdy czy inwestycji, więc fajnie będzie sobie wrzucić najnowsze jakiś daily news finansowy z Bloomberga czy z Financial Timesa, który na przykład jest na YouTubie dostępny i uzyskać syntezę, czy o jakichś trendach krypto od influencerów krypto, wrzucić tutaj, uzyskać syntezę, o czym się rozmawia, jakie są najpopularniejsze krypto danego dnia, no i wykonać odpowiednie ruchy, bądź ich nie wykonywać. Więc to jest świetne narzędzie do syntezy wiedzy, do rozmowy na temat tej wiedzy, dla copywriterów, ale nie tylko, do planowania na podstawie wiedzy. No i ten mindmap. Ten mindmap jest rewelacyjny. Aż się chyba pokuszę o ćwiczenie, mimo że to nie jest ćwiczenie dotyczące notebook LMA. Chodźmy do chata GPT. I ja muszę mu to wrzucić. No i zobaczymy, czy sobie z tym poradzi. Teraz chat GPT ma te najnowsze modele graficzne. Dodatkowo mamy rezonning po grafice. O, co to jest kortyzol? Zrozumiał. Hormony, funkcje kortyzolu. No i właśnie, i nam reprezentuje w tym momencie ten mindmap. który nam wygenerował notebook LM, więc jakby na żywo w trakcie tej lekcji wymyśliliśmy bardzo ciekawe zastosowanie dlatego, więc się cieszymy. Tu ciekawą rzeczą jest to, że ten model, który o, u mnie teraz on nie działa, bo na moim koncie ktoś jest zalogowany, wydaje się ten model O3, jakbyśmy go wybrali, on ma funkcję wnioskowania po grafice, więc pewnie on byłby najlepszy do tego zastosowania, żeby wybrać O3, wrzucić mu tą mapę, myśli, i kazać wnioskować po niej. To by chyba było najlepsze rozwiązanie. Także słuchajcie, no z mojej strony bardzo mocna polecajka jako zestaw przydatnych narzędzi i jest to nieoczywiste narzędzie ku zastosowaniu marketingowo-seo-owym, ale jak teraz pokazałem Wam to myślenia i pewne funkcje, no to mam nadzieję, że jest dla Was bardzo przydatne i będziecie w stanie na tej podstawie robić piękne rzeczy. Także dziękuję bardzo za tą lekcję i do zobaczenia.

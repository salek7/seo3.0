# Transkrypcje Lekcji - Tydzień 1

## Lekcja: Historia modeli językowych

Cześć! Witam Cię w pierwszej lekcji kursu Sensei. W pierwszej lekcji porozmawiamy sobie chwileczkę o historii oraz postaram Ci się na przykładzie opracowania, które stworzył Financial Times, wytłumaczyć, jak działa AI, skąd to się wzięło i w ogóle dlaczego to działa. Ale po kolei. Zacznijmy od historii. Dwa słowa, szybciutko. W mojej opinii, ale też w sumie na podstawie patentów i technologii, Google jest drajwerem i powodem rozwoju sztucznej inteligencji na świecie, to dzięki ich technologii znamy obecne AI, jakie znamy. Teraz Ci pokażę, jak to działa i dlaczego działa, przynajmniej spróbuję. Ale skąd to się wzięło? Wyobraźmy sobie wyszedł w arkę Google i ich największy problem. Ich największym problemem jest oczywiście potrzeba zrozumienia kontekstu zapytania. Przykład. Piszesz czarna konsola do gier, myślisz PlayStation. Albo piszesz bajka o różowej śwince, myślisz świnka Pepa. Google stało przed wyzwaniem, żeby połączyć z jednej strony zapytania, które wprowadzamy do wyszukiwarki, ale i połączyć z kontentem z tych stron internetowych, które prezentuje nam w wynikach wyszukiwania. Dodatkowo Google też musiało zrozumieć treść na naszych stronach internetowych i po raz kolejny dopasować je do wyników wyszukiwania. jest jedna zasada, która przyświeca Googlowi. Ich głównym, nadrzędnym celem jest dostarczenie sobie jak najlepszych wyników wyszukiwania i dlatego powstały technologie, jak na przykład transformery, do których zaraz dojdziemy i to one rządzą obecnym AI, takim, jakie znamy dzisiaj. Słowo historii. Pierwsza rzecz. 2013 rok Google jest ciągle na tej samej ścieżce rozwoju, czyli potrzebuje znaleźć kontekst zapytania oraz treści na stronie internetowych i powstał Word2Vector. Jest to pierwszy algorytm, którym z Damianem mówimy od zawsze o tym algorytmie, jednak nie sposób go ominąć w kontekście sztucznej inteligencji. Jest to zamienienie słów na wektory. Jak widzimy na tej animacji, nagle słowa opisane w przestrzeni wektorowej zaczęły być blisko siebie. W tym momencie Google zrozumiało, że ok, że pianino jest blisko do skrzypiec albo Mercedes do Volkswagena jako samochód. To jeszcze im nie dało możliwości zrozumienia kontekstu, ale na początku w ten sposób zaczęli przetwarzać wasze strony internetowe i tak też zaczęło powstawać pierwsze topical authority, bo skoro jesteśmy na przykład w kontekście instrumentów muzycznych, należy omówić pianino, skrzypce, gitarę i tak dalej. Z tego to się wzięło. czyli to był 2013 rok w 2014 roku Google opublikowało kolejny algorytm, czy kolejną metodę, którą nazwało sequence to sequence i na tej podstawie Google było w stanie stworzyć Google Translate'a, czyli tłumaczenie maszynowe podejrzewam, że do tej pory w ten sposób to działa okazało się, że na przestrzeni wektorowej słowo pies i słowo dog było po prostu w podobnej pozycji przez co Google było w stanie tłumaczyć na przykład z języka angielskiego na język polski i tak działa Google Translate ale to nie było kluczowe w kontekście AI W kontekście AI kluczowa jest architektura transformerów. Postaram Ci się to omówić na przykładzie chyba najlepszego opracowania, które ostatnimi czasy przygotował Financial Times. Znajduje się on, to opracowanie pod tym adresem i wszystkich zachęcam, żeby tam wejść i poczytać, co tam jest napisane, ponieważ ta strona jest fantastyczna, tam się przełączę i po kolei będziemy sobie rozmawiać, co przygotował Financial Times w obszarze transformerów. Okej. Tu jest chwileczka historii, skąd to się wzięło i tak dalej. No i zobaczmy. Mamy jakieś zdanie we go to work by train, czyli jedziemy do pracy pociągiem, no nie? I architektura transformerów wprowadza pojęcie tokenów. Każdy model językowy ma troszeczkę inny tokenizer, czyli jak tutaj zobaczymy, o ja mam taki świetny świetny pilocik od Mateusza, zobaczcie jaki bajer, o chyba to nie jest Ten bajer znajduje się tu. Czytamy tutaj opowied. Mamy represent fraction of the words, czyli tokeny w obecnym AI reprezentują fragment słowa. Pokażę to na kolejnych slajdach tej prezentacji, natomiast na potrzeby uproszczenia omówienia Financial Times stokenizował zdanie we go to work by train po słowach, żeby łatwiej było zrozumieć koncepcję. Czyli pierwsza sprawa. Mamy zdanie. Jedziemy do pracy pociągiem, tokenizujemy go, to organizujemy to zdanie na pojedyncze słowa. Chodźmy dalej. Dalej. Algorytmy sztucznej inteligencji, czy też na razie transformery jeszcze, szukają prawdopodobieństwa, w jakim prawdopodobieństwie słowo work może wystąpić, czyli because I work in my, coś tam, albo people who work from home, co nie? Czyli w tym momencie architektura transformerów szuka czegoś tutaj mamy nazwane nearby words, czyli słowa bliskie. Co się dzieje dalej? No, zobaczcie. I tutaj dzięki takiemu ćwiczeniu architektura transformerów jest w stanie określić, które słowa są blisko do słowa praca, a które są daleko. To tak na przykładzie tego Word2Vector, tam już było wiadomo, co jest blisko, co jest daleko. I przykład. Work Polka, to jest bardzo daleko. Work Zebra, albo Work Atmosphere, albo Work Dove, Dove to jest gołąb, będą daleko. ale work processes, czy work that, czy work hour, work to, work for, albo work meet, są blisko. Czyli w tym momencie transformery już wybierają słowa, które są blisko słowa work, a które są daleko odrzucając je jakby z dalszego przetwarzania. I teraz zobaczmy. W tym momencie zachodzi proces tak zwany embeddingów, czyli zmiany na te wektory, które były pokazane na przykładzie word to vector, Czyli każdy z tych jakby zlepków, work, plus coś tam, co jest jakimś prawdopodobieństwem, staje się opisane jakimś tam embeddingiem, przez co transformery są w stanie mierzyć dystanse już matematycznie. I teraz będzie fajny przykład, do którego przejdę od razu. O co z tym wszystkim chodzi? Damian w swojej części tego szkolenia będzie bardzo szczegółowo opowiadał, z czego to wynika. Natomiast przyjrzyjmy się po prostu. Jeżeli mamy słowo C opisane jakimś tam embeddingiem, czyli jakimś tam wektorem, blisko znajduje się ocean, czyli sea jako morze i ocean jako ocean. I w ten sposób Transformer wie, że to są słowa bardzo blisko, ale znaczą coś innego. Bo tutaj mamy na przykład football i soccer i one też znaczą praktycznie to samo, ale jednak nie do końca. Albo I, albo we. I w ten sposób sobie opisuje dystansami, co jest blisko, co jest daleko. Chodźmy dalej. I dzięki temu, teraz zobaczycie to, co przed chwilą oglądaliśmy na Word2 Vectorze. Google sobie zrobiło klastry, no nie? Czyli mamy na przykład train, bus, car, czyli w tym momencie Google wie, czy ten transformer wie, że to są jakieś pojazdy, a tutaj będziemy mieli jakieś colleague, school, work, no nie? Albo walk, swim i run, czyli to pewnie będzie jakiś tam klaster sportu, no nie? Jak widzicie, tutaj jest dokładnie opisane o co chodzi. Musicie pamiętać o tym, że w ten sposób te słowa stają się blisko siebie, Ale to jeszcze nic nie znaczy w kontekście transformeru. Na razie po prostu wiemy, że coś jest blisko siebie. Jak sobie przeczytacie dalej i przejdziemy dalej, zaczynamy wchodzić w kontekst transformeru. Kolejne przykładowe zdanie. I have no interest in politics. Czyli nie jestem zainteresowany polityką. I tutaj pojawia się słowo klucz. Tutaj. Self-attention. Self-attention znaczy uważność albo samouważność. Chodzi o to, że w tym momencie transformery zaczynają rozumieć, które słowo się z czym wiąże i co to tak naprawdę zaczyna znaczyć. Bo w tym momencie wiedzą, co jest blisko siebie albo co jest w klastrze na przykład pojazdów, sportu, pianin, instrumentów muzycznych. W tym momencie transformery zaczynają rozumieć, co znaczy co. Chodźmy dalej do przykładów. To jest świetnie opisane. Zobaczcie. I transformery zaczynają porównywać słowa ze sobą i nagle wiedzą, że no w tym zdaniu zaczyna ważyć najwięcej. Self-attentional look at the each token in the body, text and decide which one are most important to understanding the meaning. Czyli w tym momencie zaczynają rozumieć, które słowo znaczy najwięcej dla znaczenia całego zdania. No i w tym konkretnym przypadku jest no, jako nie mam zainteresowania polityką, prawda? Inaczej, I have interest in politics, to znaczy zupełnie coś innego, prawda? No i chodźmy dalej. Zobaczcie. I wcześniej, to jest jakby omówienie starych algorytmów, dlaczego to nie działało. Algorytmy sprawdzały sobie słowo po słowie, słowo po słowie, słowo po słowie, czy które jest najważniejsze, przez co, tu jest akurat nazwa, jakiś neural network sequence, coś tam, przez co to po prostu nie działało. I teraz zobaczcie tak. Jeżeli chodzi o ten self-attention, oni od razu sprawdzają każde słowo i typują najważniejsze, czyli słowo no. Dobra. I chodźcie dalej. W tym przykładzie również widzimy, że transformery są w stanie określić, że słowo interest, czyli zainteresowanie, jest rzeczownikiem w kontekście polityki i w tym zdaniu. Jak pójdziemy sobie dalej, Troszeczkę zmieni się teraz zdanie, które będziemy przerabiać. Czyli mamy bank interest rate rises, czyli oprocentowanie bankowe rośnie. I widzicie, w tym przypadku to słowo interest już nie jest związane tak jak tutaj z zainteresowaniem, tylko to samo słowo w innym kontekście, w innym szyku nagle zaczyna znaczyć coś innego, czyli w tym przykładzie po prostu procenty. I to właśnie robią transformery przez to porównywanie i ten self-attention, że są w stanie określić, co znaczy coś w kontekście, jakimś nam zdaniem itd. No i dobra. I widzicie, to cały czas można rozbudowywać, bo tak wyobraźmy sobie nasz content. And when we combine the sentence, the model is still able to recognize the correct meaning of each word, thanks to attention, give it to accomplished text. Czyli widzicie, jak rozbudowaliśmy po raz kolejny ten text, czyli I have no interest in hearing about the rising interest rate on the bank. Oni ciągle rozumieją, mimo że zdanie jest dużo dłuższe, że ten interest to są ciągle po prostu procenty i że jest najważniejsze. A dokładnie tutaj były nie procenty, tylko tutaj wiedział, że interest, czyli nie mam zainteresowania i zainteresowanie tutaj było ta samo słowa jako zainteresowanie, a w kolejnym zdaniu i w tym szyku jako rising rates i bank określa jako procenty. W tym momencie pojawia się zrozumienie kontekstu i dlatego Google trochę ociągnęło ten cel i dalej modele językowe zaczęły po prostu działać. No właśnie, jak czytamy dalej, te funkcje stały się kluczowe z perspektywy, widzicie, jak mam interest, profits, dividends, albo enthusiasm, encouragement. Czyli pewnie jak będziemy zamieniać te słowa, to nagle zdanie zaczyna znaczyć zupełnie coś innego. Czyli widzicie, jeżeli damy I have no interest in hearing about the rising enthusiasm rate of the bank. To to w ogóle nie funkcjonuje. I dlatego ten self-attention i transformer działa i z tego też AI działa. No i dobra, ostatni przykład, który Wam pokażę i zachęcam, żeby każdy sobie przeklikał tę stronę w Financial Times, bo jest po prostu fantastyczna. Mamy kolejne zdanie. The dog chewed the bone because it was hungry. No i tak, brać, czyli to znaczy mniej więcej tyle, że pies przeżył kość, ponieważ był głodny. No i jak zobaczymy, to słowo eat odnosi się do dog. I w ten sposób oni sobie połączyli, że dog i eat to jest ta sama rzecz. No nie? No bo w zdaniu tak to wychodzi po prostu, że pies żół, bo był głodny. Czyli ciągle ten pies. I zobaczcie. Jeżeli przejdziemy sobie dalej. If we alter the sentence rapping hungry for delicious, the model is able to reclite with it most likely the bone. Czyli w tym momencie sobie rozszerzyliśmy zdanie the dog chewed the bone because it was delicious. Czyli model po raz kolejny wie, że delicious dotyczy kości, którą przeżył pies. I tak dalej, i tak dalej. I w ten sposób generowany jest content przez modele sztucznej inteligencji. Po prostu oni wiedzą, które słowo, co znaczy w zdaniu i co z największym prawdopodobieństwem będzie następne w tym kontekście. I cała nasza zabawa ze sztuczną inteligencją to właśnie będzie walka o prawdopodobieństwo, które bezpośrednio wynika właśnie z architektury Transformerów, którą starałem Wam się pokazać. Jednak mimo wszystko każdego z Was zachęcam, po notatce do tej lekcji będzie link do Financial Timesa, żeby każdy sobie powolutku przeszedł tą stronę, bo jest naprawdę fantastyczna. Lepszej nie znalazłem. Podsumujmy. Czyli mamy na samym początku tą tokenizację, czyli w tym przypadku zmianę, zaraz Wam pokażę to zdanie, tam był pies, czy tam był bank. Później mamy word embedding, żeby sprawdzić te dystansy. Self attention, co z czego wynika i co się z czym łączy. I na końcu encoded output, czyli wynik z naszego transformera, który już po prostu obrazuje, co jest co. Ostatni przykład, na przykładzie Financial Timesa, on jest dosyć ciekawy. Zobaczcie. Wyobraźmy sobie, że w tym momencie generujemy content. AI generuje content. Generuje nam jakiś tam kontekst i szyk zdania. No i mamy po kolei. The financial times is. No właśnie, i co będzie następne po is? No i mamy jakieś tam tutaj propozycje about, more, abate, whatever. Chodźmy, zobaczymy, co Transformer nam z tego wszystkiego zrobi. Prowadza właśnie pojęcie probability score, czyli takim prawdopodobieństwem, które słowo po the financial time is wystąpi w tym konkretnym kontekście. No i widzimy, że tutaj w tym przypadku akurat about występuje jako najwyższe prawdopodobieństwo i economics jako następne słowo, czyli the financial time is about economics, co jest właśnie prawdą, bo ten financial time jest o ekonomii. No ale chodźmy dalej. I tam dalej może sobie budować, zobaczmy, tutaj jest właśnie budowanie tego, jak AI buduje nam zdania, czyli financial time is about economics i do tego dostanie doklejony jakiś end podcast, no nie? No i zobaczcie, i tych różnych wersji AI nam generuje powiedzmy kilka, w zależności od konfiguracji, którą dokładnie Damian będzie omawiał w swojej części. Natomiast w tym momencie AI wybiera nam wersję, która wystąpi z największym prawdopodobieństwem. I zobaczcie, mamy tutaj Pewnie po prostu wszystkie są prawidłowe, natomiast jak sobie policzą to prawdopodobieństwo, to im wyjdzie, że The Financial Times is a newspaper found in 1888. I w ten sposób AI właśnie generuje kontekst, content. I co jest naszym celem? Naszym celem jest to, żeby prawdopodobieństwo następnego tokenu w jakimś kontekście było jak najwyższe, co właśnie pokazuje ten przykład. Tu jest trochę więcej opisów na tej stronie. Zachęcam wszystkich do zapoznania się z tym. Mam nadzieję, udało mi się chociaż pokrótce wytłumaczyć, o co chodzi. Natomiast jak sobie przeklikacie tą stronę, to zrozumiecie doskonale, o co chodzi. Chodźmy dalej. I tam była informacja o tym, że jest wprowadzone pojęcie tokenu, transformacji na tokeny. W przykładzie Financial Timesa, żeby był najprostszy, tokenizer był po słowach. Natomiast modele sztucznej inteligencji trochę oderwały się od słów i słowa zamieniają na typowe zlepki lub fragmenty. I to jest taki przykład z tokenizera OpenAI, w którym najczęściej będziecie mieli styczność. I jak przeczytacie, o co tutaj chodzi, jest tutaj dosyć kluczowa informacja. Zobaczcie. Jeśli chodzi o procesowanie do tokenów. Process text using tokens, which are common sequence of characters found in a set of text. Czyli wasze zdania, wasze słowa zaczynają być zmieniane na najprawdopodobniej albo na typowe sekwencje znaków. Co zobaczymy tutaj? Zobaczcie. Pisałem w zdaniu Hej, co u Ciebie słychać? I zobaczcie. Mieliśmy 24 znaki. Zostało zmienione na 11 tokenów. I te kolorki oznaczają tokeny, które AI sobie dalej będzie przetwarzać. Ten transformer w AI. Później ten self-attention i tak dalej, prawdopodobieństwa i co występuje po czymś. Podsumujmy to. Jeden token, zazwyczaj po angielsku to są 4 znaki ze spacją. znaczy jeden token to są cztery znaki bez spacji a powiedzmy 1500 sub to jest 2048 tokenów tak to się mniej więcej przelicza i dlaczego to jest super ważne? bo w każdej waszej akcji wy po prostu zapłacicie za tokeny dodatkowo wyszukiwarka stosuje transformery w swojej architekturze do oceniania waszych treści, więc wasze treści bezpośrednio również zostaną zamienione na token dlatego to jest ważne jest to również ważne, ponieważ kodowanie języku angielskim będzie tańsze niż kodowanie na tokeny w języku polskim. Przez co na przykład rozmawiając ze sztuczną inteligencją, promptując ze sztuczną inteligencją, jeżeli będziecie robić to w języku angielskim, to po prostu będzie tańsze aniżeli w języku polskim. Także Damian Wam to wytłumaczy w części dotyczącej prompting engineeringu. Ja tylko zaznaczam, że to jest ważne i totalnie odrywa się od warstwy słów. Dalej. 2018 rok. Google wprowadza Berta. Jest to algorytm, albo transformer, zasadniczo, bo to jest bi-directional transformer, dwustronny, który obecnie rządzi wynikami wyszukiwania. I to właśnie BERT ocenia Wasze strony internetowe. I to właśnie BERT ocenia Wasz content. W kolejnych częściach pokażemy Wam, jak lepiej się dogadywać z tym BERTem i dlaczego warto po prostu pamiętać o tych tokenach i o budowie zdań. No i zobaczcie. W tym samym roku powstaje pierwszy GPT od OpenAI. 2018 rok. 6 lat temu. Natomiast niewiele się wydarzyło w tym czasie. Google zapowiedziało pierwsze podrygi multimodalności, czyli tego, że zrobimy Wam search overview albo AI overview, albo AI search w wynikach wyszukiwania w 2021 roku. Chodziło o to, że Google myślałoby wyszukiwać głosem, wyszukiwać obrazem, natomiast w tamtym czasie nie było technologii, żeby to zrealizować. No i zobaczcie, dwa lata temu powstał czas GPT i wszystko przyspieszyło. I obecnie jesteśmy, jakby miejsce, w którym jesteśmy tu i teraz dzisiaj, czyli GPT-40 i O-1, czyli 2024 rok. Także całość procesu takiego akceleracji w sumie od 2018 roku, a ostatnie dwa lata to istny pociąg Pendolino, a raczej Shinkansen w rozwoju tej technologii. No właśnie, i w ten sposób Google zostało, nie Google, tylko OpenAI, zostało opycenione na 157 miliardów dolarów. Jest to jedna z największych wartości prywatnych firm na świecie. I to jest doskonały przykład właśnie. Zobaczcie, wartość, to jest wartość OpenAI, 157 miliardów. I firmy takie jak Zoom, Warner Music Group, Snapchat, Dominos, Duolingo, The Vs, The New York Times są mniejsze, a raczej ich suma dopiero może się złożyć na OpenAI, które przyspieszyło ten rozwój od dwa lata temu. No i cześć, słuchaj, witam Cię w 2025 roku. Powiedzieć, że AI się rozwija, to nic nie powiedzieć. Powiedzieć, że dużo się zmieniło, to również nic nie powiedzieć. Powiedzieć, że postęp jest wykładniczy, to też nic nie powiedzieć, bo postęp jest wykładniczo-wykładniczy. Przez chwilę pokazałem Ci wycenę. Tą część nagrałem w zeszłym roku. Wycena sięgała 157 miliardów dolarów. W 2025 roku, po kilku miesiącach, tamtą część kursu nagrywałem pewnie koło września albo października, w kwietniu tego roku OpenAI uzyskał już wycenę 300 miliardów dolarów, czyli co do zasady wycena się podwoiła. Na poziomie federalnym Stanów Zjednoczonych również jest gigantyczne wsparcie dla sztucznej inteligencji. Powstały programy, które finansują rozwój sztucznej inteligencji do poziomu 500 miliardów dolarów, więc jest niesamowicie. W 2025 roku mówimy już o Ricksoningu, czyli mamy tutaj modele Gemini 2,5, mamy O3, pojawiają się modele klasy O4, więc jest niesamowicie i modele zaczęły osiągać inteligencję przewyższającą powiedzmy średniego człowieka, jeśli można mówić o średnim człowieku, no ale jeżeli to jest rozkład normalny i średnio na świecie człowiek ma IQ na poziomie 100, no to w tym momencie modele w 2025 roku osiągają już wartości na poziom 130 i przekraczają właśnie dzięki temu reasoningowi, o którym będziemy bardzo dużo mówić w tym kursie, bo jest niezwykle przydatny w kontekście SEO i w kontekście marketingowym. Dla porównania Albert Einstein miał IQ na poziomie 167, także jeszcze troszeczkę brakuje modelom do poziomu Ale wiem, że to się wydarzy w takim tempie pewnie w tym roku, a jeżeli nie, to na pewno w następnym. Otrzymaliśmy super fotorealistyczne zdjęcia od OpenAI. Totalny detal, możliwość generacji totalnie fajnych napisów, które są napisami do tej pory w zeszłym roku. Raczej napisy były przypadkowymi znakami albo jakimiś krzaczkami. W tym momencie jesteśmy w takim miejscu, gdzie to naprawdę już zaczyna fantastycznie wyglądać. Powstało narzędzie Ideogram, które polecam. Mateusz Godzic w swojej ostatniej części tego kursu na temat grafiki i wideo na pewno pokaże Ci to narzędzie. Jest fantastyczne do tworzenia zdjęć. Bardzo dużo się wydarzyło przez te kilka miesięcy w tym kontekście, ale również w kontekście wideo. W zeszłym roku prezentowałem Ci, albo członkom kursu, którzy byli w zeszłym roku, być może nie Tobie, wideo, które nie wyglądało tak, jak to, co Ci pokazuje. Absolutny detal, kontrola kamery. Dostaliśmy również informację, że Oscary zostaną rozdane w kategorii AI i firmy AI zostaną dopuszczone do Oscarów. Filmy są generowane również w formie storytellingu, czyli można opowiadać historię na wielu filmach. Filmy generują również dźwięk, na przykład w tym przypadku, jak mamy chłopca, który porusza się po lesie, być może gdzieś jesteśmy w stanie wygenerować, na przykład jakiś tam szelest lasu, czy jakiś tam dźwięk, jakieś zwierząt, które się znajdują za tym drzewem. I absolutna multimodalność w każdym wertykalu. W zeszłym roku mówiliśmy o multimodalności dotyczącej zdjęć, w tym momencie mamy dźwięk, pełne wnioskowanie po zdjęciach, rezonning po zdjęciach, czyli jesteśmy w stanie na przykład wrzucić kreację reklamową i zapytać szóstej inteligencji, co poprawić w celu lepszego performance w mojej kampanii reklamowej, albo wrzucić wszystkie zdjęcia, przypominam, że Facebook ma ten marketplace reklamowy, gdzie możemy podglądać konkurencję, wrzucić wszystkie kreacje reklamowe naszej konkurencji i zapytać sztucznej inteligencji, co tam najlepiej działa, a później to wygenerować w sztucznej inteligencji, więc jesteśmy w stanie nie tylko wnioskować po treści, po tekstach, ale również po obrazach na dzień dzisiejszy, także tutaj mamy bardzo duże przyspieszenie rozwoju. I żeby zarysować to na osi czasu, mniej więcej, no nie na osi czasu, na perspektywie rozwojowej, to w zeszłym roku raczej mówiliśmy o tym Gen AI, czy Generative AI, generowanie słów, tekstów, zdjęć, powoli wideo. W tym momencie widziałeś, jak to wygląda niesamowicie. Jesteśmy gdzieś tu. Za każdym razem, jak pokazuję ten slajd, przesuwam odrobinkę, tą strzałeczkę, bo już jesteśmy w erze agentów. Natomiast jeszcze wymaga to chwileczkę czasu, żeby one faktycznie funkcjonowały. W tym kursie również będziemy opowiadać o agentach, jak tworzyć agentów autonomicznych. Kolejnym krokiem będzie physical AI. Więc jak spotkamy się w Sensei Akademii, być może w 2026 roku ta strzałeczka będzie już dużo wyżej po stronie Physical AI, ponieważ trening trwa. To jest prezent zarządu NVIDI, który zdaje się w tym roku, gdzieś koło lutego, poinformował, że istnieje coś, co się nazywa NVIDIA Cosmos. Jest to świat równoległy do treningu właśnie Physical AI, czyli czego? Czyli samochodów autonomicznych. Bo na świecie nie ma wystarczająco dużo filmów z jazdy samochodów, na których można by samochody autonomiczne trenować. Te samochody są trenowane właśnie w tym NVIDIA Kosmos, czyli świecie równoległym, gdzie dane treningowe są generowane na potrzeby treningu. Humanoidy. Jest masę informacji, że np. humanoidy w Chinach będą produkować iPhony. Tego nie było w zeszłym roku. W tym świecie wirtualnym, w Kosmosie te humanoidy właśnie są trenowane. Czy też prezentacje tej klasy, to jest polska firma z Wrocławia, która buduje humanoida, jakby to ładnie powiedzieć, mięśniowo-szkieletowego, który ma odwzorować człowieka, mięśnie i sposób poruszania się, nie tylko robocik, ale tej klasy. Ja nie chcę wiedzieć, co ludzie będą z tym robić za kilka lat. Natomiast takie rzeczy również się dzieją w 2025 i jestem bardzo szczęśliwy, że się dzieją akurat w Polsce, we Wrocławiu. Czy też neuoprotezowanie mózgu. Naukowcy z wykorzystaniem sztucznej inteligencji są w stanie wszczepić protezę człowiekowi do mózgu i dzięki temu on odzyskuje mowę w jakimś tam stopniu, czy jest w stanie się komunikować dzięki falomózgowym, które sztuczna inteligencja rozumie. Dodatkowo w tym roku również naukowcy Google DeepMind podali informację, że są w stanie, czy w procesie dekodowania języka delfinów. Mamy masę informacji z medycyny, na przykład diagnostyki gruźlicy, przewidywanie nowych leków. Tak więc od zeszłego roku, od poprzedniego Sensei Akademii wydarzyło się bardzo, bardzo dużo. W tym kursie będziemy omawiać wszystkie kluczowe informacje dotyczące SEO i marketingu z najnowszą technologią, także zapraszam Cię do kolejnej lekcji, powoli przechodzimy do konkretu w kolejnej lekcji opowiem Ci, czym są modele językowe, bo głównie na nich będziemy się skupiać jakie są ich wyzwania jak sobie z tym radzić, no i co? zapraszam Cię dalej, cześć! 
# Transkrypcje Lekcji - Tydzień 1

## Lekcja: Historia modeli językowych

Cześć! Witam Cię w pierwszej lekcji kursu Sensei. W pierwszej lekcji porozmawiamy sobie chwileczkę o historii oraz postaram Ci się na przykładzie opracowania, które stworzył Financial Times, wytłumaczyć, jak działa AI, skąd to się wzięło i w ogóle dlaczego to działa. Ale po kolei. Zacznijmy od historii. Dwa słowa, szybciutko. W mojej opinii, ale też w sumie na podstawie patentów i technologii, Google jest drajwerem i powodem rozwoju sztucznej inteligencji na świecie, to dzięki ich technologii znamy obecne AI, jakie znamy. Teraz Ci pokażę, jak to działa i dlaczego działa, przynajmniej spróbuję. Ale skąd to się wzięło? Wyobraźmy sobie wyszedł w arkę Google i ich największy problem. Ich największym problemem jest oczywiście potrzeba zrozumienia kontekstu zapytania. Przykład. Piszesz czarna konsola do gier, myślisz PlayStation. Albo piszesz bajka o różowej śwince, myślisz świnka Pepa. Google stało przed wyzwaniem, żeby połączyć z jednej strony zapytania, które wprowadzamy do wyszukiwarki, ale i połączyć z kontentem z tych stron internetowych, które prezentuje nam w wynikach wyszukiwania. Dodatkowo Google też musiało zrozumieć treść na naszych stronach internetowych i po raz kolejny dopasować je do wyników wyszukiwania. jest jedna zasada, która przyświeca Googlowi. Ich głównym, nadrzędnym celem jest dostarczenie sobie jak najlepszych wyników wyszukiwania i dlatego powstały technologie, jak na przykład transformery, do których zaraz dojdziemy i to one rządzą obecnym AI, takim, jakie znamy dzisiaj. Słowo historii. Pierwsza rzecz. 2013 rok Google jest ciągle na tej samej ścieżce rozwoju, czyli potrzebuje znaleźć kontekst zapytania oraz treści na stronie internetowych i powstał Word2Vector. Jest to pierwszy algorytm, którym z Damianem mówimy od zawsze o tym algorytmie, jednak nie sposób go ominąć w kontekście sztucznej inteligencji. Jest to zamienienie słów na wektory. Jak widzimy na tej animacji, nagle słowa opisane w przestrzeni wektorowej zaczęły być blisko siebie. W tym momencie Google zrozumiało, że ok, że pianino jest blisko do skrzypiec albo Mercedes do Volkswagena jako samochód. To jeszcze im nie dało możliwości zrozumienia kontekstu, ale na początku w ten sposób zaczęli przetwarzać wasze strony internetowe i tak też zaczęło powstawać pierwsze topical authority, bo skoro jesteśmy na przykład w kontekście instrumentów muzycznych, należy omówić pianino, skrzypce, gitarę i tak dalej. Z tego to się wzięło. czyli to był 2013 rok w 2014 roku Google opublikowało kolejny algorytm, czy kolejną metodę, którą nazwało sequence to sequence i na tej podstawie Google było w stanie stworzyć Google Translate'a, czyli tłumaczenie maszynowe podejrzewam, że do tej pory w ten sposób to działa okazało się, że na przestrzeni wektorowej słowo pies i słowo dog było po prostu w podobnej pozycji przez co Google było w stanie tłumaczyć na przykład z języka angielskiego na język polski i tak działa Google Translate ale to nie było kluczowe w kontekście AI W kontekście AI kluczowa jest architektura transformerów. Postaram Ci się to omówić na przykładzie chyba najlepszego opracowania, które ostatnimi czasy przygotował Financial Times. Znajduje się on, to opracowanie pod tym adresem i wszystkich zachęcam, żeby tam wejść i poczytać, co tam jest napisane, ponieważ ta strona jest fantastyczna, tam się przełączę i po kolei będziemy sobie rozmawiać, co przygotował Financial Times w obszarze transformerów. Okej. Tu jest chwileczka historii, skąd to się wzięło i tak dalej. No i zobaczmy. Mamy jakieś zdanie we go to work by train, czyli jedziemy do pracy pociągiem, no nie? I architektura transformerów wprowadza pojęcie tokenów. Każdy model językowy ma troszeczkę inny tokenizer, czyli jak tutaj zobaczymy, o ja mam taki świetny świetny pilocik od Mateusza, zobaczcie jaki bajer, o chyba to nie jest Ten bajer znajduje się tu. Czytamy tutaj opowied. Mamy represent fraction of the words, czyli tokeny w obecnym AI reprezentują fragment słowa. Pokażę to na kolejnych slajdach tej prezentacji, natomiast na potrzeby uproszczenia omówienia Financial Times stokenizował zdanie we go to work by train po słowach, żeby łatwiej było zrozumieć koncepcję. Czyli pierwsza sprawa. Mamy zdanie. Jedziemy do pracy pociągiem, tokenizujemy go, to organizujemy to zdanie na pojedyncze słowa. Chodźmy dalej. Dalej. Algorytmy sztucznej inteligencji, czy też na razie transformery jeszcze, szukają prawdopodobieństwa, w jakim prawdopodobieństwie słowo work może wystąpić, czyli because I work in my, coś tam, albo people who work from home, co nie? Czyli w tym momencie architektura transformerów szuka czegoś tutaj mamy nazwane nearby words, czyli słowa bliskie. Co się dzieje dalej? No, zobaczcie. I tutaj dzięki takiemu ćwiczeniu architektura transformerów jest w stanie określić, które słowa są blisko do słowa praca, a które są daleko. To tak na przykładzie tego Word2Vector, tam już było wiadomo, co jest blisko, co jest daleko. I przykład. Work Polka, to jest bardzo daleko. Work Zebra, albo Work Atmosphere, albo Work Dove, Dove to jest gołąb, będą daleko. ale work processes, czy work that, czy work hour, work to, work for, albo work meet, są blisko. Czyli w tym momencie transformery już wybierają słowa, które są blisko słowa work, a które są daleko odrzucając je jakby z dalszego przetwarzania. I teraz zobaczmy. W tym momencie zachodzi proces tak zwany embeddingów, czyli zmiany na te wektory, które były pokazane na przykładzie word to vector, Czyli każdy z tych jakby zlepków, work, plus coś tam, co jest jakimś prawdopodobieństwem, staje się opisane jakimś tam embeddingiem, przez co transformery są w stanie mierzyć dystanse już matematycznie. I teraz będzie fajny przykład, do którego przejdę od razu. O co z tym wszystkim chodzi? Damian w swojej części tego szkolenia będzie bardzo szczegółowo opowiadał, z czego to wynika. Natomiast przyjrzyjmy się po prostu. Jeżeli mamy słowo C opisane jakimś tam embeddingiem, czyli jakimś tam wektorem, blisko znajduje się ocean, czyli sea jako morze i ocean jako ocean. I w ten sposób Transformer wie, że to są słowa bardzo blisko, ale znaczą coś innego. Bo tutaj mamy na przykład football i soccer i one też znaczą praktycznie to samo, ale jednak nie do końca. Albo I, albo we. I w ten sposób sobie opisuje dystansami, co jest blisko, co jest daleko. Chodźmy dalej. I dzięki temu, teraz zobaczycie to, co przed chwilą oglądaliśmy na Word2 Vectorze. Google sobie zrobiło klastry, no nie? Czyli mamy na przykład train, bus, car, czyli w tym momencie Google wie, czy ten transformer wie, że to są jakieś pojazdy, a tutaj będziemy mieli jakieś colleague, school, work, no nie? Albo walk, swim i run, czyli to pewnie będzie jakiś tam klaster sportu, no nie? Jak widzicie, tutaj jest dokładnie opisane o co chodzi. Musicie pamiętać o tym, że w ten sposób te słowa stają się blisko siebie, Ale to jeszcze nic nie znaczy w kontekście transformeru. Na razie po prostu wiemy, że coś jest blisko siebie. Jak sobie przeczytacie dalej i przejdziemy dalej, zaczynamy wchodzić w kontekst transformeru. Kolejne przykładowe zdanie. I have no interest in politics. Czyli nie jestem zainteresowany polityką. I tutaj pojawia się słowo klucz. Tutaj. Self-attention. Self-attention znaczy uważność albo samouważność. Chodzi o to, że w tym momencie transformery zaczynają rozumieć, które słowo się z czym wiąże i co to tak naprawdę zaczyna znaczyć. Bo w tym momencie wiedzą, co jest blisko siebie albo co jest w klastrze na przykład pojazdów, sportu, pianin, instrumentów muzycznych. W tym momencie transformery zaczynają rozumieć, co znaczy co. Chodźmy dalej do przykładów. To jest świetnie opisane. Zobaczcie. I transformery zaczynają porównywać słowa ze sobą i nagle wiedzą, że no w tym zdaniu zaczyna ważyć najwięcej. Self-attentional look at the each token in the body, text and decide which one are most important to understanding the meaning. Czyli w tym momencie zaczynają rozumieć, które słowo znaczy najwięcej dla znaczenia całego zdania. No i w tym konkretnym przypadku jest no, jako nie mam zainteresowania polityką, prawda? Inaczej, I have interest in politics, to znaczy zupełnie coś innego, prawda? No i chodźmy dalej. Zobaczcie. I wcześniej, to jest jakby omówienie starych algorytmów, dlaczego to nie działało. Algorytmy sprawdzały sobie słowo po słowie, słowo po słowie, słowo po słowie, czy które jest najważniejsze, przez co, tu jest akurat nazwa, jakiś neural network sequence, coś tam, przez co to po prostu nie działało. I teraz zobaczcie tak. Jeżeli chodzi o ten self-attention, oni od razu sprawdzają każde słowo i typują najważniejsze, czyli słowo no. Dobra. I chodźcie dalej. W tym przykładzie również widzimy, że transformery są w stanie określić, że słowo interest, czyli zainteresowanie, jest rzeczownikiem w kontekście polityki i w tym zdaniu. Jak pójdziemy sobie dalej, Troszeczkę zmieni się teraz zdanie, które będziemy przerabiać. Czyli mamy bank interest rate rises, czyli oprocentowanie bankowe rośnie. I widzicie, w tym przypadku to słowo interest już nie jest związane tak jak tutaj z zainteresowaniem, tylko to samo słowo w innym kontekście, w innym szyku nagle zaczyna znaczyć coś innego, czyli w tym przykładzie po prostu procenty. I to właśnie robią transformery przez to porównywanie i ten self-attention, że są w stanie określić, co znaczy coś w kontekście, jakimś nam zdaniem itd. No i dobra. I widzicie, to cały czas można rozbudowywać, bo tak wyobraźmy sobie nasz content. And when we combine the sentence, the model is still able to recognize the correct meaning of each word, thanks to attention, give it to accomplished text. Czyli widzicie, jak rozbudowaliśmy po raz kolejny ten text, czyli I have no interest in hearing about the rising interest rate on the bank. Oni ciągle rozumieją, mimo że zdanie jest dużo dłuższe, że ten interest to są ciągle po prostu procenty i że jest najważniejsze. A dokładnie tutaj były nie procenty, tylko tutaj wiedział, że interest, czyli nie mam zainteresowania i zainteresowanie tutaj było ta samo słowa jako zainteresowanie, a w kolejnym zdaniu i w tym szyku jako rising rates i bank określa jako procenty. W tym momencie pojawia się zrozumienie kontekstu i dlatego Google trochę ociągnęło ten cel i dalej modele językowe zaczęły po prostu działać. No właśnie, jak czytamy dalej, te funkcje stały się kluczowe z perspektywy, widzicie, jak mam interest, profits, dividends, albo enthusiasm, encouragement. Czyli pewnie jak będziemy zamieniać te słowa, to nagle zdanie zaczyna znaczyć zupełnie coś innego. Czyli widzicie, jeżeli damy I have no interest in hearing about the rising enthusiasm rate of the bank. To to w ogóle nie funkcjonuje. I dlatego ten self-attention i transformer działa i z tego też AI działa. No i dobra, ostatni przykład, który Wam pokażę i zachęcam, żeby każdy sobie przeklikał tę stronę w Financial Times, bo jest po prostu fantastyczna. Mamy kolejne zdanie. The dog chewed the bone because it was hungry. No i tak, brać, czyli to znaczy mniej więcej tyle, że pies przeżył kość, ponieważ był głodny. No i jak zobaczymy, to słowo eat odnosi się do dog. I w ten sposób oni sobie połączyli, że dog i eat to jest ta sama rzecz. No nie? No bo w zdaniu tak to wychodzi po prostu, że pies żół, bo był głodny. Czyli ciągle ten pies. I zobaczcie. Jeżeli przejdziemy sobie dalej. If we alter the sentence rapping hungry for delicious, the model is able to reclite with it most likely the bone. Czyli w tym momencie sobie rozszerzyliśmy zdanie the dog chewed the bone because it was delicious. Czyli model po raz kolejny wie, że delicious dotyczy kości, którą przeżył pies. I tak dalej, i tak dalej. I w ten sposób generowany jest content przez modele sztucznej inteligencji. Po prostu oni wiedzą, które słowo, co znaczy w zdaniu i co z największym prawdopodobieństwem będzie następne w tym kontekście. I cała nasza zabawa ze sztuczną inteligencją to właśnie będzie walka o prawdopodobieństwo, które bezpośrednio wynika właśnie z architektury Transformerów, którą starałem Wam się pokazać. Jednak mimo wszystko każdego z Was zachęcam, po notatce do tej lekcji będzie link do Financial Timesa, żeby każdy sobie powolutku przeszedł tą stronę, bo jest naprawdę fantastyczna. Lepszej nie znalazłem. Podsumujmy. Czyli mamy na samym początku tą tokenizację, czyli w tym przypadku zmianę, zaraz Wam pokażę to zdanie, tam był pies, czy tam był bank. Później mamy word embedding, żeby sprawdzić te dystansy. Self attention, co z czego wynika i co się z czym łączy. I na końcu encoded output, czyli wynik z naszego transformera, który już po prostu obrazuje, co jest co. Ostatni przykład, na przykładzie Financial Timesa, on jest dosyć ciekawy. Zobaczcie. Wyobraźmy sobie, że w tym momencie generujemy content. AI generuje content. Generuje nam jakiś tam kontekst i szyk zdania. No i mamy po kolei. The financial times is. No właśnie, i co będzie następne po is? No i mamy jakieś tam tutaj propozycje about, more, abate, whatever. Chodźmy, zobaczymy, co Transformer nam z tego wszystkiego zrobi. Prowadza właśnie pojęcie probability score, czyli takim prawdopodobieństwem, które słowo po the financial time is wystąpi w tym konkretnym kontekście. No i widzimy, że tutaj w tym przypadku akurat about występuje jako najwyższe prawdopodobieństwo i economics jako następne słowo, czyli the financial time is about economics, co jest właśnie prawdą, bo ten financial time jest o ekonomii. No ale chodźmy dalej. I tam dalej może sobie budować, zobaczmy, tutaj jest właśnie budowanie tego, jak AI buduje nam zdania, czyli financial time is about economics i do tego dostanie doklejony jakiś end podcast, no nie? No i zobaczcie, i tych różnych wersji AI nam generuje powiedzmy kilka, w zależności od konfiguracji, którą dokładnie Damian będzie omawiał w swojej części. Natomiast w tym momencie AI wybiera nam wersję, która wystąpi z największym prawdopodobieństwem. I zobaczcie, mamy tutaj Pewnie po prostu wszystkie są prawidłowe, natomiast jak sobie policzą to prawdopodobieństwo, to im wyjdzie, że The Financial Times is a newspaper found in 1888. I w ten sposób AI właśnie generuje kontekst, content. I co jest naszym celem? Naszym celem jest to, żeby prawdopodobieństwo następnego tokenu w jakimś kontekście było jak najwyższe, co właśnie pokazuje ten przykład. Tu jest trochę więcej opisów na tej stronie. Zachęcam wszystkich do zapoznania się z tym. Mam nadzieję, udało mi się chociaż pokrótce wytłumaczyć, o co chodzi. Natomiast jak sobie przeklikacie tą stronę, to zrozumiecie doskonale, o co chodzi. Chodźmy dalej. I tam była informacja o tym, że jest wprowadzone pojęcie tokenu, transformacji na tokeny. W przykładzie Financial Timesa, żeby był najprostszy, tokenizer był po słowach. Natomiast modele sztucznej inteligencji trochę oderwały się od słów i słowa zamieniają na typowe zlepki lub fragmenty. I to jest taki przykład z tokenizera OpenAI, w którym najczęściej będziecie mieli styczność. I jak przeczytacie, o co tutaj chodzi, jest tutaj dosyć kluczowa informacja. Zobaczcie. Jeśli chodzi o procesowanie do tokenów. Process text using tokens, which are common sequence of characters found in a set of text. Czyli wasze zdania, wasze słowa zaczynają być zmieniane na najprawdopodobniej albo na typowe sekwencje znaków. Co zobaczymy tutaj? Zobaczcie. Pisałem w zdaniu Hej, co u Ciebie słychać? I zobaczcie. Mieliśmy 24 znaki. Zostało zmienione na 11 tokenów. I te kolorki oznaczają tokeny, które AI sobie dalej będzie przetwarzać. Ten transformer w AI. Później ten self-attention i tak dalej, prawdopodobieństwa i co występuje po czymś. Podsumujmy to. Jeden token, zazwyczaj po angielsku to są 4 znaki ze spacją. znaczy jeden token to są cztery znaki bez spacji a powiedzmy 1500 sub to jest 2048 tokenów tak to się mniej więcej przelicza i dlaczego to jest super ważne? bo w każdej waszej akcji wy po prostu zapłacicie za tokeny dodatkowo wyszukiwarka stosuje transformery w swojej architekturze do oceniania waszych treści, więc wasze treści bezpośrednio również zostaną zamienione na token dlatego to jest ważne jest to również ważne, ponieważ kodowanie języku angielskim będzie tańsze niż kodowanie na tokeny w języku polskim. Przez co na przykład rozmawiając ze sztuczną inteligencją, promptując ze sztuczną inteligencją, jeżeli będziecie robić to w języku angielskim, to po prostu będzie tańsze aniżeli w języku polskim. Także Damian Wam to wytłumaczy w części dotyczącej prompting engineeringu. Ja tylko zaznaczam, że to jest ważne i totalnie odrywa się od warstwy słów. Dalej. 2018 rok. Google wprowadza Berta. Jest to algorytm, albo transformer, zasadniczo, bo to jest bi-directional transformer, dwustronny, który obecnie rządzi wynikami wyszukiwania. I to właśnie BERT ocenia Wasze strony internetowe. I to właśnie BERT ocenia Wasz content. W kolejnych częściach pokażemy Wam, jak lepiej się dogadywać z tym BERTem i dlaczego warto po prostu pamiętać o tych tokenach i o budowie zdań. No i zobaczcie. W tym samym roku powstaje pierwszy GPT od OpenAI. 2018 rok. 6 lat temu. Natomiast niewiele się wydarzyło w tym czasie. Google zapowiedziało pierwsze podrygi multimodalności, czyli tego, że zrobimy Wam search overview albo AI overview, albo AI search w wynikach wyszukiwania w 2021 roku. Chodziło o to, że Google myślałoby wyszukiwać głosem, wyszukiwać obrazem, natomiast w tamtym czasie nie było technologii, żeby to zrealizować. No i zobaczcie, dwa lata temu powstał czas GPT i wszystko przyspieszyło. I obecnie jesteśmy, jakby miejsce, w którym jesteśmy tu i teraz dzisiaj, czyli GPT-40 i O-1, czyli 2024 rok. Także całość procesu takiego akceleracji w sumie od 2018 roku, a ostatnie dwa lata to istny pociąg Pendolino, a raczej Shinkansen w rozwoju tej technologii. No właśnie, i w ten sposób Google zostało, nie Google, tylko OpenAI, zostało opycenione na 157 miliardów dolarów. Jest to jedna z największych wartości prywatnych firm na świecie. I to jest doskonały przykład właśnie. Zobaczcie, wartość, to jest wartość OpenAI, 157 miliardów. I firmy takie jak Zoom, Warner Music Group, Snapchat, Dominos, Duolingo, The Vs, The New York Times są mniejsze, a raczej ich suma dopiero może się złożyć na OpenAI, które przyspieszyło ten rozwój od dwa lata temu. No i cześć, słuchaj, witam Cię w 2025 roku. Powiedzieć, że AI się rozwija, to nic nie powiedzieć. Powiedzieć, że dużo się zmieniło, to również nic nie powiedzieć. Powiedzieć, że postęp jest wykładniczy, to też nic nie powiedzieć, bo postęp jest wykładniczo-wykładniczy. Przez chwilę pokazałem Ci wycenę. Tą część nagrałem w zeszłym roku. Wycena sięgała 157 miliardów dolarów. W 2025 roku, po kilku miesiącach, tamtą część kursu nagrywałem pewnie koło września albo października, w kwietniu tego roku OpenAI uzyskał już wycenę 300 miliardów dolarów, czyli co do zasady wycena się podwoiła. Na poziomie federalnym Stanów Zjednoczonych również jest gigantyczne wsparcie dla sztucznej inteligencji. Powstały programy, które finansują rozwój sztucznej inteligencji do poziomu 500 miliardów dolarów, więc jest niesamowicie. W 2025 roku mówimy już o Ricksoningu, czyli mamy tutaj modele Gemini 2,5, mamy O3, pojawiają się modele klasy O4, więc jest niesamowicie i modele zaczęły osiągać inteligencję przewyższającą powiedzmy średniego człowieka, jeśli można mówić o średnim człowieku, no ale jeżeli to jest rozkład normalny i średnio na świecie człowiek ma IQ na poziomie 100, no to w tym momencie modele w 2025 roku osiągają już wartości na poziom 130 i przekraczają właśnie dzięki temu reasoningowi, o którym będziemy bardzo dużo mówić w tym kursie, bo jest niezwykle przydatny w kontekście SEO i w kontekście marketingowym. Dla porównania Albert Einstein miał IQ na poziomie 167, także jeszcze troszeczkę brakuje modelom do poziomu Ale wiem, że to się wydarzy w takim tempie pewnie w tym roku, a jeżeli nie, to na pewno w następnym. Otrzymaliśmy super fotorealistyczne zdjęcia od OpenAI. Totalny detal, możliwość generacji totalnie fajnych napisów, które są napisami do tej pory w zeszłym roku. Raczej napisy były przypadkowymi znakami albo jakimiś krzaczkami. W tym momencie jesteśmy w takim miejscu, gdzie to naprawdę już zaczyna fantastycznie wyglądać. Powstało narzędzie Ideogram, które polecam. Mateusz Godzic w swojej ostatniej części tego kursu na temat grafiki i wideo na pewno pokaże Ci to narzędzie. Jest fantastyczne do tworzenia zdjęć. Bardzo dużo się wydarzyło przez te kilka miesięcy w tym kontekście, ale również w kontekście wideo. W zeszłym roku prezentowałem Ci, albo członkom kursu, którzy byli w zeszłym roku, być może nie Tobie, wideo, które nie wyglądało tak, jak to, co Ci pokazuje. Absolutny detal, kontrola kamery. Dostaliśmy również informację, że Oscary zostaną rozdane w kategorii AI i firmy AI zostaną dopuszczone do Oscarów. Filmy są generowane również w formie storytellingu, czyli można opowiadać historię na wielu filmach. Filmy generują również dźwięk, na przykład w tym przypadku, jak mamy chłopca, który porusza się po lesie, być może gdzieś jesteśmy w stanie wygenerować, na przykład jakiś tam szelest lasu, czy jakiś tam dźwięk, jakieś zwierząt, które się znajdują za tym drzewem. I absolutna multimodalność w każdym wertykalu. W zeszłym roku mówiliśmy o multimodalności dotyczącej zdjęć, w tym momencie mamy dźwięk, pełne wnioskowanie po zdjęciach, rezonning po zdjęciach, czyli jesteśmy w stanie na przykład wrzucić kreację reklamową i zapytać szóstej inteligencji, co poprawić w celu lepszego performance w mojej kampanii reklamowej, albo wrzucić wszystkie zdjęcia, przypominam, że Facebook ma ten marketplace reklamowy, gdzie możemy podglądać konkurencję, wrzucić wszystkie kreacje reklamowe naszej konkurencji i zapytać sztucznej inteligencji, co tam najlepiej działa, a później to wygenerować w sztucznej inteligencji, więc jesteśmy w stanie nie tylko wnioskować po treści, po tekstach, ale również po obrazach na dzień dzisiejszy, także tutaj mamy bardzo duże przyspieszenie rozwoju. I żeby zarysować to na osi czasu, mniej więcej, no nie na osi czasu, na perspektywie rozwojowej, to w zeszłym roku raczej mówiliśmy o tym Gen AI, czy Generative AI, generowanie słów, tekstów, zdjęć, powoli wideo. W tym momencie widziałeś, jak to wygląda niesamowicie. Jesteśmy gdzieś tu. Za każdym razem, jak pokazuję ten slajd, przesuwam odrobinkę, tą strzałeczkę, bo już jesteśmy w erze agentów. Natomiast jeszcze wymaga to chwileczkę czasu, żeby one faktycznie funkcjonowały. W tym kursie również będziemy opowiadać o agentach, jak tworzyć agentów autonomicznych. Kolejnym krokiem będzie physical AI. Więc jak spotkamy się w Sensei Akademii, być może w 2026 roku ta strzałeczka będzie już dużo wyżej po stronie Physical AI, ponieważ trening trwa. To jest prezent zarządu NVIDI, który zdaje się w tym roku, gdzieś koło lutego, poinformował, że istnieje coś, co się nazywa NVIDIA Cosmos. Jest to świat równoległy do treningu właśnie Physical AI, czyli czego? Czyli samochodów autonomicznych. Bo na świecie nie ma wystarczająco dużo filmów z jazdy samochodów, na których można by samochody autonomiczne trenować. Te samochody są trenowane właśnie w tym NVIDIA Kosmos, czyli świecie równoległym, gdzie dane treningowe są generowane na potrzeby treningu. Humanoidy. Jest masę informacji, że np. humanoidy w Chinach będą produkować iPhony. Tego nie było w zeszłym roku. W tym świecie wirtualnym, w Kosmosie te humanoidy właśnie są trenowane. Czy też prezentacje tej klasy, to jest polska firma z Wrocławia, która buduje humanoida, jakby to ładnie powiedzieć, mięśniowo-szkieletowego, który ma odwzorować człowieka, mięśnie i sposób poruszania się, nie tylko robocik, ale tej klasy. Ja nie chcę wiedzieć, co ludzie będą z tym robić za kilka lat. Natomiast takie rzeczy również się dzieją w 2025 i jestem bardzo szczęśliwy, że się dzieją akurat w Polsce, we Wrocławiu. Czy też neuoprotezowanie mózgu. Naukowcy z wykorzystaniem sztucznej inteligencji są w stanie wszczepić protezę człowiekowi do mózgu i dzięki temu on odzyskuje mowę w jakimś tam stopniu, czy jest w stanie się komunikować dzięki falomózgowym, które sztuczna inteligencja rozumie. Dodatkowo w tym roku również naukowcy Google DeepMind podali informację, że są w stanie, czy w procesie dekodowania języka delfinów. Mamy masę informacji z medycyny, na przykład diagnostyki gruźlicy, przewidywanie nowych leków. Tak więc od zeszłego roku, od poprzedniego Sensei Akademii wydarzyło się bardzo, bardzo dużo. W tym kursie będziemy omawiać wszystkie kluczowe informacje dotyczące SEO i marketingu z najnowszą technologią, także zapraszam Cię do kolejnej lekcji, powoli przechodzimy do konkretu w kolejnej lekcji opowiem Ci, czym są modele językowe, bo głównie na nich będziemy się skupiać jakie są ich wyzwania jak sobie z tym radzić, no i co? zapraszam Cię dalej, cześć!

---

### Lekcja: Zrozumieć Modele Językowe

Cześć, witam Cię w kolejnej lekcji. W poprzedniej zapoznaliśmy się z historią modeli językowych. Jesteśmy już w 2025 roku. Pomówmy jeszcze przez chwileczkę o podstawach modeli językowych. I chciałbym Ci opowiedzieć, czym są modele językowe, jakie mają wyzwania, jak się poruszać w tym świecie, jak je rozpoznawać, jak je oceniać, tak żeby w kolejnych lekcjach płynnie poruszać się w tym świecie i dobierać tak model, żeby spełniał Twoje zarówno oczekiwania, jak i cele czy procesy biznesowe. Okej, pierwsza sprawa. Modele językowe nie mają kreatywności. Modele językowe zostały wytrenowane na zestawie danych. Często, gęsto te dane są nieaktualne i powiedzmy wiedza modeli językowych jest gdzieś odcięta w 2023, być może w 2024 roku, więc modele nie mają kreatywności żadnej. Modele, jeśli mają podyskać jakąkolwiek kreatywność, muszą być karmione w formie feedbackowej. Tak są tworzone leki, tak są tworzone nowe proteiny. Czyli jest coś przewidywane przez model językowy, weryfikowane i po prostu jakaś baza danych budowana. Natomiast co do zasady, one są, jakie są, nie mają kreatywności, przez co nie wytworzą nowych rzeczy. To są procesory językowe, które trzeba nakarmić. Będziemy to bardzo, bardzo szeroko omawiać w tym kursie. I dopiero wnioskować i wytwarzać, powiedzmy albo nowe rzeczy na podstawie karmienia, albo syntezować na podstawie karmienia, ponieważ co do zasady one po prostu są procesorami językowymi, które przetwarzają wejście, dając nam jakieś wyjście. Ok, jak działają modele językowe? Prosty wzór na model językowy. LLM, czyli Large Language Model, równa się Deep Learning plus dane i proces treningowy. Deep Learning, żebyś zrozumiał, to jest na tej zasadzie. Machine learning, masz dane, to jest twoje zadanie, wykonaj zadanie. Deep learning to są, masz dane, zrób coś z nimi. Czyli model po prostu dostaje ogromny korpus danych i po prostu się na nich uczy w sposób taki, że nikt mu nie dał tego zadania. Jak ma się uczyć. Skąd pochodzą dane? Bazy danych, organizacje typu Common Crowd, to jest taka organizacja, która w cudzysłowie ściąga cały internet na dyskietkę, zapisuje i można te dane, czy korpus internetu pozyskać. Książki, dokumenty, wszystko, na czym nie ma prawa autorskich i można pozyskać. Wasze strony internetowe, na szczęście moje też, więc jakby cieszymy się i tak dalej, i tak dalej. Czyli powiedzmy, wyobraźmy sobie gigantyczny, wielki odkurzacz, który jest w stanie wciągnąć cały internet, to tym właśnie jest AI w procesie treningowym. Na początku wciągnęli odkurzaczem cały internet i dali, masz, zrób coś z tym. W 2025 roku Elon Musk, powiedział, że jesteśmy w miejscu, w którym dane treningowe, te realne ze świata ludzkiego powiedzmy, skończyły się. Dlatego w tym momencie dochodzą również dane syntetyczne. Przedniej lekcji mówiłem ci o treningu samochodów autonomicznych. To właśnie są trenowane na danych syntetycznych. Jak to rozumiemy? Mamy zestaw danych, który tutaj widzimy przed sobą i na podstawie tego są tworzone syntetyczne dane, czyli AI tworzy sobie większy zestaw danych, na podstawie których jest dalej trenowany. No tak to działa. W związku z tym mamy kilka problemów. Okej, w tym roku, w 2025 problemy są coraz powiedzmy mniejsze albo są redukowane, ale ciągle występują i muszę Ci o tym powiedzieć, ponieważ z perspektywy SEO czy marketingu to jest najważniejsza rzecz, którą będziemy zarządzać w tym kursie. Czyli pierwsza sprawa. Nie do końca wiemy, jak to działa. Modele mają charakterystykę probabilistyczną, czyli z jakimś prawdopodobieństwem, ci odpowiadają. Nie do końca wiemy z jakim. Znaczy wiemy, jesteśmy w stanie sterować, ale musimy na tyle dobrze zasterować tym modelem, żeby na przykład uzyskać odpowiedni format albo odpowiednią odpowiedź, która nas interesuje, ponieważ jeśli nie, to model nam coś po prostu odpowie. Także sorry, ja się zajmuję tym powiedzmy profesjonalnie, też nie wiem. W zeszłym roku powstało badanie, które pokazywaliśmy, czyli ten AI Brain Surgery, tak to się dumnie nazywa, czyli naukowcy w jakiś sposób wzięli najmniejszy możliwy model na świecie i wykazali, że jeżeli zadają mu zadanie przetłumaczenia treści na język rosyjski, to jakiś element modelu, możemy sobie wyobrazić, to jak połączenia w mózgu zaczyna realizować to zadanie. Jeżeli temu samemu modelowi dają zadanie, powiedzmy, sumaryzacji treści, to zupełnie inna część tej sieci neuronowej zaczyna realizować to zadanie, więc troszeczkę możemy sobie to wyobrażać jak ludzki mózg na takiej zasadzie, że lewa półkula pełni inna funkcja niż prawa półkula, natomiast dwie są potrzebne, żebyśmy żyli. Szczęśliwie pojawiło się kolejne badanie. Poprzednie również stworzył Antropik. To badanie również stworzył Antropik. Chciałbym Ci pokazać krótki filmik, który omawia, do czego dochodzą naukowcy, jakie są wnioski i jak działa model językowy. Ciągle traktujemy to jako black box, czyli wsadzamy coś, wyjmujemy i w środku coś dzieje. Naukowcy próbują to opisać. Zapraszam Cię na krótki filmik, który troszeczkę nam rozjaśni, co się dzieje. Jest to filmik z 2025 roku. Także dwie minutki. Zapraszam. You often hear that AI is like a "black box." Words go in and words come out, but we don't know why it said what it said. That's because AIs aren't programmed, but trained. And during training, they learn their own strategies to solve problems. If we want AIs to be as useful, reliable, and secure as possible, we want to open up the black box and understand why they do things. Ale nawet nie jest bardzo pomocny to form logical circuits. Let's take a simple example where we ask Claude to write the second line of a poem. The poem starts, he saw a carrot and had to grab it. In our study, we found that Claude is planning a rhyme even before writing the beginning of the line. Claude sees a carrot and grab it and thinks of rabbit as a word that would make sense with carrot and rhyme with grab it. Then it writes the rest of the line. His hunger was like a starving rabbit. We look at the place that the model was thinking about the word rabbit, and we see other ideas it had for places to take the poem. We also see the word habit is present there. Our new methods allow us to go in and intervene on this circuit. In this case, we dampen down rabbit as the model is planning the second line of the poem, and then ask Claude to complete the line again. His hunger was a powerful habit. We see that the model is capable of taking the beginning of a new poem and thinking of different ways it could complete it and then writing it towards those completions. The fact we can cause these changes to occur well before the final line is written is strong evidence that the model is planning ahead of time. This poetry planning result, along with the many other examples in our paper, only makes sense in a world where the models are really thinking w ich własnej way, about what they say. Just as neuroscience helps us treat diseases and make people healthier, our longer-term plan is to use this deeper understanding of AI to help make the models safer and more reliable. If we can learn to read the model's mind, we can be much more confident it is doing what we intended. You can find many more examples of Claude's internal thoughts in our new paper at anthropic.com slash research. No właśnie, tak jak słyszeliśmy, model to jest ciągle black box, który, tam było bardzo ważne zdanie, nie został zaprogramowany, tylko został wytrenowany w taki sposób, że sam rozwiązuje problemy. Naukowcy wykazali, że modele są w stanie nie tylko przewidywać następny token, mówiłem o tym w pierwszej lekcji na przykładzie Financial Timesu, ale są w stanie planować to, co się wydarzy w przyszłości, czy w następnym zdaniu. Tam był przykład z rymami, habit, rabbit, okej? Więc też musimy zapanować troszeczkę nad tym, jak on zaplanuje, to co ma tam dalej nam odpisać, prawda? To będzie Damian pokazywał w swojej części dotyczącej prompt engineeringu właśnie, jak nad tym zapanować. No i co? Na dzień dzisiejszy tyle wiemy o modelach językowych, więc przewidywanie następnego tokenu, planowanie i trening, który po prostu jest taki, że model sam się uczył i sam rozwiązywał problemy. W związku z tym mamy największe wyzwanie SEO, halucynacje, ponieważ to jest ciągle prawdopodobieństwo i ciągle planowanie z jakimś prawdopodobieństwem następnego słowa, tokenu, rymu, jak na przykładzie Rabbity Habit i to jest bardzo ciekawe. największe wyzwanie. Jeżeli dotychczas powiedzmy, szedłeś do chat-a GPT i pisałeś, napisz mi arcykuł na temat chwilówki, to na pewno dostawałeś ten arcykuł, oczywiście. Natomiast z dużą pewnością nie działał, prawda? Powiedzmy, Google nie było zainteresowane tym arcykułem w żaden sposób i przez ostatnie miesiące serwisy, które jakby nie rozwiązały, czy osoby nie rozwiązały problemu halucynacji, no jakby utraciły widoczności i będą tracić przy kolejnych kurach update'ach, więc w tym kursie bardzo mocno się pokłonimy. I zobaczcie coś ciekawego. To jest bardzo ciekawa rzecz dotycząca halucynacji. Po prawej stronie widzimy Gemini 2.0 Flash. Jest to model, który zapewne rządzi AI Overview w Polsce i na świecie. Dlaczego? No bo jest najtańszy, najszybszy, bardzo dobry przy okazji, ale widzimy, że bez dostarczenia danych charakteryzuje się poziomem halucynacji 60%. Czyli wyobraź sobie, że 60% twoich treści potencjalnie, jeżeli używasz Gemini 2.0 Flash, może być shalucynowane. No i zobacz, jeżeli oni mają wyzwanie odpowiadania na świecie ludziom w AI Overview i tworzenia tych wyników AI-owych, ale mają taką charakterystykę halucynacji, no to jest to gigantyczny problem. Gdzie tutaj mamy czata? No powiedzmy GPT-4O. Jest to najpopularniejszy model czatowy i mocno wykorzystywany w kontekstach SEO. 57% halucynacji. Czyli co drugie zdanie może być fałszywe w tym przypadku. Ale zobacz jedną bardzo ważną rzecz. Powiedziałem już. chyba w poprzedniej lekcji, że modele są to procesory językowe, być może w tej. Jeżeli ten sam model Gemini Flash, który charakteryzuje się 60% halucynacją, zostanie nakarmiony, ten sam model charakteryzuje się halucynacją 0.7, karmieniem modeli językowych. I tego właśnie poszukuje Google. Google wie, że sam model co do siebie, sam black box, który wystawia nam, halucynuje. Jak zostanie nakarmiony faktycznymi danymi z Twojej strony internetowej, z mojej i tego właśnie poszukuje AI Overview, co będziemy bardzo szczegółowo omawiać w tym kursie, ten sam model już zbliża się do zera, jeśli chodzi o poziom halucynacji. Więc da się ten problem rozwiązać i da się skutecznie budować content z wykorzystaniem modeli językowych, jak zostaną nakarmione w taki sposób, jak Google karmi AI Overview. 0,7% po nakarmieniu. Przypominam, 60 bez. Także to jest ważne, bardzo ważne i musimy to zaadresować w tym kursie i zrobimy to. Kolejny problem. Stronniczość. No, to wynika z tego, że modele nie zostały na przykład wytrenowane na danych polskich. Jeżeli dane polskie stanowią jakiś korpus treningowy, no to jest to pewnie jakiś procent albo poniżej jednego procenta po prostu gdzieś coś po polsku się znalazło. Przez co będą stronnicze zapewne do języka angielskiego. No bo tam pewnie korpus językowy jest absolutnie największy. szczęśliwie pewnie niestronnicze do rosyjskiego, bo pewnie tam nie było języka rosyjskiego w procesie treningowym, albo był w małym stopniu, więc jakby to jest taka dobra informacja. Ale dlaczego to jest problem? Jeżeli pójdziesz do czata i każesz mu napisać artykuł na jakiś temat, to z dużym prawdopodobieństwem zostanie napisany z perspektywy amerykańskiej. Ze stuprocentową pewnością zostanie napisany z interpunkcją amerykańską, typu na przykład duża litera po dwukropku. Listowanie zupełnie inaczej się tworzy w języku polskim niż w języku amerykańskim. W języku amerykańskim czy w angielskim, co do zasady. Pierwszy element listy jest w dużej litery, drugi w dużej litery, trzeci w dużej litery i ostatni z dużej litery w listowaniu. W języku polskim, i to mi zawsze zwracali proofreaders czy edytorzy językowi, profesjonaliści w języku polskim, że tylko pierwszy element w języku polskim jest w dużej litery i kończy się przecinkiem, każdy następnie z małej litery i kończy się kropką. W języku angielskim tak się nie dzieje. Tak więc jeżeli pójdziesz do wczoraj, przepraszam, nie definiując zależności językowych, czy sposobu pisania, otrzymasz tekst sformatowany po amerykańsku po prostu. I wystarczy na niego spojrzeć, już wiesz, że on jest z czata, więc jakby to jest właśnie stronniczość i też będziemy tym zarządzać. Niedeterministyczne zachowanie. O tym już mówiłem. Model ci coś odpowie. Z jakimś prawdopodobieństwem trzeba tym zarządzić i bezpieczeństwo danych. To jest też ogromne wyzwanie modeli językowych na dzień dzisiejszy, bo twoje dane są wysyłane gdzieś. Na przykład do Stanów Zjednoczonych. Pewnie tam nie ma RODO takiego, jakiego jest w Unii Europejskiej. I OpenAI obiecuje nam, ale twoje dane nie będą brane w procesie treningowym pod uwagę. No okej. Tak jest w API. Jeżeli powiedzmy dzwonisz bezpośrednio, rozmawiasz z modelem. Natomiast w czacie, jak sobie czatujesz, Twoje dane mogą być brane pod uwagę w kolejnym procesie treningowym. Czat czasami ma te takie okienka feedbackowe, gdzie pytać się, w jaki sposób ci odpowiedział, co jest lepsze, co jest gorsze. I wyobraź sobie sytuację. Chcesz robić jakieś projekcje finansowe w swojej firmie, wrzucasz dane finansowe do czata, bo chcesz sobie coś policzyć, chcesz sobie policzyć EBITDA czy tam inne historie. Twoje dane są wzięte pod uwagę w procesie treningowym, bo tak się może wydarzyć. A ja później przychodzę i piszę, podaj mi wynik finansowy twojej firmy. No i na podstawie danych finansowych, które ty sobie trenowałeś, EBITDA czy tam jakieś forecasty finansowe, no możemy się dowiedzieć ile zarobiłeś pieniędzy, nie? Więc jakby tutaj musimy uważać, rozwiązanie są modele open source'owe, które również będziemy omawiać w tym kursie, ponieważ na dzień dzisiejszy jesteś w stanie postawić model nawet na swoim laptopie czy w swojej firmie. Bardzo dobry model i być w pełni bezpieczny. Możesz też wybrać firmy, które działają na terenie Unii Europejskiej, są trochę lepiej regulowane. Oczywiście są też firmy chińskie, no i tam już po prostu nie wiemy, co się z tym wydarza. Co do zasady brak pamięci? To jest kolejne wyzwanie modeli językowych. Jeżeli rozmawiasz z modelem językowym w sposób, powiedzmy, profesjonalny przez API, tworzysz proces, on nie ma pamięci, on jest jaki jest, on to jest black box. Wsadzasz, wyjmujesz, wsadzasz, wyjmujesz. Przychodzisz jutro i on nie pamięta, o czym z tobą rozmawiał ostatnio. Też będziemy tym zarządzać, tak żeby modele zyskały pamięć. Oczywiście, systemy czatowe typu chat GPT, Klode, Grok, one... wewnętrznie budują sobie pamięć w środowisku czatowym. To się zaczyna dziać, pamiętają twoje konwersacje, pamiętają kontekst, możesz zarządzić to projektowo, pokażę ci to w następnych lekcjach. Natomiast one działają na takiej zasadzie, że to jest pamięć w narzędziu. To nie jest pamięć modeli, bo model jest jaki jest, jest czarnym skrzynką. Pamięć jest w narzędziu czata, czyli właśnie oni obudowali go pamięcią. W tym kursie również pokażemy ci, jak taką pamięć zbudować sobie wewnętrznie. Kontekst window. W 2024 roku był to ogromny problem. W 2025 roku, zgodnie z naszymi przewidywaniami, z moimi przewidywaniami i Damiana, kontekst window się rozszerza, przestaje to być wyzwanie. W 2024 roku 128 tysięcy tokenów, czyli wyzwanie ciągle, bo nie możesz wsadzić ile chcesz. W tym roku jesteśmy na poziomie miliona tokenów, więc można tam wsadzić naprawdę bardzo dużo, ale pamiętaj, że to jest ciągle limitowane. To nie jest tak, że wsadzisz tam ile chcesz, masz limit. Więc ten kontekst window też jest istotny i często jest limitowana odpowiedź, czyli dobra, ok, w 2025 roku wsadzisz milion tokenów, czy pewnie dużo książek na przykład i ok, masz myśl, a ciągle odpowiedź jest limitowana po drugiej stronie na przykład do 16 czy 32 tysięcy tokenów, więc to nie jest tak, że ci napiszę książkę na jeden raz. Tutaj z tego wynika. No i parametry. Porozmawiamy o parametrach. To już nie są wyzwania, to nie są problemy, natomiast tak są opisywane modele językowe parametrami. Zrozummy to. Parametry, tak jak mówiłem o tym AI Brain Surgery, to jest właśnie to, że lewa półkula odpowiada za coś, prawa półkula odpowiada za coś. Możemy sobie wyobrazić to jak połączenia w mózgu, czy połączenia jakiejś tam sieci, mniej więcej. Im więcej parametrów, tym mądrzejszy model, co prezentuje następny slajd. Działa to mniej więcej na takiej zasadzie. Są małe modele, teraz raz pokażę, one mają mało parametrów i są duże modele. Małe mniej potrafią, bo mają mniej połączeń mózgowych, duże modele, teraz mówi się o modelach, które mają powiedzmy 600 miliardów parametrów, czyli 600, wyobraźmy sobie to jako połączeń mózgowych, po prostu potrafią dużo więcej. I widzimy, tutaj model właśnie rośnie i w momencie, kiedy osiąga powiedzmy te 540 miliardów parametrów, nagle on umie tłumaczyć żarty albo jakieś tam logiczne zadania, czego mały model nie umiał. Więc jakby tak się modele dzielą. Małe dure. Do tego jeszcze przejdziemy. Każdy model językowy jest opisany swojego rodzaju benchmarkami. To też się zmienia w czasie, bo okazuje się, że benchmarki, którymi modele językowe były opisywane jeszcze w zeszłym roku, przestają być aktualne. Dlaczego? Bo wszystkie modele zaczęły osiągać górne granice typu 89, 91, 95. nie było tego miejsca już od 91% na przykład do 100%, żeby rosnąć, dokładnie dywersyfikować, opisywać ten modelem, bo już są takie dobre po prostu, ale powiem Ci, jak dotychczas się to opisywało, bo jest to całkiem istotne. Kilka kluczowych benchmarków. Multitask Language Understanding, Graduate Level Google Proof, Math Work Problem, Evil Coding, Multilingual Grade School Math, Discrete Resonning Over Paragraph. To są przykładowe benchmarki. Więc większość modeli językowych, z którymi mamy do czynienia w 2025 roku, w każdym z tych benchmarków osiąga maksymalne wartości. Co tu jest istotne? To są zadania tekstowe, to są multiselekty często gęste, na przykład ten discrete reasoning over paragraph, typu model dostaje ileś paragrafów treści i ma na tej podstawie wnioskować. czy ten Human Evil Code Generation podświetle go. Jest to na przykład benchmark, który mówi o tym, jak dobrze model koduje. Albo jakieś zadania matematyczne, co widzimy wyżej. Albo jakieś zadania właśnie wnioskowania po języku i tak dalej. To są zawsze zadania tekstowe. W 2025 roku ja ten kurs nagrywam, tę lekcję nagrywam w zdaje się 24 bądź 5, to dzisiaj jest 25, 25 kwietnia. Dwa tygodnie temu dostaliśmy również informacje, że aktualne modele językowe rozwiązują test Turinga. Test Turinga działa w taki sposób, że jest powiedzmy AI, czy komputer, nazwijmy to po prostu komputerem i człowiek i sędzia. Sędzia ludzki, człowiek. I komputer daje odpowiedź i człowiek daje odpowiedź, a sędzia próbuje zgadnąć, co zostało wygenerowane przez komputer. No w tym roku już jakby test Turinga modele językowe zdają, człowiek się pomylił, zdaje się, w 75% przypadków na korzyść komputera. I każdy model jest opisany takimi benchmarkami, ale jest jeden najważniejszy, który wysuwa się na prowadzenie. Dlaczego? Ponieważ często gęsto jest wyścig między firmami, że o, ja mam taki ten human evil, ja mam coś takiego, ja mam coś takiego, ja mam coś takiego i przez te modele językowe zostały trenowane pod benchmarki po prostu na zasadzie, okej, trenujemy cię na konkretny zestaw zadań, żeby być wysoko w benchmarkach, ale one niekoniecznie są takie po prostu super, realnie. Dlatego powstał Humanity Last Exam. Coś takiego. To jest bardzo, bardzo ciekawa sprawa, bo tak jak powiedziałem, benchmarki już nie dojeżdżają. Nie ma miejsca na ocenę modeli, ponieważ no tak jak masz 95% na przykład w jakimś benchmarku, no to no to nie ma miejsca, żeby oceniać. W związku z tym naukowcy z całego świata się powiedzmy skrzyknęli, to był taki program finansowany duży i naukowcy z Politechnik, z Uniwersytetów, z Akademii z całego świata wymyślali zadania logiczne, na które nie da się odpowiedzieć w jeden prosty sposób. Jest to zestaw, powiedzmy, nie pamiętam ilu, ale pewnie, nie wiem, 200-300 pytań. Naukowcy za stworzenie takich pytań byli wynagradzani finansowo i to są pytania klasy takiej, że model musi się bardzo mocno zastanowić, wnioskować, poszukać informacji na temat tego, żeby rozwiązać zadanie. Możemy sobie wyobrazić jakieś odjechane zadanie, na przykład z fizyki kwantowej, jakiś powiedzmy, nie wiem, ruch elektronów, w jakiejś tam próżni, w jakiejś tam studni potencjałów, takie zadania miałem na mojej Politechnice na przykład. No tego się nie da w proze odpowiedzieć po prostu, bo to jest tak trudne. I właśnie tak wygląda Himanetielas exam. I modele na szczęście zaczynają być już tak opisywane. To nam troszeczkę dywersyfikuje jakby zrozumienie i to jest przykład Gemini 2.5 Pro. Obecnie chyba najlepszy model na świecie. No, czy nie wiem, kiedy oglądasz tą lekcję, być może już się dużo zmieniło, bo co tydzień są ruchy. Modele osiągają teraz w tym Humanity Lasexam poziom 18%, co do zasady prawie 19% poprawnych odpowiedzi na te super trudne pytania logiczne wymyślone przez naukowców i to powoduje, że jest dużo miejsca jeszcze na ten wzrost. Co ważne, to są modele, one same w sobie, na poziomie swojej wiedzy osiągają takie takie wyniki, więc jeżeli chcesz ocenić model językowy w sposób niezmanipulowany, no to właśnie patrzymy na Humanity Last Exam jako ten kluczowy benchmark. 18-8% 25 kwietnia. Tak to wygląda. Na pewno to się będzie rozpędzać. Też jest ciekawa nazwa. Humanity Last Exam. Ostatni egzamin ludzkości. Czyli co? Jak osiądniem 100%, to co będzie? No nie wiem, zobaczymy. Istnieją areny, takie leaderboardy. Możesz sobie wpisać w Google chatbot, arena, chatbot, leaderboard, AI leaderboard i oni te wszystkie benchmarki unifikują do jakiegoś jednego skoru, do jednej metryki. W ten sposób też jest dosyć łatwo się poruszać, no jak widzimy właśnie ten Gemini 2.5 Pro, który osiąga to 18-8% w Humanity Last Exam jest najlepszy na dzień dzisiejszy na świecie, więc możemy sobie w ten sposób łatwo ocenić, że okej, to jest dobry model do moich zadań albo najlepszy ogólnie, jeżeli chcemy to zrobić w klasy no-brainer. Ten jest najlepszy, bierzemy go. Możemy oceniać na zasadzie właśnie, na przykład jeżeli mamy konkretne zadania, bo jeden sobie poradzi w logice, czy w rezoningu, okej, to robimy to w ten sposób. Jeżeli mamy wyzwanie na przykład programowania, no to już spójrzmy na ten Human Evil, że na przykład, nie wiem, Cloud 3.7 Sonnet na dzień dzisiejszy będzie w tym najlepszy, no bo się po prostu w tym specjalizuje, jakby w zeszłym roku tego nie widzieliśmy. W 2025 roku widzimy zdecydowaną specjalizację modeli w jakimś tam kierunku, albo firm wręcz, OK, Antropic wyspecjalizuje się w programowaniu, a na przykład OpenAI będzie się specjalizować w czymś innym, takie rzeczy obserwujemy. Dobra, podzielmy te modele w jakiś tam sposób, żebyśmy lepiej się odnaleźli w tym świecie i umieli doskonale w nim poruszać. Przede wszystkim są modele małe, to są te ilości parametrów, które modele mają, czyli modele małe będą potrafić troszkę mniej, ale do niektórych zastosowań mogą być doskonałe. Małe modele też są dużo tańsze w utrzymaniu, dużo tańsze w promptowaniu, więc jakby to jest istotne, że na przykład, ok, mamy ogromne zadanie. Wyobraźmy sobie zadanie, nie wiem, klasyfikacji polskiego internetu. No, brzmi jak drogo. Ale mały model może się okazać do tego doskonały, żeby na przykład skategoryzować strony internetowe, bo to potrafi i będzie to na przykład 10 razy tańsze niż w przypadku modeli dużych. Takim najlepszą reprezentacją na dzień dzisiejszy małych modeli jest Gemma 3 od Google. Jest to model open source'owy, który możesz ściągnąć na swój komputer. tuneować, dostrajać jest za darmo i to jest najlepsza reprezentacja małego modelu, który jest świetny. Zobacz, to są właśnie te benchmarki i widzisz, już masz my tutaj ten score, jeden, tę zunifikowaną wartość. Nie mówimy, że to ma 40 benchmarków, nie, mamy jeden score i model, który ma 27 miliardów parametrów. To widzimy na dole, ja mam taki fajny pilot, pokażę ci to, to patrzymy tutaj, gdzie to jest, o, 27 miliardów parametrów. Praktycznie dorównuję modelowi, który ma 371 miliardów parametrów. Oczywiście są jakieś drobne różnice w tym skorze, ale małe modele zaczynają powiedzmy dojeżdżać w tym przykładzie DeepSea i to jeszcze DeepSea reasoningowego, o którym zaraz będę mówić, a raczej o reasoningu, więc jakby nie zamykajmy się na małe modele, no najlepsza na świecie jest na dzień dzisiejszy gamma. Jak oni to osiągnęli? Oni osiągnęli to w procesie kwantyzacji. Nie będziemy dokładnie tego omawiać w tym kursie, natomiast mniej więcej chodzi o to, że w trakcie treningu model był kwantyzowany, czyli powiedzmy liczby zmienną przecinkowe, tam 32-bitowe zostały na 8 bitów. Sorry. Po prostu był zmniejszany na tyle skutecznie, że nie utracił jakości, miał mniejszej ilości połączeń w mózgu, więc nie zamykamy się na małe modele. Gemma jest najlepszym przykładem, że warto je brać pod uwagę do jakichś zadat. No i modele duże. Modele duże, no to są te wszystkie, które widzimy. O3, O4, GPT-4O, Sonety, Gemini, to są modele duże. One mają setki tysięcy, setki miliardów parametrów. I jak rozpoznajemy duży, mały? Pokażę Ci. To jest bardzo proste, ale jak chcesz się poruszać w tym świecie, musisz to wiedzieć. Zawsze model jest opisany w ten sposób. 1B, 1 miliard parametrów, 27B to jest 27 miliardów parametrów. W ten sposób są opisywane, więc jeżeli musisz wybrać sobie model do swojego zadania, no to możesz w ten sposób, właśnie jak Ci pokazałem, zweryfikować jego jego rozmiar. Niektórzy producenci modeli językowych nie podają rozmiaru, na przykład OpenAI nie podaje rozmiaru, bardziej piszą mini, czyli GPT-4-O duży, GPT-4-O mini mały. Także tak możemy się poruszać w tym świecie, a ten 4-O mini jest też również bardzo ciekawy. Komercyjne i open source'owe. Komercyjne płacimy, są pozamykane OpenAI, Gemini, ale Gemma jest już od Google open source'owa, czyli możemy ściągnąć na swój komputer i po prostu sobie zainstalować w swojej organizacji, zapewnić bezpieczeństwo danych, zoptymalizować koszty. Będą na ten temat kolejne lekcje. Modele są również opisywane w ten sposób. Czat i Instruct. Czat, model skonfigurowany, żeby z Tobą rozmawiać, wymieniać opinie, czatować, odpowiadać Ci. Model Instruct jest to model instrukcyjny, który porusza się po Twoich instrukcjach tam zrealizować zadanie powiedzmy profesjonalne. Żeby Ci to lepiej wytłumaczyć. I to jest najczęstszy błąd, jaki podpieniają ludzie, którzy na przykład idą w świat open source'u. Wybierają sobie jakiś model, powiedzmy Lame, czy whatever sobie wybierają, i wybierają model czatowy. I tam chcą sobie zrobić na przykład keyword research albo jakąś ekstrakcję. No to się nie wydarzy, bo ten model jest skonfigurowany do rozmowy z Tobą, więc wymieni opinię z Tobą na temat keyword research'u. Model instrukcyjny jest to model profesjonalny, więc w ten sposób możemy jeszcze modele rozróżniać, jeżeli chcemy się lepiej poruszać w tym świecie. No i zobacz, tak też są opisywane. Ta stara lama, tutaj co ją widzimy z zeszłego roku jest 8b, czyli czatowa. Jak nie ma napisać czat, to jest czatowa zazwyczaj. No instrukcyjna, czyli skonfigurowana do wykonywania instrukcji. Więc jeżeli nie ma tutaj czata, no to to będzie czatowe. Co prawda w tym roku już coraz więcej się mówi o takich modelach uniwersalnych, ale ciągle chciałem Ci to powiedzieć, żebyś umiał się poruszać w tym świecie. Okej. Mamy rozwój modeli wizyjnych. Modele już powiedzmy widzą. Modele wnioskują po tym, co widzą. W 2025 roku możesz rzucić zdjęcia i kazać modelowi na przykład wnioskować, co tam się znajduje na tym zdjęciu, jak coś zmienić, albo rozmawiać ze zdjęciami. Pokażę Ci bardzo fajną lekcję na koniec tego panelu tego tygodnia dotyczącą notebook LM. Co tam zrobiliśmy właśnie z wykorzystaniem tej wizyjności, tego, że model widzi. No i najważniejsze, modele reasoningowe. Tak też się dzielą modele. Nie wszystkie mają funkcję wizji, ok? Nie wszystkie mają funkcję reasoningu, czyli tego myślenia, przemyśliwania tematu, rozbijania tematu na małe części, w myśl zasady, dziel i rząd, czyli podziel duży problem na zestaw małych problemów, suma małych problemów stanowi duży problem. Tak działają modele rezonningowe, to są te właśnie IQ 132, czy też Gemini, którego pokazywałem. Tak też dzielimy modele. I zobacz, to jest ten leaderboard, który Ci pokazałem wtedy w kontekście tego ArenaScore, a teraz jakby się przyjrzeć, najlepsze modele na świecie właśnie mają tą charakterystykę rezonningową. Czy coś pominąłem? Nie. Wszystkie, które zaznaczyłem, no to na dole, O4, O1 również są rezonningowe. Czy mają zastosowania PSEO? Mają. Ogromne, na przykład do tworzenia nagłówków, do filtrowania nagłówków, do ustawienia hierarchii nagłówków, do jakiejś logiki, treści i tak dalej. Do generowania treści, no nie za bardzo, ale do przygotowania wsadu pod generowanie treści, w pełności tak, to pokażę Ci również w tym kursie, mają gigantyczne zastosowanie dla programistów. Ponieważ programiści mają problemy logiczne właśnie, albo mają jakiś kod, który trzeba, nie wiem, napisać jakieś bardzo skomplikowane zapytanie do bazy danych, czy coś takiego. Tam znajdują super zastosowanie właśnie modele klasy rezonningowej. No i powoli zbliżając się do SEO, chociaż SEO będzie za jakiś czas w tym kursie, jakie mamy wyzwania? Po stronie modeli językowych mamy wyzwanie halucynacji, wyzwanie właśnie biasu, czyli stronniczości, mamy problemy z danymi, a raczej z danymi historycznymi i ok. Jeśli chodzi o wyzwania SEO, znaczenie danych historycznych. Coraz bardziej w SEO Google bierze pod uwagę dane historyczne i sygnały ludzkie. Jest to potwierdzone przed kongresem amerykańskim, jest to potwierdzone w Google Leaków, że Google bierze to pod uwagę. No i teraz my, jako specjaliści SEO, czy też osoby, które chcą być, nie wiem, wysoko w Google, pewnie mają wyzwanie, jak na treści AI zdobyć dane historyczne, skoro, czy nikt nie chce tej treści czytać, albo mało czytać, albo treść jest shalucynowana, no to jak na treści shalucynowanej, czy niskiej jakości ma pozyskać pozytywne dane historyczne, kiedy cały internet jest rozpędzony w Polsce od, nie wiem, 15-20 lat, już te dane historyczne ma. Ogromne wyzwanie ze strony SEO. Po stronie SEO będziemy również tym zarządzać. Kolejne. Inflacja treści. Treści w internecie jest po prostu za dużo. Jeżeli potrzebujesz usmażyć naleśniki, wystarczą ci dwa przepisy. Jeden z wodą mineralną gazowaną, drugi bez wody mineralnej gazowanej zazwykłą. Na przykład. Co do zasady tyle. To jest taki przykład, który zawsze podaję. Po prostu treści jest za dużo, treści jest trochę za darmo. Ekonomia treści jest taka, że kurczę, cena idzie do zera, więc przestaje być po prostu copywriting czy treść jakimś gigantycznym wyróżnikiem, jeśli mamy jeszcze modele językowe, które mają wyzwania halucynacji, no to jest to chyba jedno z największych wyzwań, które stoi przed specjalistami SEO i w tym kursie również tym będziemy zarządzać i koszt przetwarzania. To wynika bezpośrednio z inflacji treści, bo Google za każdą twoją stronę internetową, za każdy nie wiem, crawl, indeksację płaci. Więc trzeba sobie zadać pytanie, dlaczego ma zapłacić za treść z czata GPT, która ma halucynacje, inne problemy, albo masową publikację treści, dlaczego ma zaindeksować. No i właśnie obserwujemy w tym roku problemy z indeksacją, jakby nie jest przewagą konkurencyjną masowe generowanie treści po prostu w czacie, bo to fajnie, ale cała konkurencja to robi, albo wpadł na ten genialny pomysł, że tak będzie budować przewagi, a Google za to nie chce płacić, tym również zarządzimy w tym kursie w kolejnych tygodniach dotyczącym kontentu. No i koniec. Co się dzieje? AI Search. Musimy myśleć o w jednej strony wyszukiwarkach przyszłości, ale one trochę już istnieją. Nie wszystkie funkcje są jeszcze w pełni dostępne w Polsce, no ale tak może wyglądać wyszukiwarka produktowa na świecie. No tak robi to Perplexity i tak to będzie za jakiś czas wyglądać również w Polsce, w Google, więc pewnie to jest doskonały moment, żeby się zastanowić, jak tam się znaleźć w przyszłości. O tym również opowiemy w tym kursie. No i to, co stało się faktem kilka tygodni temu, czyli AI overview, no AI już jest w searchu, prawda? Google już odpowiada bezpośrednio w wynikach wyszukiwania. Gigantyczne wyzwanie SEO, żeby A, tam się znaleźć, B, żeby odebrać z tego jakiś ruch, zoptymalizować starą treść, na przykład inflacja treści. Pokaż kotku, co masz w środku. Jakbym zadał Ci pytanie, co jest na Twoim blogu 5 lat temu, to pewnie nie wiesz. Więc na tym będziemy się również zastanawiać i pokazywać, jak tam się znaleźć, jak to działa. Bo zobacz, z jednej strony Google zabierze Ci ruch, zabierze. Są badania ze Stanów. 30-37% CTR-u, czyli ruchu z AI Overview. ale masz narzędzie, które pozwala Ci rosnąć 10 razy szybciej, więc jakby też sobie pozastanawiamy się na ten temat w tym kursie, w kolejnych lekcjach. No i tyle. Zapraszam Cię do kolejnych lekcji. W kolejnej lekcji pokażę Ci podstawowe narzędzia, jak się po nich poruszać, co tam jest istotne, jakie są fajne funkcje, jak się odnaleźć w tym świecie. I na końcu podsumuję mój prywatny ranking, czego ja używam i co Ci polecam. Do zobaczenia. Cześć.

---

### Lekcja: Podstawowe Narzędzia AI

Cześć, witam Cię w kolejnej lekcji. W tej lekcji przejdziemy przez zestaw podstawowych narzędzi AI, tak żeby każdy z Was umiał poruszać się w tym świecie. Na pewno poruszacie się w czacie i znacie czat. To, co Wam pokażę, to przede wszystkim będzie czat. Poklikamy sobie po jego funkcjach, żeby lepiej zrozumieć, co to tam się dzieje. Pokażę Wam Perplexity, który jest bardzo przydatnym narzędziem w kontekście SEO, marketingu i ogólnie to jest po prostu wyszukiwarka przyszłości, więc przyszłości. Przyszło jest dziś. Więc jakby wyszukiwarka wiedzowa bardzo przydatna. Przejdziemy sobie przez Klodę, przejdziemy sobie przez Groka, którego osobiście uwielbiam i stosuję go pasjami. Przejdziemy sobie przez Gemini i skończymy na Google AI Studio, jako ten ostatni, ostatnie, ostatnie narzędzie, chyba o niczym nie zapomniałem. Czat. Czat każdy zna. Jeżeli nie zna, to zachęcam. Podstawowe narzędzie i najpopularniejsze narzędzie AI-owe na świecie. To, co jest super istotne, to są tutaj modele i wybieranie, umiejętność wybierania odpowiedniego modelu, ponieważ oferta i nazywnictwo w czacie, tutaj, że jak się rozwiniemy, będzie ich więcej, jest dosyć zawiła, a w związku z tym, że publikowane są kolejne modele co miesiąc, co miesiąc, co miesiąc, Powoli ciężko się odnaleźć w tym świecie, więc pokrótce Wam mniej więcej powiem, jak się tutaj nawigować i na co zwracać uwagę. GPT-840 jest to model, powiedzmy, na dzień dzisiejszy podstawowy do zadań typowych, prostych, szybkich podsumowań, szybkich generacji. Jest to model, który również został wypuszczony w zeszłym roku, więc na dzień dzisiejszy jest to, powiedzmy, starość. ale ciągle do zastosowań prostych możemy się pokusić o 4.0. Na końcu jest model 4.5, który jest modelem gigantycznym, arcydrogim, tam milion tokenów kosztuje na poziomie 150 dolarów. Ten model charakteryzuje się naprawdę potężną wiedzą i powiedzmy mocą w cudzysłowie, więc to są bardziej skomplikowane na operacje i jeżeli nie jesteśmy heavy user, chyba limit dzienny wynosi 50 zapytań do tego modelu, stosujmy go, bo jest największy, nierizoningowy i radzi sobie całkiem, całkiem. To, co widzimy poniżej, modele z tym przedrostkiem O, są to modele klasy reizoningowej, czyli to są modele, które powiedzmy, tak w cudzysłowie, myślą, wnioskują przed udzieleniem ci odpowiedzi, przemyślą temat, udzielając dopiero odpowiedzi. To ma ogromne zastosowanie np. dla programistów, gdzie mają np. skomplikowaną logikę algorytmu jakiegoś, albo jakiś problem logiczny, bo jakieś zapytanie SQL-owe, które trzeba przemyśleć, zoptymalizować, te modele znajdują tam największe zastosowanie. W kontekście SEO, gdzie widzę zastosowanie dla nich, na przykład do sortowania nagłówków, układania w logiczną całość nagłówków, wyciągania esencji z treści, czy czegoś takiego głębokiego, gdzie to nie jest przypadkowy ciąg znaków ze spasją, tylko trzeba by się na chwileczkę zastanowić, na przykład jakie nagłówki dobrać do kontentu, albo jaka wiedza jest kluczowa, albo jakiegoś porównania. mamy stronę A, stronę B, porównajmy ją. Nie w sposób przypadkowy, tylko zastanów się nad tym. Co ciekawe, ten model O3 również ma możliwość wnioskowania po zdjęciach, czyli wrzucimy mu jakiś obrazek i będziemy rozmawiać z obrazkiem i wnioskować po obrazku. Co się znajduje na obrazku, co jest kluczowe na obrazku, co zmienić na obrazku i tak dalej, więc jakby to jest bardzo fajne. I zobaczcie, ten model 4,5, który jest modelem właśnie nierizoningowym, czyli nie ma możliwości myślenia, powiedzmy w cudzysłowie, jest oceniany na poziom 100 IQ. Zamianym się zawsze śmiejemy, że średni człowiek ma 100 IQ, więc jakby to jest tego typu pomysł, tego typu, powiedzmy, no, klasa inteligencji, jeśli można mówić o średnim człowieku. O3 wykazują naukowcy, że ma poziom 132 IQ. Właśnie przez ten rezonik, przez to myślenie, więc jakby tutaj jesteśmy już na poziomie naprawdę jakiegoś nieco geniusza, ale przynajmniej jest dobrze. Te modele O4 są jeszcze w klasie mini, czyli to są modele, które dopiero się pojawiły i są to modele małe. Można stosować, ja akurat nie stosuję ich, natomiast doszukuję się zastosowań gdzieś przy bardziej programowaniu. Co jest ciekawe, w historii właśnie nazewnictwa modeli przez OpenAI, a jest duża oferta modelowa, Pominęli model O2, ponieważ jest to najpopularniejsza sieć telefonii komórkowej na Wyspach Brytyjskich. W związku z tym nie chcieli pozwów o znak towarowy czy coś tego typu. Więc było O1 i O3 od razu, O2 zostało pominięte w ramach ciekawostki. Co my tutaj widzimy? Okno czata, wszystko jasne. Natomiast jest tutaj trochę ukrytych rzeczy i trochę ukrytych funkcji, które warto sprawdzić. Jeżeli damy slash, to nam pojawi się taka sztuczka, czyli okno, gdzie mamy po prostu rozwinięte jego funkcjonowanie. funkcjonalności, które są naprawdę bardzo ciekawe, na przykład wyszukiwanie informacji, czyli możemy dzięki temu, mamy pewność, że model pójdzie poszukać informacji w internecie na jakiś konkretny temat, na przykład jaka dziś jest pogoda w Warszawie i w tym momencie on sobie gdzieś pójdzie do internetu, przeszuka sieć i mi odpowie faktycznie, bo wiemy już, że największym wyzwaniem modeli językowych jest alucynacja i problem braku aktualnej wiedzy. Te modele często mają wiedzę odciętą rok, dwa, lata temu. A tu widzimy jasno, że dzisiaj jest 24 kwietnia, czyli wiecie, kiedy nagrywam ten kurs. Wiemy, jaka jest pogoda, czyli za oknem, czy za tą ścianą mamy 22 stopnie Celsjusza, więc jest fantastycznie. I to jest prawda. Mamy taką funkcję wyszukiwania. Więc fajnie. Wracamy do początku. Co my tu jeszcze mamy? Mamy funkcję stworzenia obrazów. To są te najnowsze modele do generacji obrazu. One akurat chyba działają na 4O. Stwórz obraz. To są te obrazy, które trendowały w internecie w kontekście Gilbi i to zrobiło ogromny wiral dla czata. Stwórzmy jakiś obraz. Powiedzmy, stwórz obraz AI Ninja w studio nagraniowym. Także niech mnie stworzy. Zobaczymy, jak mnie widzi wszuszczą inteligencję na podstawie takiego promptu. Niech to się dzieje w tle. A my otwieramy sobie kolejne okno, później wrócimy do tej funkcji. Czyli jeżeli chcemy tworzyć zdjęcie, stwórz obraz. To, co jest arcyprzydatne dla marketerów, specjalistów, jest tutaj opcja Canva. Jest to opcja edytora. Jak sobie wejdziemy jeszcze raz slash, damy Canva i piszemy napisz mi prosty skrypt php. Otworzy nam się edytor. Ojojoj, nie chce mi się otworzyć kredytor. Ktoś korzysta z mojego czata. Otwórz. Canvas. Możemy tak w ten sposób mu kazać to otworzyć. Oj, oj, oj, oj, oj, oj. Ach, ten Canvas. Jeszcze raz. Banowali mnie do tego, że po prostu jestem na firmowym koncie i wstyd się przyznać, ale ktoś korzysta teraz z konta. Ale chwali się, 1620 ktoś pracuje. Jeszcze raz. Dobra, szanowni państwo, mieliśmy mały problem techniczny na moim koncie chat GPT. Po prostu w Vestigio jeszcze aktywnie pracują i mam obcięte konto, ale Mateusz użyczył mi swojego laptopa, więc jesteśmy na jego koncie czata. Pokażę Wam teraz właśnie tą kanwę, jak otworzyć. Czyli pierwsza sprawa, jeszcze raz ten slash, mamy opcję canvas i w tym momencie będziemy mieli właśnie, zobaczycie co, bardzo przydatny edytor, który Wam się przydatny w codiennej pracy. Bierzecie prawie ten slash, kanwaj, piszecie na przykład napisz mi bardzo prosty skrypt PHP. i zobaczcie teraz co się dzieje otwiera się okno, to u mnie nie działało chat GPT teraz ma tak, że jak dużo ludzi korzysta z jednego konta, a u mnie to jest możliwe, że tak się dzieje to automatycznie obniża funkcjonalność do niższej no tak jest no ale ok, co my tutaj widzimy weszliśmy w kanwasy, czyli dostajemy od nich edytor i możemy na przykład zepsuć kod i mu napisać naprawę napraw mi kod. Bardzo przydatna funkcja właśnie debugowania, jeżeli coś się zepsuło, możemy sobie rozmawiać z naszym kodem, albo jak tu będzie plan na główku, zaraz sobie to otworzymy, to będziemy mogli pracować z treścią, rozwijać ją, rozmawiać o treści i kazać mu pracować. W kontekście programowania również znajdujemy tutaj trochę fajnych funkcjonalności, bo teraz jesteśmy przy pisaniu kodu, bardzo przydatne funkcje. Możemy dodać komentarze do kodu, możemy dodać jakiś tam dziennik zmian, naprawić jakieś tam błędy, jakiś port do języka. Czyli port do języka polega na tym, że mamy napisane w języku PHP, a chcemy mieć w Pythonie. No i jak go odpalimy, no to zaraz nam przepisze ten kod z PHP na Pythona. Również bardzo przydatne. Ja obecnie przesiadam się z PHP właśnie na Pythona i często wykorzystuję AI do przepisywania po prostu kodu. O, tutaj coś u Mateusza widzę, też jest mały lag. złośliwość, złośliwość, złośliwość. Prosty coś tutaj napisało. Prosty skrypt Python. A, tutaj trzeba było kliknąć. Jest przepisane. Okej. Mamy przepisane na Pythona i co mamy tutaj ciekawe? Czat ostatnio również udostępnił możliwość uruchamiania kodu w sobie, czyli jesteśmy w stanie na przykład edytować jakiś tam element strony internetowej i sobie robić podgląd. Więc jak damy uruchom, no to mamy, uruchomiona konsolę, że witaj, witaj świecie, czyli to co mieliśmy mieliśmy uzyskać, czyli świecie i bitaj, prawda? Czyli uzyskaliśmy ten kod. Bardzo przydatna rzecz. Jeszcze czemu? Możemy się tym podzielić. Czyli mamy jakiś tam fragment kodu, czy coś nad czym pracujemy, to mamy funkcję udostępnienia i naszemu współpracownikowi jesteśmy w stanie wysłać link i on będzie pracował na tym samym, co my. Wracając do kontekstu SEO, pewnie fajnie będzie tworzyć jakiś content z wykorzystaniem czata, nie? No i do tego Canvas znajduje również doskonałe zastosowanie. Napiszmy mu napisz mi plan nagłówków dla artykułu o kortyzolu. Mój ulubiony kortyzol. No i dobra. Otrzymujemy jakiś plan nagłówków. Nie wchodzimy w to, czy on jest jakby profesjonalny, czy taki ma być, czy nie, czy on ma sens, czy to nie ma znaczenia. dla nas. Otrzymujemy tekst i co do tekstu dostaniemy zupełnie inny zestaw funkcjonalności, który nam daje Canvas. Widzimy tutaj, możemy jakby jakieś edycje prowadzić, dostosować długość, poziom odczytu, udoskonalać, dodawać emoji, więc bardzo przydatne. Do tego wszystkiego czatować z naszą pracą i udostępniać, powielać. Fantastycznie. Dobra, wróćmy dalej do czata. Co on jeszcze potrafi? Czat również ma GPT-sy, czyli możliwość konfiguracji modelu do Waszych konkretnych zastosowań. Nie będę tego omawiać w tej lekcji. Macie dedykowane dwie następne lekcje dotyczące GPT-sów i dwa ćwiczenia dotyczące GPT-s. Tam będę to pokazywać. Co my tutaj jeszcze mamy? Coś dosyć ciekawego, ale chyba już poproszę Mateusza, żeby przepiął mi komputer. No dobra, jesteśmy już na moim laptopie. W międzyczasie możemy zobaczyć, jak wygląda Ninja. w studiu nagraniowym. To też zostało zrobione w czacie. Oczywiście tutaj na co warto zwrócić uwagę, no są litery. Do tej pory była plastelina, jeśli chodzi o modele graficzne. No to, a co, to chyba się nie udało. No ale co do zasady, dostaliśmy jakieś zdjęcie dużo lepsze niż to było do tej pory, więc czat potrafi to robić, ale też potrafi na przykład dokonywać edycji, czyli wrzucasz mu zdjęcie i możesz coś dokleić do tego zdjęcia. Zaraz wam to pokażę. Ponieważ w czacie pojawiła się taka opcja typu biblioteka bibliotece mamy wszystkie zdjęcia, które zostały w jakiś sposób wygenerowane. I tutaj mamy przykład zdjęcia, które moja siostra wygenerowała w czacie, czyli wrzuciła zdjęcie psa rasy Beagle, wrzuciła swoje szelki, którymi handluje w swoim sklepie internetowym i nałożyła szelki na psa. Więc takie rzeczy potrafi robić czat, a my mamy punkcję biblioteki, czyli po prostu podglądu tego, co robiliśmy do tej pory. Co tutaj mamy więcej w czacie? Mamy możliwość projektów, czyli na przykład jakiś tam Szymon, to jest jeden z programistów w Estigio ma swoje projekty, gdzie sobie tutaj widzimy moduł skrapowania, skrapowanie treści, jakby realizuje jakieś swoje projekty. Ja mam jakieś moje programowanie, jakieś Roberty, także możemy sobie poukładać w projekty nasze czaty. Niestety nie ma to takich dobrych funkcjonalności jak w innych narzędziach, które zaraz Wam pokażę, ponieważ projekt stanowi co do zasady tego katalog, gdzie jesteśmy w stanie porządkować. Projekt nie ma jakiejś dodatkowej wiedzy, więc cała historia wszystkich czatów zapewne będzie brana pod uwagę w tym konkretnym projekcie, chyba to nie działa w ten sposób, że następuje jakakolwiek separacja. To, co jest jeszcze warte podkreślenia, to są GPT-sy, nie będziemy tworzyć, ale na pewno mamy marketplace z GPT-sów, gdzie możemy się poruszać i stosować gotowe rozwiązania skonfigurowane do danych czynności, których potrzebujemy. I chyba tyle, tu jest jeszcze jedna funkcja zbadaj głęboko, ale o niej będziemy rozmawiać w osobnej lekcji, więc nie będziemy tego prezentować teraz. Także tyle. Chat GPT fajnie się rozwija i z tego co widać jako trend narzędzie bardziej staje się produktem i feature'ami i funkcjonalnościami, aniżeli nowymi modelami, mimo że cały czas dostają. Działa coraz fajniej, generuje fajne obrazki. Canvas jest rewelacyjny do pracy z treścią, do pracy z kodem, może troszkę mniej, ale ciągle dla małych zastosowanie jest rewelacyjne. Także mocna polecajka i na pewno entry level jak najbardziej. Kolejnym narzędziem, którym chciałbym Państwu przedstawić jest Perplexity. Jest to stricte wyszukiwarka AI-owa, która pozyskuje wiedzę z internetu, aktualną wiedzę i syntezuje do zjadliwej formy. jest bardzo, bardzo korzystne z perspektywy SEO, chociażby po to, żeby rozpocząć pracę w kanbasach w czacie nad naszym artykułem, pójdę do Perplexity, wyszukać aktualnej wiedzy, kontekstu informacji i wrócić do czatu, przepraszam, żeby dalej z nim pracować. Co tutaj mamy ciekawego? Przede wszystkim badania, czyli zaopansowaną analizę tematyczną, ale to sobie omówimy w kolejnej lekcji. I mamy tutaj opcję wyszukiwania, mamy opcję włączoną Pro. Perplexity ma to do siebie, że on sam wybierze nam tryb. W zależności od złożoności problemu na chwilę obecną, wcześniej mieliśmy możliwość konfiguracji, jakiego trybu chcemy użyć. W tym momencie on sam nam dostosuje tryb na przykład do trudności naszego zapytania, więc potencjalnie możemy zapytać, jaki powerbank można zabrać na pokład linii lotniczej lot. Pewnie jest to typowe pytanie. Pewnie jest to pytanie, gdzie nie chce nam się czytać artykułu, na czyjejś stronie internetowej, jak i Powerbank, możemy zapakować. Więc Perplexity jest doskonałym narzędziem, żeby tą robotę wykonać za nas. Jak widzimy, poszli na stronę lotu, przeczytali stronę lotu i dostajemy syntezę wszystkich informacji. Oczywiście z cytowaniem jest bardzo ciekawe. I zachęcam, żeby na przykład tak pozyskaną wiedzę skopiować i pójść z tym do czata i w ten sposób pracować ze swoją treścią, żeby dane były zawsze, uwaga, faktyczne i prawdziwe. Co my tutaj jeszcze więcej mamy? Oczywiście mamy możliwość wyboru modelu, no ale tych modeli jest tyle, że właśnie zrobili to, czego brakuje w czacie, po prostu wybierz najlepszy, więc jakby to jest spoko, ale bardzo mi się podoba to, że oni są w stanie wybierać modele, które nie są ich, bo w czacie mamy tylko OpenAI, wszędzie w każdym innym środowisku będą modele tylko danego producenta. Tutaj możemy wybrać Antropika, możemy iść do OpenAI, możemy wybrać Googla, możemy wybrać Groka, czyli XAI, bardzo fajnie, czyli mamy wybór, możemy bierać modelek, na których chcemy pracować. Bardzo miłe. Mamy obrazy, więc tutaj możemy wyszukiwać sobie informacje, które nam są potrzebne jako obrazy, odpowiedzi, więc jakby co do zasady tyle. Oczywiście naszego Perplexity, czy nasz wynik jesteśmy w stanie udostępnić i to też ostatnio udało mi się wykorzystać i było bardzo przydatne, ponieważ musiałem komuś wytłumaczyć, czym jest format Mastermind z formą HotSeed i nie chciało mi się tego pisać, więc zapytałem Perplexity co to jest, zobaczyłem, że odpowiedź jest taka, jaka powinna być i po prostu sobie wysłałem link. Masz, tu masz informacje, co ja od ciebie chcę, ja chcę, to przeczytaj sobie. I efekt tego ćwiczenia był taki, że osoba mi odpisała, że bardzo dziękuję za wyjaśnienie, nareszcie to rozumie, więc jakby tutaj jest w porządku. Dobra, co jest fajnego w Perplexity? Zaraz sobie tego poszukamy, tylko to jest na stronie głównej. Perplexity ma swoje API. Tutaj nie wiem, czy na tym komputerze jesteśmy zalogowani, czy nie, to nie ma znaczenia, natomiast te modele, które stoją za Perplexity, za tym głębokim reasoningiem i za innym deep researchem, są dostępne. Więc jakby to jest bardzo fajnie, to nie jest temat dzisiejszej lekcji, ale wiemy, że są te modele dostępne. Jeżeli chcemy zbudować nasz proces profesjonalnie w oparciu o Perplexity, tak jest to możliwe, tak jest to bardzo kuszące. Będziemy się tym bawić w kolejnych lekcjach. To my tutaj jeszcze mamy w tym Perplexity. Wiadomo, pogoda, aktualne newsy, jakieś przestrzenie. No fajna zabawka. Raczej odkrywanie, czy jakieś newsy. Perplexity ma zakusy, żeby być po prostu takim, no powiedzmy, portalem, gdzie ludzie poszukują wiedzy, informacji. I fajnie. Pojawiają się sygnały, że Perplexity chce przejąć amerykańskiego TikToka, więc jest to bardzo ciekawe, jak się wydarzy. To bardzo może zmienić search. Podsumowanie Perplexity. Wyszukiwarka, synteza wiedzy, Synteza wiadomości, ściąganie aktualnej wiedzy, budowanie aktualnej wiedzy w serczu i w kontekście, w którym się znajdujemy i fajnie ją stąd zabrać i przenieść do naszego ulubionego edytora, do czata, czy gdziekolwiek pracujemy. Do tego wszystkiego jeszcze możliwość API, tak żeby włączyć Perplexity w nasz proces. Fajnie. Dobra, szanowni Państwo, wracamy do kolejnego narzędzia. Tym narzędziem teraz będzie grog, czyli produkt od X. nie dajcie się zmylić, jestem na moim profilu X-owym. Natomiast tutaj znajduje się zakładka GROK i jest to fantastyczny zarówno model, jak i narzędzie, którego pasjami wykorzystuję do programowania, jakby się przyjrzeć mojej historii. To tu jest pisanie, tu jest pisanie, tu jest pisanie kodu po prostu i działa z tym fantastycznie. Jeżeli jesteście programistami albo potrzebujecie pisać kod, to jest super. Co do zasady mamy jeden model, GROK. To jest interfejs w X-ie. ma oczywiście ten deep search, to w następnej lekcji ma opcję thinkingu, czyli głębokiego myślenia o danym temacie. Bardzo przydatne. Jak ja mam problem logiczny, polegający na przykład na przetworzeniu jakichś skryptów, czy stworzeniu jakiegoś systemu, to połącza mu thinking, żeby się mocno nad tym zastanowił i to fajnie działa, edycję zdjęć. Natomiast to jest interfejs w X. Ja mam konto premium na X, w związku z czym zostałem obdarowany grokiem w wersji SuperGrog w domenie grog.com. I tutaj znajduje się więcej opcji, które są bardzo ciekawe. Co do zasady to jest tak, że praktycznie wszystkie te narzędzia mają bardzo podobne funkcjonalności, natomiast grok zaczyna z czymś, czego nie ma zarówno w Perplexity, jak i nie ma w czacie GPT i od razu do tego przejdę, czyli do Workspaces. Czyli jesteśmy w stanie sobie robić jakby, powiedzmy nie wiem, obszary robocze, w których będziemy pracować tylko i wyłącznie w danym kontekście. czyli na przykład mamy jakiś tam kontekst naszym kontekstem będzie napisz mi prosty skrypt w Python pokazujący cyfr dobra, starczy i w tym momencie mamy ten workspace który już będzie umiejscowiony na jakimś konkretnym kontekście w tym kontekście mamy akurat pisanie w Pythonie i każda kolejna iteracja a każdy kolejny proces będzie już umieszczony w kontekście z historią, z poprzednimi skryptami. To jest niesamowicie korzystne. Dodatkowo możemy do tego dokleić na przykład dane. Te dane to jest na przykład nasz dotychczasowy system albo dotychczasowe pliki, albo dotychczasowy content, albo dotychczasowe, nie wiem, wytyczne redakcyjne, czy cokolwiek sobie dokleimy i to będzie nam pracować w tym jednym systemie. Jak założymy sobie nowy, to mamy jakby separację i kolejny kontekst, czyli potencjalnie z perspektywy agencji SEO ja mogę mieć 40 klientów, 40 workspaców i w każdym workspacie pracuję tylko na danych dotyczących danego klienta, jego kontentu, jego wytycznych, jego innych historii i w ten sposób bardzo fajnie jestem sobie w stanie to segmentować. Tego nie ma w innych narzędziach, znaczy jest w klodę, zaraz pokażę, natomiast to mi się bardzo spodobało w Groku. Dodatkowo możemy do każdego workspacu dodać wytyczne, czyli powiedzmy jakieś wytyczne systemowe, jeśli to jest na przykład programowanie, to wrzucam tam wytyczne dotyczące danego systemu, jak oczekuję. Jeżeli to jest na przykład aspekt kontentowy w kontekście SEO danego klienta, jakieś wytyczne dotyczące kontentu SEO, priorytety, niepriorytety, tone of voice, brand i tam inne historie, więc możemy sobie to skonfigurować i bardzo fajny sposób tutaj z tym sobie współpracować. Więc jakby polecam, to są jakby historie konwersacji, więc jakby to jest bardzo przyjemne. Wracając na stronę główną groka, oczywiście mamy to, że jakieś są persony. Ja tego nie stosuję, natomiast jeżeli ktoś chce mieć lojalnego przyjaciela, no to powiedzmy będzie miał lojalnego przyjaciela, prawda? Edycję zdjęć, to jest fajne. Wrócamy zdjęcie, zmień coś, działa. Tworzenie zdjęć, okej, ale nie jest tak dobre jak w czacie. I research. Research jest bardzo ciekawy. Łacza się opcja deep researchu i deeper searchu, która będzie nam przeszukiwać zasoby bardzo głęboko w celu jakby syntezy wiadomości. O tym będzie następna lekcja. No i co? Można mu włączyć search, ma search, plus co jest fajne, ma search wewnątrz X, czyli platformy społecznościowe. Co tam trenduje, co tam się dzieje, on już to wie, jest to w jego bazie wiedzy, to go mocno odróżnia od konkurencji. I na przykład jeżeli jesteśmy w kontekście politycznym, albo coś tego typu, no to ta wiedza jest na bieżąco, typu o czym napisał dzisiaj Donald Tusk, czy ktoś inny to już to tam jest i co do zasady tylko tam. Więc podsumowanie. Grok. Super. Ja jestem zachwycony osobiście grokiem. Model groka, jeżeli byśmy sobie wpisali powiedzmy LM Leaderboard to pewnie nie powinien czat bodaryna. No na dzień dzisiejszy znajduję na raz, dwa, trzy, na czwartym miejscu przez dłuższy okres czasu był Na pierwszym miejscu teraz jakby mamy powrót do OpenAI i do Google, do którego zaraz przejdziemy, natomiast to jest bardzo, bardzo wysoko. Polecam. Tutaj mamy opcję również thinkingu i deep researchu. Już ma jedną opcję, ma canvas, tylko że ta opcja została udostępniona na świecie kilka dni temu i raz mi się włączy, raz mi się nie włączy. Dobra, dajmy temu szansę. Jeżeli chcemy, na przykład napisz mi prosty skrypt PHP, otwórz w Canvas. Być może zadziała, być może nie. Dzisiaj mi nie zadziałało, wczoraj mi działało. No jeszcze to nie działa, więc to się narzędzie cały czas akurat rozwija i są dokładane nowe funkcje. Jest ogłoszone, że jest udostępnione, ale widocznie jeszcze w Stanach Zjednoczonych i do nas musi to przyjść. Dobra, chodźmy do kolejnego narzędzia, które jest jakby bardzo podstawowe, ale również bardzo przydatne. Jest to Cloder. Bardzo przydatne narzędzie, w szczególności w kontekście programowania i wszystko wskazuje na to, że firma, która stoi za Clodę, czyli firma Anthropic, zaczyna się w tym wysoce specjalizować. Modele są też tak tuningowane. To są twórcy tego protokołu MCP, również dla programistów i taki też jest Cloder. Doszukuje się to głównie takich zastosowań, ale oczywiście świetnie sobie radzi z tekstem, z dadaniami SEO, natomiast z programowaniem zdecydowanie najlepiej. Co tutaj mamy? Modele. Co do zasady mamy, no po sumie będziemy używać jednego, czyli tego 3,7 Sonnet. Mamy jakieś starsze modele, ale jakby na dzień dzisiejszy nie widzę dla nich zastosowań, bo to jest po prostu najlepsze. Standardowo okno czatał i kilka rzeczy, których nie widzieliśmy wcześniej. Pierwsza rzecz. Take screenshot. OK, tam nie było, tam było wrzuć plik, tutaj możemy zrobić screena, miło, ale to, co jest ważne dla programistów, i tu widać właśnie specjalizację, możemy dodać GitHuba, możemy dodać nasze reprezentacje na GitHubie i one zostaną wciągnięte i będziemy z nimi czatować, programować. To jest fantastyczna funkcja, przeleciała troszeczkę poniżej radaru mainstreamowego, ale to jest bardzo fajnie. Możemy też dodać nasze pliki, zsynchronizować się z Google Drive'em, przez co będziemy w stanie pracować na naszych plikach, raportach, istniejącym kontencie, czy co tam mamy na Google Drive'ie. Możemy też jakby robić to w projektach, do których zaraz przejdziemy. To, co mi się podoba w Cloudę, to są tak zwane artefakty. Czyli jesteśmy w stanie, powiedzmy, napisz mi prosty licznik HTML i JS liczący od 10 do 0. On to wykona jako model specjalizujący się w programowaniu. Powinien mi otworzyć taki artefakt, który znajdzie się po prawej stronie. Drafting Artefact. No i zaczyna nam to po prostu pisać w osobnym okienku, czyli to, co dla programistów jest bardzo korzystne. Programowanie po prawej, kodowanie po lewej, więc bardzo fajnie. I właśnie to, co chciałem Wam pokazać, czyli automatycznie interpretacja. Jesteśmy w stanie widzieć w klodę to, nad czym pracujemy i dewelopować sobie live. Mamy na przykład tutaj przycisk reset. Możemy mu napisać zmień na czerwony i on przepisze nam kod i zaraz go nam zaprezentuje po prawej stronie. O, zmienił. Widzicie, zmienia style. I zaraz nam zaprezentuje ten kod. To, co wprawne oko właśnie zobaczyło, a ja wam wytłumaczę, to jest to, że dotychczas modelek, które były stosowane do programowania, i to jest kolejna przewaga, Clode, mamy czerwony przycisk, jeżeli dałeś mu zadanie zmienić coś w kodzie, to on przepisywał cały kod od góry do dołu. A Clode zrobiło tylko edycję w konkretnym miejscu. To właśnie świadczyło ich specjalizacji w programowaniu, że nie muszą przepisać całości, tylko jednak edytują kluczowe miejsca. Okej, mamy jakąś opcję publish, więc jesteśmy w stanie się z tym share'ować z naszym teamem, jesteśmy w stanie to kopiować, downloadować, jesteśmy w stanie wersjonować. Dla programistów jest to rzecz naprawdę bardzo, bardzo przydatna i na dzień dzisiejszy w mojej firmie Vestigio dużo kodu tutaj powstaje. Dodatkowo jak ja używam kursora, o którym również będziemy mówić w tym kursie, to zazwyczaj jest on zasilony właśnie tym modelem od Antropica, czyli 3,5 sonet. Dobra, kolejna rzecz, która tutaj wydaje się być bardzo fajna, to są również projekty, które w kontekście programowania wydaje mi się, że są bardzo ciekawe, ale również w kontekście pracy SEO, tworzenia treści. Zobaczcie, oczywiście mamy tutaj czatowanie, czatowanie, czatowanie, ale mamy tutaj też instrukcję, czyli jeżeli chcemy pisać na przykład stronę internetową o czymś albo konkretną instrukcję systemową, mamy na przykład jakiś konkretny system, nad którym pracujemy, tworzymy i on ma konkretne wytyczne co do funkcjonalności i innych historii, wrzucamy tutaj i system już będzie pamiętał, co my w ogóle piszemy, prawda? Daniel będzie pokazywał to w swojej części na przykładzie tworzenia stron w Lowy Bull. To są właśnie te instrukcje. Dodatkowo jesteśmy w stanie dorzucać wiedzę i po raz kolejny, to czego nie ma nigdzie, Google Drive, synchronizacja z naszym drivem i GitHub, czyli konkretnie nasz projekt stworzony tutaj, pracujemy nad stroną internetową Sensei Academy, podpiętą do GitHub'a, synchronizacja i pracujemy na konkretnym produkcie, a nie jak sobie po prostu wymyślamy jakieś tam rzeczy. No i oczywiście tutaj też jakieś tam synchronizacje, jakieś tam konfiguracje, możliwość używania stylów. Co ciekawe, jesteśmy w stanie stworzyć swój styl. Jeżeli byśmy chcieli pisać treść, no to powiedzmy, nie wiem, nasz brand voice, tone of voice charakteryzuje się jakimś konkretnym stylem i tak chcemy go utrzymać, więc sobie tutaj definiujemy swój własny styl czy tone of voice. No i tak też będzie tworzony content dla nas, więc jakby bardzo przydatne narzędzie. Chyba w szczególności przez te projekty i te możliwości synchronizacji, których nie prezentuje konkurencja. Mocna polecajka co do Klode. Na pewno dla programistów, na pewno dla osób, które chcą pracować w kodzie. No i jeżeli chcesz mieć fajnie poukładane projekty, odseparowane z instrukcjami, no to grog ci to da. Klody też, co Ci da. ChatGPT, Perplexity Ci tego nie da, więc jakby dla profesjonalistów również bardzo mocna polecajka. No i dobra, chodźmy do następnego narzędzia. Następnym narzędziem jest Gemini. Jak widzę, mam wersję Advance, bo mamy Westigio, mamy tą chmurę Google, Gmaila i wszystkie inne rzeczy, więc widocznie dostaliśmy za darmo. Co tutaj mamy? Kurczę, to samo co wszędzie, ponieważ teraz jest trochę wyścig na funkcjonalności, a praktycznie każdy ma te same, chociaż tutaj nie ma tego, co mi się najbardziej spodobało w poprzednim Groku i Klode. No nie mamy projektów, więc nie jesteśmy w stanie się separować, no ale powiedzmy, jeżeli to jest dedykowane do konta Gmail, czy G Suite, lepiej mówić, no to każdy ma swój, no ale ciągle tych kontekstów tam będzie dużo, więc nie ma nie ma jakiejś tam wielkiej separacji tych. No ale okej. Co my tutaj mamy? Wrzucanie plików i wrzucanie zdjęć. No miło, ale widzicie tutaj, no tu widzicie, no muszę sobie włączyć, administrator musi mi włączyć Google Drive'a, a w CloudEich jakby to mam, plus GitHub, więc funkcji mniej, Deep Research, który w następnej lekcji, no i Canvas, mam nadzieję, że tutaj mi się chociaż otworzy ten Canvas w Gemini, napisz mi prosty skrypt PHP o, coś tam rozkminił, dobra mamy Canvas, mamy Canvas więc chyba w pierwszy raz się udało dzisiaj otworzyć go u mnie. Co ciekawe, tutaj jest jakieś wersjonowanie. Dalej tego nie widziałem w innych rozwiązaniach. Oczywiście możliwość czatowania z moim kodem, kolorowanie składni, kopiowanie. No ale widzicie, nie ma opti-sharowania, jakichś takich rzeczy. To są okej, to są mega proste rzeczy. No ale czasami są przydatne. Tak jak na przykładzie tego klode projektów albo share'owania jakichś rzeczy w canvasach Choto GPT, no tutaj niestety tego tego nie widzę. Co my tutaj mamy? Oczywiście mamy dostęp do przeróżnych modeli i teraz tak, to co mam wybrane, ten 2,5 Pro Experimental to jest obecnie najlepszy model językowy na świecie i on jest tutaj w pakiecie, więc jakby jeżeli ktoś potrzebuje do głębokiego rezoningu, myślenia, no to jest to bardzo przydatne i pewnie tutaj bym postawił po stronie Gemini na jakieś naprawdę skomplikowane rzeczy, typu chcesz stworzyć dokumentację techniczną, oprogramowania, czy jakieś coś poważnego, czy przetworzyć jakieś dane, albo się mocno zastanowić, no to pewnie bym tutaj przyszedł, przez to, że ten model na dzień dzisiejszy jest najlepszy na świecie, więc pewnie tutaj uzyskamy jakiś pi razy, czyli najlepszy możliwy efekt. Deep Research to pokazujemy w następnej lekcji. No i co? No i Gemini jak Gemini. No Rocket Science. Kolejny czat. Nie za dużo funkcji. Na szczęście Google bardzo szybko się rozbija, więc zakładam, że tutaj przybędzie. Natomiast od sieczą przychodzi dla nas również Google AI Studio. Jest to rzecz, której pewnie się nie będziecie zbyt często pojawiać, ale być może będziecie. No dobra. Gemini jest to proste zastosowanie, czyli ja mam w swoim koncie Google G Suite Gemini Advanced, mam dostęp do modeli, ja sobie czatuję, klikam i sobie używam i generalnie jesteśmy zadowoleni. Google AI Studio jest to trochę bardziej zaawansowane środowisko, w którym przede wszystkim jesteśmy w stanie konfigurować tego Gemini'a, to Damian będzie omawiać w swoich lekcjach jakby znaczenie tych funkcji temperatury i tak dalej. Natomiast mamy trochę większy wachlarz modeli, bo możemy korzystać z modeli Gemma, modeli 1,5. No zapewne w większości przypadków będziemy korzystać z tego modelu, ale co do zasady mamy tutaj większy wachlarz dostępnych modeli. Mamy opcję konfiguracji, to wyprzedzając Damiana Wam powiem jak to wygląda. Wartość 0 jest to 0 kreatywności, wartość 2 to jest kreatywność klasy LSD. Wartość 1 to jest po prostu kreatywność, powiedzmy. No i tak sobie możemy tym żonglować. Czyli chcemy na przykład zrobić AI Studio profesjonalne zastosowanie, na przykład ekstrakcję słów kluczowych albo coś takiego, dajmy temperaturę na zero, a chcemy pisać kreatywne treści, no to dajmy na wyżej. Czyli tutaj jest pierwsza taka możliwość konfiguracji. W żadnym innym środowisku nie mieliśmy takiej możliwości. Mamy też trochę narzędzi. Mamy Structure Output, czyli możemy strukturyzować odpowiedź, że na przykład chcemy odpowiedź w JSON-ie, w jakimś tam formacie, więc jakby tutaj mamy. Mamy egzekucję kodu, wykonywanie kodu. Widzieliśmy to w klodę, na przykładzie tego lecznika, tutaj widocznie też jest. Function calling, czyli tak zostanie ustrukturyzowana odpowiedź, żeby przekazać to do następnej funkcji, wywołać następną funkcję. Zaawansowane ustawienie, nie będziemy tego stosować, ale to, co możemy zastosować, to jest grounding with Google. I to polega na tym, że AI Studio daje Wam możliwość przeszukiwania Google'a w celu uzupełnienia odpowiedzi faktyczną wiedzę. To jest bardzo ciekawe, bo tak działa AI Overview. Google przeszukuje wiedzę i stosując te modele, które mamy w AI Studio, syntezuje ją do postaci AI Overview. I tutaj jest ta funkcja wystawiona, więc możemy sobie wykonać takie ćwiczenie. Tu jest troszeczkę bardziej zaawansowanych funkcji, nie będziemy w nie chodzić. Ta funkcja grounding wydaje się ciekawa. No i na przykład możemy wrzucić jakiś prompt. Napisz mi plan nagłówków dla frazy. Kortyzol. No i teraz powinien się wydarzyć thinking. Okej, mamy thinking, czyli ten najnowszy model. Powinien wydarzyć się grounding. Mam nadzieję, że się wydarzy. O, tu sobie myśli teraz o tym temacie. Ale chyba jeszcze nam nie poszedł do searchu. O, niestety nie poszedł nam do searchu. Poszedł nam tylko do do thinkingu. No dobra, ale to możemy na przykład zadać inne pytanie. wyszukaj mi wszystkie o jest Google Search Suggest Display Google Suggest nie, to są sugestie wyszukaj informacji na temat kortyzolu i popraw plan no i teraz być może nam się to uda nie dostaliśmy statusu o tym groundingu, czyli wyszukiwaniu, być może on to realizuje w tle, nie prezentując tego statusu, ponieważ tutaj nam pokazał to jest, search sources, czyli na potrzeby, o, i są nawet cytowania, czyli robi to w tle. Na podstawie wyniku Google stworzył nam ten plan nagłówków, podając źródła 14 stron internetowych, które zostały wzięte pod uwagę w procesie właśnie tego groundingu, więc chyba to dla mnie jest największa przewaga Google AI Studio. Dodatkowo widzimy tutaj troszeczkę więcej jakby zastosowań, czyli możemy z nim rozmawiać, czyli bawić się modelami głosowymi. Video Generation, więc jakby to jest ten najnowszy model VO2 tutaj dostępne zapewne. Tak jest. Mamy VO2. Możemy sobie generować wideo. No, jakieś tam aplikacje od Gemini, więc jakby bardzo fajne narzędzie dla troszeczkę bardziej zaawansowanych użytkowników. Największa jego przewaga jest to grounding with Google Search. No i pewnie wideo, bo w poprzednich jakby nie widziałem takich zastosowań. Być może to, że jest dostępny najlepszy model na dzień dzisiejszy na świecie. To tyle. Mam nadzieję, że to była taka prosta, ale przynajmniej troszeczkę ciekawa podróż po podstawowych narzędziach AI-owych, z którymi będziecie się zderzać. Jeśli ja miałbym ocenić po moim uważaniu, albo jak ja stosuję, to na pewno na pierwszym miejscu na dzień dzisiejszy jest dla mnie grok. Ze względu na jakość modelu, na jakość programowania, bardzo mi się podobają te workspacy. w groku. Plode, fantastyczne narzędzie przez projekty i możliwość synchronizacji danych. Chat GPT, szybkie akcje, ale jakby nie jestem jakimś heavy userem. Google Gemini, ciekawostka jako narzędzie, modele stosuję z pasjami w profesjonalnych zastosowaniach. No i Perplexity, do szybkich akcji, do wyszukiwania wiedzy w internecie. Także tak wygląda mój ranking. No i co? Dzięki za to lekcje i do zobaczenia w kolejnej. Cześć!

---

### Lekcja: Pozyskiwanie wiedzy i deep research

Cześć, witam Cię w kolejnej lekcji, fajnie, że jesteś. Przed chwilą omówiłem wszystkie podstawowe narzędzia AI-owe, pokrótce pokazując najciekawsze funkcje i podając moją ocenę. Kilkukrotnie w tych narzędziach mówiłem o funkcji Deep Research, czy też Deep Search. Przyjrzymy się jej nieco bliżej, ponieważ wydaje się ona być bardzo... No okej, ciekawa, ale też istotna w kontekście wykorzystania sztucznej inteligencji, czy tych modeli, nie modeli, rozwiązań na potrzeby SEO. Już o tym mówiłem. Modele nie mają aktualnej wiedzy. Modele mają wiedzę odciętą 2-3 lata temu zazwyczaj. Może rok temu niektóre, ale ciągle nie jest to wiedza aktualna. Ty pisząc artykuły musisz mieć wiedzę faktyczną w kontekście polskim, taką jaką Google docenia, prezentuje w AI Overview i tego czego Google de facto poszukuje. to musi być wiedza faktyczna. Więc twoim zadaniem jest jakby pozyskać ją. To, co widziałeś, Perplexity, czyli narzędzie do wyszukiwarka. Wyszukiwarka i synteza wiedzy na pewno jest świetnym rozwiązaniem. Nie będę się nad nim skupiać, aż tak nad tym typowym Perplexity. Zadanie jest proste. Idziesz do Perplexity, wyszukujesz aktualnej wiedzy, kopiuj wklej do czatu na przykład, czy do twojego ulubionego narzędzia, edytora i już jesteś w stanie z tym pracować. Odbijać się dalej w kierunku na nagłówki, odbijać się dalej w kierunku na tej treści, czy cokolwiek uzupełnianie i tak dalej. To jest jasna sprawa. Pomyślmy chwilę o deep researchu. Zaczynając po kolei, chodźmy do czata. To, co już widzieliśmy, to jest funkcja wyszukiwania, czyli wyszukiwarka, która przeszukuje faktyczne wiadomości. Zapewne czat jest, czy OpenAI, częścią Microsoftu w taki czy inny sposób, na pewno Microsoft jest inwestorem, więc pewnie jest to zasilone bingiem, zakładam, że tak to jest. Natomiast jest to funkcja zbadaj głęboko. I funkcja Zgadaj Głęboko działa mniej więcej w taki sposób. Zadamy jedną frazę, czyli zadamy po raz kolejny frazę kortyzol. Faza kortyzol zostanie rozpisana do szeregu fraz powiązanych. To się nazywa query expansion. Daleko, daleko w tym kursie będziemy bardzo szczegółowo to umawiać. Pokrótce. Mamy frazę kortyzol. Tym sobie rozpiszemy na wszystkie frazy i konteksty powiązane, żeby zrobić głębokie uczenie, żeby nie pierać naszej operacji na dziesięciu stronach internetowych, tylko na dziesięciu frazach, dziesięciu stronach internetowych czy potencjalnie na puli stu. w celu zebrania pełnego obrazu danego tematu. To jest bardzo korzystne w kontekście podyskiwania wiedzy. Tak więc piszemy zbadaj zbadaj mi temat kortyzolu. I tutaj jakby nie wykonał mi jakiegoś głębokiego badania. Spróbuję jeszcze raz. Zobacz, że on już ma tą wiedzę na temat kortyzolu. dobra, niech się produkuje, zapewne może sobie bada w tym momencie głęboko. Perplexity również ma takie badania, czyli ma tą funkcję badania i tutaj mamy właśnie zaawansowana analiza na każdy temat, mają tutaj podpięte modele wnioskujące i właśnie ten już głęboki deep research, czyli dajemy frazę kortyzol i to, co tutaj się zacznie wydarzać w Perplexity, zaraz zobaczymy właśnie to query expansion, jak są rozpisywane konteksty kortyzolu na na tematy, o widzicie, tu się zaczyna rezonik, gdzie on, i will search, coś tam, coś tam, kortyzol i trzy więcej, czyli tu się wykonała funkcja tego query expansion, o którym już powiedziałem, czyli transformacji jednej frazy, zestaw fraz powiązanych i wnioskowanie na tej podstawie. Czytanie stron internetowych, to teraz aktualnie obserwujemy, czyli perplexity dokonuje funkcji odczytywania Dalej. OK? I na podstawie wnioskowania po tych stronach uznał, że brakuje mu jeszcze kolejnych kontekstów typu kortyzol badania i kortyzol normy i szuka kolejnych stron internetowych, które analizuje, zbierając cały czas esencje do kupy. Zaraz przeczyta kolejne strony internetowe, zastanowi się czego mu brakuje w kontekście kortyzolu, rozpisze dodatkowe frazy w celu poszukiwania i pójdzie szukać dalej najprawdopodobniej. I will search, bla bla bla. No i jeszcze nie ma frazy kortyzol definicja, czyli cały czas rozszerza nam temat w celu zaprezentowania całego badania danego tematu. I tak będzie postępować do momentu, w którym jakby wysyci temat i udzieli Wam konkretnej odpowiedzi, czyli doskonała rzecz, żeby zasilić model językowy. O, widzimy, jest kolejne analizowanie tematu i zobaczymy, czy już jest usatysfakcjonowane ilością wiedzy, czy rozpisze kolejne tematy i poszukiwania. Dobra, w tym momencie już wiemy, że ma chyba całą wiedzę. Przeszukuje źródła, analizuje źródła i zaraz zacznie nam drukować pełne podsumowanie. Niech sobie pracuje. Perplexity. O, i mamy opracowanie. Użył do tego 36 źródeł plus 37, 8, 9. 39 źródeł w celu opracowania kontekstu kortyzolu. bo tutaj widać, że się coś przycina albo wolno chodzi, albo jest duże obciążenie perpleksji, no i dostajemy pełną esencję wszystkiego, co istnieje w kontekście kortyzolu w polskim internecie. Co robimy? Idziemy z tym kopii wklej do czata na przykład, czy do kanbasa, czy do naszego edytora i tutaj pracujemy. Hej, podałem Ci całą wiedzę w kontekście kortyzolu, zaplanuj mi artykuł na ten temat. Albo hej, podałem Ci całą wiedzę na temat kortyzolu, czego brakuje w moim artykule, co dopisać. To jest właśnie to zadanie, które możemy w ten sposób zrealizować. To jest arcy, arcy, arcy przydatne, jak widzimy. Naprawdę tutaj tej wiedzy będzie bardzo duża. Ja tutaj często rozwiązywałem w ten sposób problemy, których nie byłem w stanie rozwiązać w prozy w Google, typu jak podpiąć jakąś analitykę w systemie Envato, Marketplace, coś tam, coś tam, coś tam, coś tam. Nie ma takiej informacji. Natomiast tutaj Perplexity poszło tak głęboko, że mi znalazło tą informację, jak to zrobić. De facto to nie działało. ale koniec końców znalazło, wykopało spod ziemi to, czego ja potrzebowałem, żeby się wydarzyło. Jakieś opracowania bardziej poważne możemy w ten sposób sobie tutaj zbudować. Co tam w czacie? Czat też nam coś tutaj porobił, pohalucynował, ale po raz kolejny, moi kochani pracownicy z Festigio działają na naszym koncie, więc jakby nie będziemy de-brysearchu opierać o czat. Co do zasady w czacie działa to identycznie. Grok. Grok ma to samo, tylko Grok ma tą unikalną funkcję, że Grok ma wiedzę z X, czyli to, co się dzieje live, nie? Jak jesteśmy w jakimś kontekście, nie wiem, krypto, czy kontekście inwestycyjnym, na przykład cały świat inwestycyjny dzieje się na X, czy krypto, więc tutaj pewnie tą wiedzę będziemy mieli najlepszą. No i tutaj sobie możemy zaznaczyć opcję Deep Search, albo coś takiego, co się nazywa Deep Search. To jest bardzo, bardzo ciekawe. Spróbujmy tego Deep Searcha i wpiszmy kortyzol. I zobaczmy, co nam zacznie tutaj robić grok. No widzimy, jest to samo, albo przynajmniej w podobny sposób, jak zaprezentował to nam Perplexity. Mamy jakieś tam wnioskowanie po jednej frazie, wyszukiwanie rezultaty, sprawdzanie i tutaj widzimy cały tok myślowy, jak Perplexity wyszukuje cały internet, łącznie z XM w Wikipedii w kontekście kortyzolu. Zaraz do tego sobie wrócimy, bo jakby deeper brzmi jak coś długiego, to sobie wrócimy. Na chwilę obecną grok podaje nam 11 źródeł. Perplexity zrobiło to na źródła 39. Także to jest taka rzecz. Idziemy dalej. I teraz tak. Najlepszy deep research ze względu, że to jest Google, ma Google, czyli najlepsze deep research ma Gemini. Ta pojemność i ilość źródeł, które Google przeszokuje w celu zbudowania tej syntezy i pełnego obrazu wiedzy w Google jest największy. Przynajmniej tak mi się wydaje. Zaraz zobaczymy ile zrobi tego grok. Ale do tej pory Google zawsze zawsze dawał mi najwięcej. więc pewnie już czujesz to, że środowiska zaczynają uzupełniać siebie czyli jeżeli nam się okaże, że w Google zaraz zobaczymy będzie najwięcej przeszukanych źródeł i najdokładniejsza wiedza to warto będzie tam poszukać wiedzy i z nią pójść po raz kolejny do naszego ulubionego środowiska edytora i tak sobie wybierać środowiska do zastosowań, tak to niestety działa na chwilę obecną, to się znajduje w Google tutaj mamy opcję Deep Research i w tym momencie damy frazę kortyzol, czyli ciągle jesteśmy w tym samym kontekście, żeby mieć jakby odniesienie i porównanie. No i Google zacznie nam przeszukiwać cały internet, po raz kolejny tworząc to QR Expansion, rozpisując kortyzol na bardzo dużą ilość fraz. O, tutaj mamy jakieś tam definicje, napisał nam jakiś tam wstępny plan, no i robimy start research. No i ruszył. Na początku nam rozpisał temat, w tym momencie jakby już dokonuje już procesu przeszukiwania całego internetu i zbierania dla nas wiedzy. Więc zobaczymy, ile tutaj nam poda wyników. Jak widzimy tutaj, w tym momencie dzieje się proces jakiegoś rezoningu. O, zobaczcie, czyli wymyślił sobie, co on potrzebuje, jak potrzebuje, jakie tam są najważniejsze rzeczy, jakie są następne stepy. Sam sobie to jako agent rozpisał. No i tu mamy ilość źródeł, które już sobie wyłuskał. Co jest ciekawe, zobaczcie, tu są strony amerykańskie. To jest w zupełności okej, żeby pójść do największego internetu na świecie i tam pozyskać wiedzę, na której dalej w Polsce będziemy uzupełniać graf polskiego internetu. Zobaczycie w którymś z moich następnych tygodni, jak bardzo to jest istotne. Ile mamy źródeł w Groku? Bo już nam odpowiedział. Grok nam odbił odpowiedź na podstawie 32 źródeł. No i co do zasady, ok, mamy po angielsku, translate to Polish, wiecie o co chodzi, chociaż język nie ma znaczenia. No i mamy głęboki research dotyczący kortyzolu. Chwilkę to trwało, kluczowe cytaty, więc mamy faktyczną wiedzę, zsyntezywaną z 32 stron internetowych. Bierzemy do udłowionego edytora i odbijamy się ze strategią. Nagłówki, planowanie, optymalizacja, inne historie. Co tam w Gemini? Gemini nie przestaje analizować. Jak widzimy, ten rezonik trwa. ile tu mamy źródeł, nie podaje nam ilości źródeł, mam nadzieję, że na końcu będzie podsumowanie ale powiedzmy to jest raz, dwa, trzy, cztery cztery razy, raz, dwa, trzy cztery, pięć, sześć, siedem osiem, dziewięć, dziesięć, no dobra, Gemina już wygrała tu jest czterdzieści, cztery razy dziesięć więc jakby w tym pierwszym rzucie już mamy czterdzieści źródeł, które bierze pod uwagę czterdzieści jeden, czterdzieści dwa czterdzieści trzy, czterdzieści cztery czterdzieści osiem, już mamy pięćdziesiąt i on nie skończył, raz, dwa, trzy cztery, raz, dwa, trzy, cz 5, 20, 23. No dobra. Szkoda liczyć. Gemini wygrał i jeszcze nie skończył liczyć. Cały czas nam wnioskuje w kolejnych tematach i w kolejnych kontekstach, których mógł nie pokryć w temacie kortyzolu i pewnie będzie dobierać źródeł. Ten Deep Research tu jest najdłuższy, ale chyba wiecie dlaczego. Dlatego, że tych źródeł już mamy tutaj naprawdę sporo, już nawet nie wiem ile musiałbym policzyć jeszcze raz, ale na pewno więcej niż 40 i to zdecydowanie więcej niż 40 więc no to potwierdza to, co powiedziałem. Po deep research idziemy do Gemini, bierzemy ze sobą tam, gdzie potrzebujemy i budujemy na tej podstawie strategię, nasz content, tak, żeby był faktyczny, prawdziwy i chyba to jest najlepsze rozwiązanie właśnie stosować ten deep research. W kontekście semantyki i planowania też Wam pokażę dużo zastosowań w kolejnych lekcjach z wykorzystaniem właśnie tego query expansion i deep researchu. Także dajmy temu jeszcze chwilę i zobaczymy finalnie ile jest źródeł. No dobra, minęło kilka chwil z Mateuszem, skomentowaliśmy Gemini sobie w tle, że robi wrażenie. Jeszcze nie skończył, proces myślowy u niego trwa. Na chwilę obecną jeszcze nie skończył, ciągle poszukuje wiedzy w internecie. Na chwilę obecną zweryfikował dla nas 73 strony internetowe, więc jakby jest to mocarna ilość w porównaniu z konkurencją i i proces trwa od świeżego, żeby się upewnić, czy nic tam się w tle nie wydarzyło. No nie, no proces jeszcze trwa, jeszcze myśli, więc jakby ogromna ilość wiedzy. No, zdecydowanie Gemini wygrywa konkurs. Nawet bez wiedzy, ile tego będzie na końcu. No, już mamy 73. Pewnie doskonałym pomysłem jest również zabranie tego z Gemini do Notebook LM, o którym opowiem w ostatniej lekcji z tego tygodnia. No cóż, dzięki za tę lekcję. Myślę, że już rozumiecie koncepcję debrysearchu, przynajmniej po co to robimy. W kolejnych tygodniach wytłumaczę Wam, jak to robimy i jak możecie zrobić swój debrysearch na swoje potrzeby. No i co? Używajcie, bierzcie wiedzę faktyczną, bo tego poszukuje Google, tego oczekuje AI Overview. No i pracujcie w ten sposób. Dzięki, cześć!